{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c351bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from LogisticRegressor import LogisticRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f517c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data2.txt\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dfea0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1         x2  y\n",
       "0  34.623660  78.024693  0\n",
       "1  30.286711  43.894998  0\n",
       "2  35.847409  72.902198  0\n",
       "3  60.182599  86.308552  1\n",
       "4  79.032736  75.344376  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d401f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(df['x1'], df['x2'],c=df['y'])\n",
    "plt.title(\"Data Distribution\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a67775",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df[['x1','x2']])\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0965d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegressor(learning_rate=0.01, iterations=80000,reg_lambda=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a66535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Iteration Training loss: 0.6931271807599427\n",
      "[0. 0.] 0.0\n",
      "50. Iteration Training loss: 4.605164176043671\n",
      "[0.52292427 0.23017911] -0.032600000000000004\n",
      "100. Iteration Training loss: 5.623396920661013\n",
      "[ 0.14015528 -0.30477587] -0.07800000000000012\n",
      "150. Iteration Training loss: 3.6640946481761087\n",
      "[ 0.36944896 -0.10925241] -0.11330000000000026\n",
      "200. Iteration Training loss: 4.605163639981461\n",
      "[0.5696271  0.11241855] -0.14860000000000023\n",
      "250. Iteration Training loss: 5.668297106899449\n",
      "[ 0.16894709 -0.34371182] -0.19360000000000022\n",
      "300. Iteration Training loss: 3.5051017369730615\n",
      "[ 0.36351247 -0.11724349] -0.2289000000000002\n",
      "350. Iteration Training loss: 4.605163132208299\n",
      "[0.57960669 0.09392632] -0.2642000000000003\n",
      "400. Iteration Training loss: 5.902909608784346\n",
      "[ 0.14885316 -0.33594024] -0.3092000000000007\n",
      "450. Iteration Training loss: 3.4898208272465983\n",
      "[ 0.37045396 -0.12098866] -0.3444000000000011\n",
      "500. Iteration Training loss: 4.605163006643304\n",
      "[0.56974421 0.10060183] -0.3797000000000015\n",
      "550. Iteration Training loss: 5.6752336510724355\n",
      "[ 0.1687564  -0.34050144] -0.42460000000000186\n",
      "600. Iteration Training loss: 3.5828536458426314\n",
      "[ 0.37628353 -0.11484912] -0.45980000000000226\n",
      "650. Iteration Training loss: 4.605163016802123\n",
      "[0.56295359 0.10786957] -0.4952000000000027\n",
      "700. Iteration Training loss: 5.586074481974741\n",
      "[ 0.1671774  -0.32935521] -0.5400000000000027\n",
      "750. Iteration Training loss: 3.274364122461067\n",
      "[ 0.36611421 -0.13239871] -0.5755000000000027\n",
      "800. Iteration Training loss: 4.605162090138288\n",
      "[0.5725274  0.09127123] -0.6107000000000029\n",
      "850. Iteration Training loss: 5.5413891580683865\n",
      "[ 0.16370354 -0.3197999 ] -0.655400000000003\n",
      "900. Iteration Training loss: 3.4351183921035933\n",
      "[ 0.36346747 -0.11557675] -0.6908000000000032\n",
      "950. Iteration Training loss: 4.605163581182658\n",
      "[0.58129584 0.11492223] -0.7259000000000032\n",
      "1000. Iteration Training loss: 5.6254812470092475\n",
      "[ 0.17307344 -0.33652855] -0.7709000000000032\n",
      "1050. Iteration Training loss: 3.240637957502803\n",
      "[ 0.36205193 -0.12857599] -0.8064000000000032\n",
      "1100. Iteration Training loss: 4.605163343162007\n",
      "[0.58761831 0.10595122] -0.8414000000000034\n",
      "1150. Iteration Training loss: 5.642923020492193\n",
      "[ 0.17148393 -0.3341736 ] -0.8864000000000033\n",
      "1200. Iteration Training loss: 3.505678673024267\n",
      "[ 0.36518115 -0.10723199] -0.9217000000000034\n",
      "1250. Iteration Training loss: 4.605163157808563\n",
      "[0.58285168 0.10734082] -0.9569000000000036\n",
      "1300. Iteration Training loss: 5.623872343013952\n",
      "[ 0.17047843 -0.32954344] -1.0018000000000038\n",
      "1350. Iteration Training loss: 3.2166307717704363\n",
      "[ 0.36311941 -0.12739974] -1.0373000000000032\n",
      "1400. Iteration Training loss: 4.605163150649286\n",
      "[0.58743921 0.10661986] -1.0723000000000027\n",
      "1450. Iteration Training loss: 5.773931791974295\n",
      "[ 0.16166484 -0.32874473] -1.1173000000000022\n",
      "1500. Iteration Training loss: 3.4068587913196366\n",
      "[ 0.35920964 -0.10769545] -1.1526000000000016\n",
      "1550. Iteration Training loss: 4.605162939968839\n",
      "[0.57516705 0.11335775] -1.187800000000001\n",
      "1600. Iteration Training loss: 5.71645609954808\n",
      "[ 0.1627938 -0.3235265] -1.2327000000000006\n",
      "1650. Iteration Training loss: 3.532150394951145\n",
      "[ 0.38046433 -0.1089537 ] -1.2679\n",
      "1700. Iteration Training loss: 4.605160419110219\n",
      "[0.57310531 0.09319   ] -1.3033999999999994\n",
      "1750. Iteration Training loss: 5.673840075631993\n",
      "[ 0.16738134 -0.32424746] -1.348099999999999\n",
      "1800. Iteration Training loss: 3.453477177127427\n",
      "[ 0.37165074 -0.108155  ] -1.3833999999999984\n",
      "1850. Iteration Training loss: 4.605162257930725\n",
      "[0.56919555 0.11289429] -1.4186999999999979\n",
      "1900. Iteration Training loss: 5.60262297034732\n",
      "[ 0.16893908 -0.31879886] -1.4634999999999974\n",
      "1950. Iteration Training loss: 3.4774911035486027\n",
      "[ 0.38571121 -0.11309779] -1.4987999999999968\n",
      "2000. Iteration Training loss: 4.605163054066685\n",
      "[0.59149289 0.11201332] -1.5339999999999963\n",
      "2050. Iteration Training loss: 5.596052167623862\n",
      "[ 0.16723079 -0.31433262] -1.5788999999999958\n",
      "2100. Iteration Training loss: 3.3585400689896376\n",
      "[ 0.3663632  -0.10866831] -1.6142999999999952\n",
      "2150. Iteration Training loss: 4.605162069294854\n",
      "[0.58403373 0.1059045 ] -1.6494999999999946\n",
      "2200. Iteration Training loss: 5.588162946003638\n",
      "[ 0.17007288 -0.31559479] -1.6942999999999941\n",
      "2250. Iteration Training loss: 3.3423272552245633\n",
      "[ 0.36920529 -0.10993047] -1.7296999999999936\n",
      "2300. Iteration Training loss: 4.605163114072663\n",
      "[0.58792155 0.12064898] -1.764799999999993\n",
      "2350. Iteration Training loss: 5.772210456171697\n",
      "[ 0.1602679  -0.31558391] -1.8097999999999925\n",
      "2400. Iteration Training loss: 3.451901689816242\n",
      "[ 0.38218207 -0.10751064] -1.844999999999992\n",
      "2450. Iteration Training loss: 4.605161818490449\n",
      "[0.58470603 0.10804062] -1.8802999999999914\n",
      "2500. Iteration Training loss: 5.558348322831915\n",
      "[ 0.177041   -0.31864585] -1.925099999999991\n",
      "2550. Iteration Training loss: 3.466277597663665\n",
      "[ 0.38786141 -0.10802081] -1.9603999999999904\n",
      "2600. Iteration Training loss: 4.605161903139582\n",
      "[0.59038537 0.10753045] -1.9956999999999896\n",
      "2650. Iteration Training loss: 5.705394280603874\n",
      "[ 0.17323689 -0.32372545] -2.0405999999999893\n",
      "2700. Iteration Training loss: 3.2254984676180025\n",
      "[ 0.37500263 -0.11743947] -2.0759999999999885\n",
      "2750. Iteration Training loss: 4.605160762401857\n",
      "[0.58078431 0.10767165] -2.1111999999999878\n",
      "2800. Iteration Training loss: 5.720817696783988\n",
      "[ 0.16639468 -0.31405801] -2.1559999999999877\n",
      "2850. Iteration Training loss: 3.2035572846737517\n",
      "[ 0.37741594 -0.118932  ] -2.191399999999987\n",
      "2900. Iteration Training loss: 4.605162804633149\n",
      "[0.59551001 0.12007803] -2.226399999999986\n",
      "2950. Iteration Training loss: 5.561598359102304\n",
      "[ 0.17124791 -0.30626792] -2.2712999999999854\n",
      "3000. Iteration Training loss: 3.4695957293987765\n",
      "[ 0.38634469 -0.10130482] -2.306599999999985\n",
      "3050. Iteration Training loss: 4.605159842163521\n",
      "[0.5854771  0.10435949] -2.341999999999984\n",
      "3100. Iteration Training loss: 5.570773448111158\n",
      "[ 0.17352739 -0.30808754] -2.3866999999999834\n",
      "3150. Iteration Training loss: 3.5889409534408525\n",
      "[ 0.38555335 -0.08921498] -2.421899999999983\n",
      "3200. Iteration Training loss: 4.605158920632136\n",
      "[0.58784522 0.10109498] -2.4573999999999825\n",
      "3250. Iteration Training loss: 5.597274498563807\n",
      "[ 0.17589551 -0.31135206] -2.502099999999982\n",
      "3300. Iteration Training loss: 3.4188389801631582\n",
      "[ 0.37945421 -0.09740527] -2.5373999999999817\n",
      "3350. Iteration Training loss: 4.6051609832858835\n",
      "[0.59014079 0.11289269] -2.5726999999999807\n",
      "3400. Iteration Training loss: 5.587675353645187\n",
      "[ 0.1761162  -0.30913045] -2.6174999999999797\n",
      "3450. Iteration Training loss: 3.4023336340094774\n",
      "[ 0.39128307 -0.10436412] -2.652799999999979\n",
      "3500. Iteration Training loss: 4.605161129508546\n",
      "[0.5900808  0.11647215] -2.688099999999978\n",
      "3550. Iteration Training loss: 5.564515242192145\n",
      "[ 0.17605621 -0.30555099] -2.7328999999999772\n",
      "3600. Iteration Training loss: 3.1641959958809274\n",
      "[ 0.38022735 -0.11437274] -2.7683999999999767\n",
      "3650. Iteration Training loss: 4.605162452618137\n",
      "[0.59963809 0.12494812] -2.803399999999976\n",
      "3700. Iteration Training loss: 5.650854726259829\n",
      "[ 0.18476118 -0.32174261] -2.8483999999999754\n",
      "3750. Iteration Training loss: 3.3374993973417775\n",
      "[ 0.38355891 -0.10090634] -2.8836999999999744\n",
      "3800. Iteration Training loss: 4.605161453604993\n",
      "[0.59009464 0.12395815] -2.9188999999999736\n",
      "3850. Iteration Training loss: 5.840917543437188\n",
      "[ 0.17073745 -0.31720095] -2.9638999999999727\n",
      "3900. Iteration Training loss: 3.2046886328848583\n",
      "[ 0.38010736 -0.10721383] -2.9991999999999717\n",
      "3950. Iteration Training loss: 4.60516157718025\n",
      "[0.59003465 0.1275376 ] -3.034299999999971\n",
      "4000. Iteration Training loss: 5.6833231092618215\n",
      "[ 0.17699598 -0.31054492] -3.079199999999971\n",
      "4050. Iteration Training loss: 3.1543779322649335\n",
      "[ 0.38179151 -0.11042849] -3.11459999999997\n",
      "4100. Iteration Training loss: 4.6051622359286295\n",
      "[0.60120225 0.12889237] -3.1495999999999693\n",
      "4150. Iteration Training loss: 5.643322014595821\n",
      "[ 0.18632534 -0.31779836] -3.194599999999969\n",
      "4200. Iteration Training loss: 3.1746597850956007\n",
      "[ 0.38173152 -0.10684903] -3.2299999999999676\n",
      "4250. Iteration Training loss: 4.605162171711446\n",
      "[0.59939681 0.13193062] -3.2649999999999673\n",
      "4300. Iteration Training loss: 5.682389177668448\n",
      "[ 0.18320323 -0.31507095] -3.3099999999999667\n",
      "4350. Iteration Training loss: 3.381741709630097\n",
      "[ 0.40044499 -0.10072851] -3.3451999999999664\n",
      "4400. Iteration Training loss: 4.605161092164403\n",
      "[0.59689363 0.126749  ] -3.380399999999966\n",
      "4450. Iteration Training loss: 5.504160596752465\n",
      "[ 0.18056098 -0.29637281] -3.4251999999999656\n",
      "4500. Iteration Training loss: 3.405911026488007\n",
      "[ 0.38663215 -0.08820563] -3.460499999999965\n",
      "4550. Iteration Training loss: 4.60515979197713\n",
      "[0.5961359  0.12210859] -3.4957999999999645\n",
      "4600. Iteration Training loss: 5.550757142729213\n",
      "[ 0.18211131 -0.29991455] -3.5405999999999636\n",
      "4650. Iteration Training loss: 2.970051921428166\n",
      "[ 0.37666517 -0.11363282] -3.5761999999999627\n",
      "4700. Iteration Training loss: 4.605158687543854\n",
      "[0.59165194 0.12275318] -3.6111999999999624\n",
      "4750. Iteration Training loss: 5.682067763474248\n",
      "[ 0.18055432 -0.30608032] -3.655999999999962\n",
      "4800. Iteration Training loss: 3.417221265792815\n",
      "[ 0.39157034 -0.08674747] -3.6911999999999616\n",
      "4850. Iteration Training loss: 4.605159489605735\n",
      "[0.59975743 0.12325591] -3.726499999999961\n",
      "4900. Iteration Training loss: 5.524717001331344\n",
      "[ 0.19007929 -0.30462595] -3.77129999999996\n",
      "4950. Iteration Training loss: 3.3045406136944195\n",
      "[ 0.39647573 -0.09693576] -3.80659999999996\n",
      "5000. Iteration Training loss: 4.605159382801542\n",
      "[0.60593845 0.12111836] -3.8417999999999597\n",
      "5050. Iteration Training loss: 5.71458294807562\n",
      "[ 0.18111374 -0.30578505] -3.886699999999959\n",
      "5100. Iteration Training loss: 3.365490567470837\n",
      "[ 0.39835549 -0.09144261] -3.9218999999999586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5150. Iteration Training loss: 4.605160201513889\n",
      "[0.60102987 0.13104448] -3.9570999999999583\n",
      "5200. Iteration Training loss: 5.588924454260474\n",
      "[ 0.19431974 -0.31138766] -4.001999999999957\n",
      "5250. Iteration Training loss: 3.1220481736870487\n",
      "[ 0.38679764 -0.10096329] -4.0373999999999475\n",
      "5300. Iteration Training loss: 4.605159037419862\n",
      "[0.60107138 0.12792942] -4.072499999999938\n",
      "5350. Iteration Training loss: 5.714851429342416\n",
      "[ 0.1824724  -0.30396441] -4.117399999999928\n",
      "5400. Iteration Training loss: 3.2320315436276945\n",
      "[ 0.38443375 -0.08897061] -4.152699999999919\n",
      "5450. Iteration Training loss: 4.605159861054841\n",
      "[0.60281731 0.13309548] -4.187799999999909\n",
      "5500. Iteration Training loss: 5.481305415491719\n",
      "[ 0.18648466 -0.29002632] -4.2325999999998976\n",
      "5550. Iteration Training loss: 3.369779264098522\n",
      "[ 0.39255583 -0.08185914] -4.267899999999888\n",
      "5600. Iteration Training loss: 4.605157543714109\n",
      "[0.60031413 0.12791386] -4.303199999999878\n",
      "5650. Iteration Training loss: 5.773799477565759\n",
      "[ 0.18178524 -0.30417674] -4.348099999999867\n",
      "5700. Iteration Training loss: 3.395729250080324\n",
      "[ 0.40006599 -0.08275145] -4.383199999999857\n",
      "5750. Iteration Training loss: 4.605159664630936\n",
      "[0.61462922 0.12919733] -4.4183999999998465\n",
      "5800. Iteration Training loss: 5.579470117829192\n",
      "[ 0.19960847 -0.3103943 ] -4.463299999999836\n",
      "5850. Iteration Training loss: 3.3272859069802907\n",
      "[ 0.40274264 -0.08799055] -4.498499999999827\n",
      "5900. Iteration Training loss: 4.6051567844411805\n",
      "[0.59168992 0.13642663] -4.533799999999816\n",
      "5950. Iteration Training loss: 5.671723157538627\n",
      "[ 0.19063291 -0.3041272 ] -4.578599999999806\n",
      "6000. Iteration Training loss: 3.3092735962052955\n",
      "[ 0.39376708 -0.08172346] -4.613799999999797\n",
      "6050. Iteration Training loss: 4.605153680007284\n",
      "[0.6000297  0.12563964] -4.649099999999785\n",
      "6100. Iteration Training loss: 5.517322382699592\n",
      "[ 0.19742415 -0.29941226] -4.693799999999774\n",
      "6150. Iteration Training loss: 3.0785616465696877\n",
      "[ 0.39259239 -0.09676615] -4.729199999999765\n",
      "6200. Iteration Training loss: 4.605156914590185\n",
      "[0.60643735 0.13189619] -4.764299999999754\n",
      "6250. Iteration Training loss: 5.561251783426007\n",
      "[ 0.19477713 -0.29749478] -4.809099999999744\n",
      "6300. Iteration Training loss: 3.3412880672667105\n",
      "[ 0.40934035 -0.08554599] -4.844299999999733\n",
      "6350. Iteration Training loss: 4.605147896999361\n",
      "[0.60536546 0.1174943 ] -4.879699999999723\n",
      "6400. Iteration Training loss: 5.379639371793444\n",
      "[ 0.20147903 -0.29140151] -4.924299999999711\n",
      "6450. Iteration Training loss: 3.33588156704059\n",
      "[ 0.40775412 -0.083104  ] -4.959599999999701\n",
      "6500. Iteration Training loss: 4.605155707804\n",
      "[0.60928669 0.13165942] -4.99489999999969\n",
      "6550. Iteration Training loss: 5.542421513237739\n",
      "[ 0.19848334 -0.29733735] -5.039699999999678\n",
      "6600. Iteration Training loss: 3.1762869705607373\n",
      "[ 0.40270625 -0.09035219] -5.074999999999669\n",
      "6650. Iteration Training loss: 4.605158087914491\n",
      "[0.61542668 0.13726179] -5.11009999999966\n",
      "6700. Iteration Training loss: 5.403192184453445\n",
      "[ 0.20347151 -0.29203246] -5.154899999999649\n",
      "6750. Iteration Training loss: 3.137005869051225\n",
      "[ 0.40585618 -0.0936555 ] -5.190299999999638\n",
      "6800. Iteration Training loss: 4.605151855154191\n",
      "[0.60194683 0.13346143] -5.22549999999963\n",
      "6850. Iteration Training loss: 5.460086001721754\n",
      "[ 0.20188778 -0.29226963] -5.270099999999621\n",
      "6900. Iteration Training loss: 3.4281257655351025\n",
      "[ 0.40753743 -0.07023133] -5.305199999999614\n",
      "6950. Iteration Training loss: 4.605152230843845\n",
      "[0.60044592 0.13767769] -5.340499999999606\n",
      "7000. Iteration Training loss: 5.503822717716158\n",
      "[ 0.20208401 -0.29387374] -5.385199999999595\n",
      "7050. Iteration Training loss: 3.21240072916394\n",
      "[ 0.39441807 -0.07635027] -5.420499999999587\n",
      "7100. Iteration Training loss: 4.605159190493067\n",
      "[0.61832091 0.14726705] -5.455499999999577\n",
      "7150. Iteration Training loss: 5.5489614625169725\n",
      "[ 0.19596168 -0.28751467] -5.5003999999995665\n",
      "7200. Iteration Training loss: 3.162523833337998\n",
      "[ 0.39240556 -0.07681782] -5.535699999999558\n",
      "7250. Iteration Training loss: 4.605149527020062\n",
      "[0.60796198 0.13317307] -5.57079999999955\n",
      "7300. Iteration Training loss: 5.459791561938447\n",
      "[ 0.20210598 -0.28733719] -5.615399999999542\n",
      "7350. Iteration Training loss: 3.402400177075025\n",
      "[ 0.41355258 -0.07051968] -5.650499999999534\n",
      "7400. Iteration Training loss: 4.605149974226973\n",
      "[0.60646107 0.13738933] -5.685799999999526\n",
      "7450. Iteration Training loss: 5.45501704817132\n",
      "[ 0.20031012 -0.28302421] -5.730399999999517\n",
      "7500. Iteration Training loss: 3.281097171760524\n",
      "[ 0.40227327 -0.07077614] -5.765599999999509\n",
      "7550. Iteration Training loss: 4.60515604237109\n",
      "[0.60762798 0.15135888] -5.800699999999501\n",
      "7600. Iteration Training loss: 5.655545282456347\n",
      "[ 0.19482776 -0.28850146] -5.845499999999491\n",
      "7650. Iteration Training loss: 3.3816190311454006\n",
      "[ 0.41377573 -0.06862363] -5.880499999999484\n",
      "7700. Iteration Training loss: 4.605147892830289\n",
      "[0.60668422 0.13928539] -5.915799999999476\n",
      "7750. Iteration Training loss: 5.4741238012132465\n",
      "[ 0.20053326 -0.28112816] -5.960399999999467\n",
      "7800. Iteration Training loss: 3.399135055209048\n",
      "[ 0.41197987 -0.06431065] -5.995499999999459\n",
      "7850. Iteration Training loss: 4.605148311629907\n",
      "[0.60488836 0.14359837] -6.030799999999451\n",
      "7900. Iteration Training loss: 5.465568393291058\n",
      "[ 0.19903235 -0.2769119 ] -6.075399999999443\n",
      "7950. Iteration Training loss: 3.103326532694203\n",
      "[ 0.40382969 -0.07954119] -6.110699999999435\n",
      "8000. Iteration Training loss: 4.605157218312912\n",
      "[0.62603538 0.1498965 ] -6.145599999999427\n",
      "8050. Iteration Training loss: 5.665429423214756\n",
      "[ 0.19384494 -0.28248586] -6.190499999999418\n",
      "8100. Iteration Training loss: 3.360135566970694\n",
      "[ 0.41858987 -0.06782882] -6.22549999999941\n",
      "8150. Iteration Training loss: 4.605157363525599\n",
      "[0.62394457 0.1543062 ] -6.2605999999994015\n",
      "8200. Iteration Training loss: 5.419494417497215\n",
      "[ 0.21114435 -0.28555415] -6.305399999999392\n",
      "8250. Iteration Training loss: 3.376407939574919\n",
      "[ 0.41649906 -0.06341913] -6.340499999999384\n",
      "8300. Iteration Training loss: 4.60514546361015\n",
      "[0.60940754 0.14448989] -6.375799999999376\n",
      "8350. Iteration Training loss: 5.650959111542141\n",
      "[ 0.19986504 -0.2858106 ] -6.420499999999367\n",
      "8400. Iteration Training loss: 3.252460380301837\n",
      "[ 0.40521975 -0.06367558] -6.455599999999359\n",
      "8450. Iteration Training loss: 4.605145941413951\n",
      "[0.60761168 0.14880287] -6.490799999999352\n",
      "8500. Iteration Training loss: 5.4340317217592675\n",
      "[ 0.21166245 -0.28375481] -6.535399999999343\n",
      "8550. Iteration Training loss: 3.3575622021740634\n",
      "[ 0.4173121  -0.06171651] -6.570499999999336\n",
      "8600. Iteration Training loss: 4.60514162780207\n",
      "[0.61601754 0.14097171] -6.605799999999327\n",
      "8650. Iteration Training loss: 5.42896621847973\n",
      "[ 0.20986659 -0.27944183] -6.650399999999318\n",
      "8700. Iteration Training loss: 3.056890394458692\n",
      "[ 0.40857202 -0.07675361] -6.68569999999931\n",
      "8750. Iteration Training loss: 4.605151254173376\n",
      "[0.61718445 0.15494127] -6.720699999999302\n",
      "8800. Iteration Training loss: 5.428020920455226\n",
      "[ 0.20777578 -0.27503214] -6.765399999999293\n",
      "8850. Iteration Training loss: 3.2291170465154857\n",
      "[ 0.40973893 -0.06278406] -6.800599999999284\n",
      "8900. Iteration Training loss: 4.605151520555543\n",
      "[0.61509364 0.15935096] -6.835699999999276\n",
      "8950. Iteration Training loss: 5.633570942389205\n",
      "[ 0.20229342 -0.28050939] -6.880499999999267\n",
      "9000. Iteration Training loss: 3.244967632954165\n",
      "[ 0.40764812 -0.05837436] -6.915599999999259\n",
      "9050. Iteration Training loss: 4.6051551465942735\n",
      "[0.6265961  0.16150347] -6.950599999999251\n",
      "9100. Iteration Training loss: 5.416434746189111\n",
      "[ 0.21379587 -0.27835688] -6.995399999999242\n",
      "9150. Iteration Training loss: 3.046783729737635\n",
      "[ 0.40670436 -0.07044786] -7.030699999999234\n",
      "9200. Iteration Training loss: 4.605139395405352\n",
      "[0.61205907 0.15168716] -7.065799999999226\n",
      "9250. Iteration Training loss: 5.652830941428019\n",
      "[ 0.20251656 -0.27861333] -7.110499999999217\n",
      "9300. Iteration Training loss: 3.0052728031852745\n",
      "[ 0.4091585  -0.07335503] -7.1456999999992075\n",
      "9350. Iteration Training loss: 4.605132973638807\n",
      "[0.62031015 0.14355919] -7.1807999999991985\n",
      "9400. Iteration Training loss: 5.319971068250836\n",
      "[ 0.22012066 -0.27644746] -7.225299999999191\n",
      "9450. Iteration Training loss: 3.047813021347368\n",
      "[ 0.41184088 -0.07011066] -7.260599999999184\n",
      "9500. Iteration Training loss: 4.605152987991097\n",
      "[0.63404657 0.15932703] -7.295499999999176\n",
      "9550. Iteration Training loss: 5.44583331323272\n",
      "[ 0.21926426 -0.28204242] -7.340299999999167\n",
      "9600. Iteration Training loss: 3.0171309575563146\n",
      "[ 0.40763415 -0.06753716] -7.375599999999158\n",
      "9650. Iteration Training loss: 4.605153917342568\n",
      "[0.63600522 0.16243775] -7.410399999999151\n",
      "9700. Iteration Training loss: 5.626593641473359\n",
      "[ 0.20255642 -0.27132007] -7.455299999999144\n",
      "9750. Iteration Training loss: 3.0143706136851494\n",
      "[ 0.41485513 -0.07088903] -7.490499999999136\n",
      "9800. Iteration Training loss: 4.605128719722397\n",
      "[0.61822775 0.14973688] -7.525599999999128\n",
      "9850. Iteration Training loss: 5.666263998788404\n",
      "[ 0.20868455 -0.28063016] -7.570299999999115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900. Iteration Training loss: 3.3471893785273115\n",
      "[ 0.42854947 -0.05312872] -7.605199999999106\n",
      "9950. Iteration Training loss: 4.605146062058057\n",
      "[0.62386516 0.16322535] -7.640399999999098\n",
      "10000. Iteration Training loss: 5.441433936375272\n",
      "[ 0.21276936 -0.26835388] -7.68509999999909\n",
      "10050. Iteration Training loss: 3.2065983428619638\n",
      "[ 0.42561997 -0.06000575] -7.72019999999908\n",
      "10100. Iteration Training loss: 4.6051457156907825\n",
      "[0.62356068 0.16554748] -7.755299999999073\n",
      "10150. Iteration Training loss: 5.402324848520335\n",
      "[ 0.21855677 -0.27134926] -7.799999999999065\n",
      "10200. Iteration Training loss: 3.1212222372873013\n",
      "[ 0.41909976 -0.06011938] -7.835199999999054\n",
      "10250. Iteration Training loss: 4.605151308943247\n",
      "[0.63784015 0.1663337 ] -7.870099999999046\n",
      "10300. Iteration Training loss: 5.418287321835075\n",
      "[ 0.21768967 -0.26958458] -7.914899999999038\n",
      "10350. Iteration Training loss: 3.0694821495979716\n",
      "[ 0.41639145 -0.06019079] -7.950099999999029\n",
      "10400. Iteration Training loss: 4.605123583272155\n",
      "[0.63068952 0.14842465] -7.985199999999023\n",
      "10450. Iteration Training loss: 5.302749777991365\n",
      "[ 0.22284777 -0.26688916] -8.029699999999016\n",
      "10500. Iteration Training loss: 3.296921637611848\n",
      "[ 0.4254609  -0.04760509] -8.064799999999005\n",
      "10550. Iteration Training loss: 4.605109118054319\n",
      "[0.62020852 0.1513443 ] -8.100099999999\n",
      "10600. Iteration Training loss: 5.609929388063721\n",
      "[ 0.20700226 -0.26522391] -8.144699999998988\n",
      "10650. Iteration Training loss: 3.0099179021381546\n",
      "[ 0.42240938 -0.06498112] -8.179799999998982\n",
      "10700. Iteration Training loss: 4.6051482158194\n",
      "[0.63117292 0.17371614] -8.214699999998974\n",
      "10750. Iteration Training loss: 5.339373419593088\n",
      "[ 0.22327267 -0.26632837] -8.259399999998962\n",
      "10800. Iteration Training loss: 3.134867383676491\n",
      "[ 0.43288284 -0.06086479] -8.294499999998957\n",
      "10850. Iteration Training loss: 4.605145479820057\n",
      "[0.63825483 0.16794553] -8.329499999998948\n",
      "10900. Iteration Training loss: 5.577614016461312\n",
      "[ 0.21096436 -0.264621  ] -8.374299999998938\n",
      "10950. Iteration Training loss: 3.175354599926729\n",
      "[ 0.43048676 -0.05435762] -8.409299999998929\n",
      "11000. Iteration Training loss: 4.60514725223184\n",
      "[0.63592884 0.17425593] -8.44429999999892\n",
      "11050. Iteration Training loss: 5.589586924034909\n",
      "[ 0.21188428 -0.26487733] -8.489099999998913\n",
      "11100. Iteration Training loss: 3.027743562049046\n",
      "[ 0.4162601 -0.0536355] -8.524199999998906\n",
      "11150. Iteration Training loss: 4.605119181253984\n",
      "[0.62820739 0.16046989] -8.5591999999989\n",
      "11200. Iteration Training loss: 5.614618388270229\n",
      "[ 0.21350244 -0.26699484] -8.603799999998893\n",
      "11250. Iteration Training loss: 2.939162792062051\n",
      "[ 0.422192  -0.0624493] -8.638899999998886\n",
      "11300. Iteration Training loss: 4.6050967170083315\n",
      "[0.63310883 0.15075131] -8.67389999999888\n",
      "11350. Iteration Training loss: 5.402249506774357\n",
      "[ 0.22923671 -0.27099558] -8.718399999998873\n",
      "11400. Iteration Training loss: 3.1786409373191\n",
      "[ 0.4350921  -0.05157267] -8.753399999998868\n",
      "11450. Iteration Training loss: 4.605120172458656\n",
      "[0.63189281 0.16351117] -8.788499999998862\n",
      "11500. Iteration Training loss: 5.478290221358227\n",
      "[ 0.22937166 -0.27458858] -8.833099999998852\n",
      "11550. Iteration Training loss: 2.9661298853209774\n",
      "[ 0.42646732 -0.05960146] -8.868199999998845\n",
      "11600. Iteration Training loss: 4.605143782668126\n",
      "[0.64535882 0.17385401] -8.902999999998837\n",
      "11650. Iteration Training loss: 5.346366199985869\n",
      "[ 0.22344746 -0.25676776] -8.947699999998829\n",
      "11700. Iteration Training loss: 2.899491002829438\n",
      "[ 0.42712207 -0.06319113] -8.982899999998821\n",
      "11750. Iteration Training loss: 4.605128195859091\n",
      "[0.64777627 0.1622221 ] -9.017699999998817\n",
      "11800. Iteration Training loss: 5.539770899730338\n",
      "[ 0.22013804 -0.26337564] -9.062399999998808\n",
      "11850. Iteration Training loss: 2.9489633540863824\n",
      "[ 0.42912255 -0.05892683] -9.0974999999988\n",
      "11900. Iteration Training loss: 4.605109890135345\n",
      "[0.63095915 0.16709565] -9.132499999998792\n",
      "11950. Iteration Training loss: 5.31206660829969\n",
      "[ 0.23169572 -0.26144424] -9.176999999998783\n",
      "12000. Iteration Training loss: 3.170063987478496\n",
      "[ 0.441457   -0.04897828] -9.211999999998778\n",
      "12050. Iteration Training loss: 4.605113558449512\n",
      "[0.63855266 0.16600885] -9.24709999999877\n",
      "12100. Iteration Training loss: 5.462336869934784\n",
      "[ 0.22980578 -0.26710048] -9.291699999998762\n",
      "12150. Iteration Training loss: 2.802623904711908\n",
      "[ 0.42357998 -0.06219707] -9.326899999998755\n",
      "12200. Iteration Training loss: 4.605096551771058\n",
      "[0.63921376 0.16169855] -9.361799999998746\n",
      "12250. Iteration Training loss: 5.302631096986556\n",
      "[ 0.2295447  -0.25482754] -9.406299999998739\n",
      "12300. Iteration Training loss: 3.232881000215375\n",
      "[ 0.44178693 -0.04081887] -9.44129999999873\n",
      "12350. Iteration Training loss: 4.605097716536131\n",
      "[0.63747313 0.16579041] -9.47649999999872\n",
      "12400. Iteration Training loss: 5.5624046860599\n",
      "[ 0.22299304 -0.26157426] -9.521099999998713\n",
      "12450. Iteration Training loss: 3.030237887134999\n",
      "[ 0.4338158  -0.04851724] -9.556099999998704\n",
      "12500. Iteration Training loss: 4.605118555356093\n",
      "[0.65157731 0.16704266] -9.590999999998697\n",
      "12550. Iteration Training loss: 5.392382716709441\n",
      "[ 0.22938827 -0.2568071 ] -9.635599999998691\n",
      "12600. Iteration Training loss: 3.121012964648944\n",
      "[ 0.44014729 -0.04427393] -9.670599999998682\n",
      "12650. Iteration Training loss: 4.605131250912749\n",
      "[0.6518169  0.17660348] -9.705499999998677\n",
      "12700. Iteration Training loss: 5.278376465403856\n",
      "[ 0.23897203 -0.25985115] -9.750099999998673\n",
      "12750. Iteration Training loss: 3.1161569157711595\n",
      "[ 0.44145314 -0.04363989] -9.785099999998668\n",
      "12800. Iteration Training loss: 4.605101528449809\n",
      "[0.6420159  0.17158612] -9.820099999998664\n",
      "12850. Iteration Training loss: 5.5388376168552815\n",
      "[ 0.22556008 -0.25800828] -9.864699999998658\n",
      "12900. Iteration Training loss: 3.025610128897621\n",
      "[ 0.43638284 -0.04495126] -9.89969999999865\n",
      "12950. Iteration Training loss: 4.605030318522746\n",
      "[0.63560623 0.16170015] -9.934799999998644\n",
      "13000. Iteration Training loss: 5.394971826170682\n",
      "[ 0.23570009 -0.25977425] -9.979199999998638\n",
      "13050. Iteration Training loss: 2.770108424649239\n",
      "[ 0.42792099 -0.05614957] -10.01439999999863\n",
      "13100. Iteration Training loss: 4.6050846991375725\n",
      "[0.64702187 0.16798493] -10.049199999998626\n",
      "13150. Iteration Training loss: 5.4427101712804475\n",
      "[ 0.23877387 -0.2650908 ] -10.093799999998616\n",
      "13200. Iteration Training loss: 3.224737909139569\n",
      "[ 0.4477225  -0.03423007] -10.128599999998611\n",
      "13250. Iteration Training loss: 4.60513108557538\n",
      "[0.65900644 0.1817197 ] -10.163499999998606\n",
      "13300. Iteration Training loss: 5.364063830978033\n",
      "[ 0.23355969 -0.25168991] -10.2081999999986\n",
      "13350. Iteration Training loss: 3.0782703313813493\n",
      "[ 0.44459815 -0.04106134] -10.243099999998595\n",
      "13400. Iteration Training loss: 4.605086506218051\n",
      "[0.64901343 0.17218879] -10.278099999998588\n",
      "13450. Iteration Training loss: 5.424915509799649\n",
      "[ 0.23351131 -0.25375866] -10.322599999998584\n",
      "13500. Iteration Training loss: 2.9354309753713235\n",
      "[ 0.4399529  -0.04650898] -10.357499999998584\n",
      "13550. Iteration Training loss: 4.605123639119926\n",
      "[0.65916152 0.18218277] -10.392299999998574\n",
      "13600. Iteration Training loss: 5.360651365432426\n",
      "[ 0.23404213 -0.2486612 ] -10.436899999998571\n",
      "13650. Iteration Training loss: 2.8956552956180435\n",
      "[ 0.43754157 -0.04584696] -10.471899999998572\n",
      "13700. Iteration Training loss: 4.605132962690927\n",
      "[0.65537498 0.19306895] -10.506599999998564\n",
      "13750. Iteration Training loss: 5.130911193769029\n",
      "[ 0.24740645 -0.24952451] -10.551199999998557\n",
      "13800. Iteration Training loss: 2.9767762587419093\n",
      "[ 0.4471132  -0.04464738] -10.586199999998556\n",
      "13850. Iteration Training loss: 4.605078083337704\n",
      "[0.65568256 0.17295193] -10.62109999999855\n",
      "13900. Iteration Training loss: 5.470359948446491\n",
      "[ 0.23837622 -0.25775729] -10.665699999998543\n",
      "13950. Iteration Training loss: 2.9869765754513242\n",
      "[ 0.45231374 -0.04560914] -10.70059999999854\n",
      "14000. Iteration Training loss: 4.60508768264534\n",
      "[0.66064149 0.17471899] -10.73539999999854\n",
      "14050. Iteration Training loss: 5.4751994927595105\n",
      "[ 0.23152182 -0.24748683] -10.779899999998538\n",
      "14100. Iteration Training loss: 3.053886225915028\n",
      "[ 0.444522   -0.03342787] -10.814699999998536\n",
      "14150. Iteration Training loss: 4.605074567831389\n",
      "[0.65577465 0.17704725] -10.849599999998537\n",
      "14200. Iteration Training loss: 5.450303571670928\n",
      "[ 0.24204158 -0.25766305] -10.894099999998536\n",
      "14250. Iteration Training loss: 3.0294690064900744\n",
      "[ 0.45036933 -0.03733493] -10.928899999998537\n",
      "14300. Iteration Training loss: 4.605021124002478\n",
      "[0.65582022 0.16919348] -10.963799999998535\n",
      "14350. Iteration Training loss: 5.4486331461585635\n",
      "[ 0.23848713 -0.25126069] -11.008199999998533\n",
      "14400. Iteration Training loss: 2.919774179034677\n",
      "[ 0.44818647 -0.0420643 ] -11.043099999998534\n",
      "14450. Iteration Training loss: 4.6050532758290075\n",
      "[0.65552352 0.17754255] -11.077899999998532\n",
      "14500. Iteration Training loss: 5.466083524397345\n",
      "[ 0.24055272 -0.25332207] -11.122399999998532\n",
      "14550. Iteration Training loss: 2.7255746019020735\n",
      "[ 0.43706695 -0.04685926] -11.15739999999853\n",
      "14600. Iteration Training loss: 4.605059436900154\n",
      "[0.66093069 0.17764978] -11.191899999998528\n",
      "14650. Iteration Training loss: 5.461520136624166\n",
      "[ 0.24133762 -0.25229703] -11.236399999998522\n",
      "14700. Iteration Training loss: 2.7212245464178006\n",
      "[ 0.43785184 -0.04583422] -11.27139999999852\n",
      "14750. Iteration Training loss: 4.605081614551061\n",
      "[0.64913517 0.19346544] -11.305899999998518\n",
      "14800. Iteration Training loss: 5.117367584257787\n",
      "[ 0.24514986 -0.23456709] -11.350199999998514\n",
      "14850. Iteration Training loss: 2.7789885100697824\n",
      "[ 0.43742203 -0.03910603] -11.38529999999851\n",
      "14900. Iteration Training loss: 4.604999749157865\n",
      "[0.65244374 0.17847033] -11.419999999998508\n",
      "14950. Iteration Training loss: 5.181371013856833\n",
      "[ 0.24176146 -0.23242517] -11.464199999998504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000. Iteration Training loss: 2.8935261065121916\n",
      "[ 0.45253776 -0.0395152 ] -11.499199999998503\n",
      "15050. Iteration Training loss: 4.60498783592515\n",
      "[0.65569645 0.17722689] -11.5339999999985\n",
      "15100. Iteration Training loss: 5.464401616518172\n",
      "[ 0.2367621  -0.24136309] -11.578299999998498\n",
      "15150. Iteration Training loss: 3.0507211062342674\n",
      "[ 0.44285659 -0.01946493] -11.612999999998495\n",
      "15200. Iteration Training loss: 4.6050509128584824\n",
      "[0.66232953 0.18503666] -11.64779999999849\n",
      "15250. Iteration Training loss: 5.186412429703312\n",
      "[ 0.24941699 -0.23867408] -11.692099999998486\n",
      "15300. Iteration Training loss: 2.9765268557526894\n",
      "[ 0.45915944 -0.03417496] -11.726999999998485\n",
      "15350. Iteration Training loss: 4.6050668482336174\n",
      "[0.65294265 0.19731051] -11.761699999998486\n",
      "15400. Iteration Training loss: 5.454048914916489\n",
      "[ 0.23620978 -0.23651754] -11.806099999998484\n",
      "15450. Iteration Training loss: 3.0201030608089168\n",
      "[ 0.45376434 -0.02538805] -11.840799999998481\n",
      "15500. Iteration Training loss: 4.605001153041768\n",
      "[0.65841569 0.18466095] -11.875499999998482\n",
      "15550. Iteration Training loss: 5.051804282588753\n",
      "[ 0.25639522 -0.2360536 ] -11.919599999998479\n",
      "15600. Iteration Training loss: 2.925092059740588\n",
      "[ 0.45932641 -0.03437   ] -11.954399999998474\n",
      "15650. Iteration Training loss: 4.604967451528216\n",
      "[0.64921751 0.1892648 ] -11.989099999998471\n",
      "15700. Iteration Training loss: 4.958706720437616\n",
      "[ 0.25772727 -0.23088483] -12.033099999998464\n",
      "15750. Iteration Training loss: 2.7724072801146162\n",
      "[ 0.45150788 -0.03845011] -12.067999999998465\n",
      "15800. Iteration Training loss: 4.604938658330577\n",
      "[0.65797231 0.18278021] -12.102499999998466\n",
      "15850. Iteration Training loss: 5.392534670556051\n",
      "[ 0.25115657 -0.24662239] -12.146599999998466\n",
      "15900. Iteration Training loss: 2.906753702872835\n",
      "[ 0.45881381 -0.0316141 ] -12.181199999998467\n",
      "15950. Iteration Training loss: 4.60503013462309\n",
      "[0.67017971 0.18851742] -12.215699999998467\n",
      "16000. Iteration Training loss: 5.369415738896069\n",
      "[ 0.24858245 -0.24020022] -12.259899999998467\n",
      "16050. Iteration Training loss: 2.8787631442429564\n",
      "[ 0.45643941 -0.03013191] -12.294499999998466\n",
      "16100. Iteration Training loss: 4.605067112555014\n",
      "[0.67579735 0.19435369] -12.328899999998463\n",
      "16150. Iteration Training loss: 5.1905691532395055\n",
      "[ 0.25102603 -0.2309225 ] -12.373099999998457\n",
      "16200. Iteration Training loss: 2.9690000904982283\n",
      "[ 0.45473588 -0.0204505 ] -12.407599999998453\n",
      "16250. Iteration Training loss: 4.604988485591049\n",
      "[0.66616468 0.19025379] -12.442099999998453\n",
      "16300. Iteration Training loss: 5.109880963772874\n",
      "[ 0.25426553 -0.22863956] -12.485999999998446\n",
      "16350. Iteration Training loss: 2.97802245150903\n",
      "[ 0.45327783 -0.01693843] -12.520499999998444\n",
      "16400. Iteration Training loss: 4.605037659479183\n",
      "[0.66311511 0.2021158 ] -12.55479999999844\n",
      "16450. Iteration Training loss: 5.067527686100856\n",
      "[ 0.26224078 -0.23413319] -12.598799999998437\n",
      "16500. Iteration Training loss: 2.848628454139391\n",
      "[ 0.45983577 -0.02917464] -12.633299999998433\n",
      "16550. Iteration Training loss: 4.605036216268373\n",
      "[0.66502165 0.20305577] -12.66729999999843\n",
      "16600. Iteration Training loss: 5.109279123830143\n",
      "[ 0.25106148 -0.2215032 ] -12.711199999998428\n",
      "16650. Iteration Training loss: 2.8140575803087313\n",
      "[ 0.45806436 -0.02862566] -12.745699999998426\n",
      "16700. Iteration Training loss: 4.60492084689274\n",
      "[0.66426366 0.19178497] -12.779899999998426\n",
      "16750. Iteration Training loss: 5.070187110764131\n",
      "[ 0.25565654 -0.22324428] -12.823499999998425\n",
      "16800. Iteration Training loss: 3.111050952860794\n",
      "[ 0.46685334 -0.01114368] -12.85769999999842\n",
      "16850. Iteration Training loss: 4.604920732899536\n",
      "[0.66975805 0.19051174] -12.891899999998422\n",
      "16900. Iteration Training loss: 5.402448663746438\n",
      "[ 0.24852883 -0.23197316] -12.935799999998412\n",
      "16950. Iteration Training loss: 2.8363040065905336\n",
      "[ 0.45939852 -0.0241618 ] -12.969999999998414\n",
      "17000. Iteration Training loss: 4.604937448477132\n",
      "[0.65991273 0.20127997] -13.003899999998417\n",
      "17050. Iteration Training loss: 5.273362950904934\n",
      "[ 0.25183124 -0.22674531] -13.047499999998418\n",
      "17100. Iteration Training loss: 2.9627521992446453\n",
      "[ 0.46411255 -0.01634704] -13.08159999999842\n",
      "17150. Iteration Training loss: 4.60494723418258\n",
      "[0.66732495 0.1996812 ] -13.115599999998423\n",
      "17200. Iteration Training loss: 4.96228137011361\n",
      "[ 0.26236585 -0.22055738] -13.158599999998426\n",
      "17250. Iteration Training loss: 2.8728575461408234\n",
      "[ 0.46147897 -0.01923749] -13.192499999998429\n",
      "17300. Iteration Training loss: 4.6049493331878555\n",
      "[0.66927184 0.20101291] -13.225899999998433\n",
      "17350. Iteration Training loss: 4.874910598524154\n",
      "[ 0.26704227 -0.21980975] -13.268799999998436\n",
      "17400. Iteration Training loss: 2.866033684052458\n",
      "[ 0.45795574 -0.01539466] -13.303099999998436\n",
      "17450. Iteration Training loss: 4.604939358452565\n",
      "[0.66574861 0.20485574] -13.33649999999844\n",
      "17500. Iteration Training loss: 5.1567060414920896\n",
      "[ 0.26026133 -0.22552677] -13.379499999998442\n",
      "17550. Iteration Training loss: 2.9107375708844887\n",
      "[ 0.45642662 -0.00927405] -13.413099999998446\n",
      "17600. Iteration Training loss: 4.604914687501983\n",
      "[0.67084945 0.20150481] -13.446899999998449\n",
      "17650. Iteration Training loss: 5.169089161209618\n",
      "[ 0.25963531 -0.22385368] -13.489899999998451\n",
      "17700. Iteration Training loss: 2.8700550764894444\n",
      "[ 0.46712049 -0.0179761 ] -13.523499999998455\n",
      "17750. Iteration Training loss: 4.604942587409561\n",
      "[0.66932033 0.20762542] -13.556899999998459\n",
      "17800. Iteration Training loss: 5.087529106672552\n",
      "[ 0.26383304 -0.2227571 ] -13.599899999998462\n",
      "17850. Iteration Training loss: 2.9140004526592462\n",
      "[ 0.46559137 -0.01185549] -13.633499999998465\n",
      "17900. Iteration Training loss: 4.604909173128788\n",
      "[0.67367483 0.20396926] -13.667099999998468\n",
      "17950. Iteration Training loss: 5.147093483270847\n",
      "[ 0.25686765 -0.21603812] -13.710099999998471\n",
      "18000. Iteration Training loss: 2.9003605821569582\n",
      "[ 0.46435283 -0.01016054] -13.743699999998475\n",
      "18050. Iteration Training loss: 4.604937669023892\n",
      "[0.67214571 0.21008986] -13.777099999998478\n",
      "18100. Iteration Training loss: 5.072214817202313\n",
      "[ 0.26093156 -0.21526863] -13.820099999998481\n",
      "18150. Iteration Training loss: 2.939670358333998\n",
      "[ 0.46268988 -0.00436702] -13.853699999998485\n",
      "18200. Iteration Training loss: 4.604851466922431\n",
      "[0.6704742  0.20648414] -13.887299999998488\n",
      "18250. Iteration Training loss: 5.072999844720701\n",
      "[ 0.26528605 -0.21892479] -13.930299999998491\n",
      "18300. Iteration Training loss: 2.906031890603726\n",
      "[ 0.46704438 -0.00802318] -13.963899999998494\n",
      "18350. Iteration Training loss: 4.6049282453572715\n",
      "[0.67483725 0.21222722] -13.997299999998498\n",
      "18400. Iteration Training loss: 5.0316482105595375\n",
      "[ 0.2636231  -0.21313127] -14.040299999998501\n",
      "18450. Iteration Training loss: 2.9433473611611998\n",
      "[ 0.46545151 -0.00242643] -14.073899999998504\n",
      "18500. Iteration Training loss: 4.604943330537109\n",
      "[0.67352475 0.2170369 ] -14.107299999998508\n",
      "18550. Iteration Training loss: 5.010564151347614\n",
      "[ 0.26266106 -0.20930544] -14.150299999998511\n",
      "18600. Iteration Training loss: 2.7872809862077292\n",
      "[ 0.4615822  -0.00914429] -14.183999999998514\n",
      "18650. Iteration Training loss: 4.60478439640539\n",
      "[0.66979562 0.2099255 ] -14.217399999998518\n",
      "18700. Iteration Training loss: 5.2552779574721225\n",
      "[ 0.25900202 -0.2166136 ] -14.260399999998521\n",
      "18750. Iteration Training loss: 2.787057426068478\n",
      "[ 0.46125098 -0.00708937] -14.293999999998524\n",
      "18800. Iteration Training loss: 4.604771277179966\n",
      "[0.66953449 0.21178365] -14.327399999998528\n",
      "18850. Iteration Training loss: 4.976351408816573\n",
      "[ 0.26779555 -0.21041639] -14.370299999998531\n",
      "18900. Iteration Training loss: 2.9361686564220344\n",
      "[ 0.46997442 -0.00069539] -14.403899999998535\n",
      "18950. Iteration Training loss: 4.604912110549256\n",
      "[0.67825793 0.21817763] -14.437299999998539\n",
      "19000. Iteration Training loss: 4.9766644045216015\n",
      "[ 0.26753442 -0.20855824] -14.480299999998541\n",
      "19050. Iteration Training loss: 2.9324065319247965\n",
      "[0.46978337 0.00096599] -14.513899999998545\n",
      "19100. Iteration Training loss: 4.604901612790431\n",
      "[0.67806688 0.21983902] -14.547299999998549\n",
      "19150. Iteration Training loss: 5.262694054036764\n",
      "[ 0.2582887  -0.21123591] -14.590399999998551\n",
      "19200. Iteration Training loss: 2.7805077560733564\n",
      "[ 0.46046757 -0.00151491] -14.623999999998555\n",
      "19250. Iteration Training loss: 4.6047291106233885\n",
      "[0.66875108 0.21735811] -14.657399999998558\n",
      "19300. Iteration Training loss: 4.9472542535711055\n",
      "[ 0.27280909 -0.21006272] -14.700299999998562\n",
      "19350. Iteration Training loss: 2.912712257186623\n",
      "[ 0.47505805 -0.0005385 ] -14.733899999998565\n",
      "19400. Iteration Training loss: 4.604866746062914\n",
      "[0.68334156 0.21833453] -14.767299999998569\n",
      "19450. Iteration Training loss: 4.947386990546165\n",
      "[ 0.27254796 -0.20820457] -14.810299999998572\n",
      "19500. Iteration Training loss: 2.9110007079070717\n",
      "[0.47479691 0.00131966] -14.843899999998575\n",
      "19550. Iteration Training loss: 4.6048564318714345\n",
      "[0.68308042 0.22019268] -14.877299999998579\n",
      "19600. Iteration Training loss: 5.232602580743594\n",
      "[ 0.26330224 -0.21088225] -14.920399999998581\n",
      "19650. Iteration Training loss: 2.7583622731554907\n",
      "[ 0.4655512  -0.00135802] -14.953999999998585\n",
      "19700. Iteration Training loss: 4.604651210387177\n",
      "[0.67383471 0.21751501] -14.987399999998589\n",
      "19750. Iteration Training loss: 5.2343470444024875\n",
      "[ 0.2630411  -0.20902409] -15.030399999998592\n",
      "19800. Iteration Training loss: 2.7561729898066516\n",
      "[0.46529006 0.00050014] -15.063999999998595\n",
      "19850. Iteration Training loss: 4.604633506575212\n",
      "[0.67357357 0.21937316] -15.097399999998599\n",
      "19900. Iteration Training loss: 4.950212898871183\n",
      "[ 0.27183464 -0.20282688] -15.140299999998602\n",
      "19950. Iteration Training loss: 2.903819287967879\n",
      "[0.47408359 0.00669735] -15.173899999998605\n",
      "20000. Iteration Training loss: 4.604820994303881\n",
      "[0.6823671  0.22557037] -15.20729999999861\n",
      "20050. Iteration Training loss: 4.950470778290391\n",
      "[ 0.2715735  -0.20096873] -15.250299999998612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20100. Iteration Training loss: 2.73865085714379\n",
      "[ 0.47056474 -0.00100435] -15.283999999998615\n",
      "20150. Iteration Training loss: 4.604809078990602\n",
      "[0.68210597 0.22742853] -15.31729999999862\n",
      "20200. Iteration Training loss: 4.953026749855265\n",
      "[ 0.27138245 -0.19930734] -15.360299999998622\n",
      "20250. Iteration Training loss: 2.7344931443888805\n",
      "[0.47037369 0.00065703] -15.393999999998625\n",
      "20300. Iteration Training loss: 4.604538709822035\n",
      "[0.6786572  0.21953006] -15.427399999998629\n",
      "20350. Iteration Training loss: 4.976947468170502\n",
      "[ 0.26759845 -0.19449412] -15.470199999998632\n",
      "20400. Iteration Training loss: 2.8251316733958816\n",
      "[0.47530847 0.00556402] -15.503799999998636\n",
      "20450. Iteration Training loss: 4.604571687520759\n",
      "[0.68332618 0.21999498] -15.53719999999864\n",
      "20500. Iteration Training loss: 5.069161586938463\n",
      "[ 0.26312175 -0.19222173] -15.580099999998643\n",
      "20550. Iteration Training loss: 2.8924704244863593\n",
      "[0.4695306  0.01627294] -15.613499999998645\n",
      "20600. Iteration Training loss: 4.604660782958778\n",
      "[0.6830649  0.22627018] -15.646899999998649\n",
      "20650. Iteration Training loss: 5.134446002660615\n",
      "[ 0.27207559 -0.20490775] -15.689899999998651\n",
      "20700. Iteration Training loss: 2.9120710945659147\n",
      "[0.47971616 0.01230414] -15.723299999998655\n",
      "20750. Iteration Training loss: 4.604709315335412\n",
      "[0.67876451 0.23392712] -15.75659999999866\n",
      "20800. Iteration Training loss: 4.814165525387895\n",
      "[ 0.2766196  -0.19232145] -15.799499999998663\n",
      "20850. Iteration Training loss: 2.8535002307011332\n",
      "[0.47758999 0.01139341] -15.833099999998666\n",
      "20900. Iteration Training loss: 4.604556033428106\n",
      "[0.69112429 0.22139065] -15.86649999999867\n",
      "20950. Iteration Training loss: 4.954612407145138\n",
      "[ 0.2751002 -0.1959134] -15.909299999998673\n",
      "21000. Iteration Training loss: 2.7867972086473407\n",
      "[0.48259995 0.00473504] -15.942899999998676\n",
      "21050. Iteration Training loss: 4.604689641883663\n",
      "[0.6855128  0.23323666] -15.97609999999868\n",
      "21100. Iteration Training loss: 4.9975047217770046\n",
      "[ 0.28004007 -0.202375  ] -16.01909999999867\n",
      "21150. Iteration Training loss: 2.871978226869313\n",
      "[0.47691837 0.01677782] -16.052499999998645\n",
      "21200. Iteration Training loss: 4.604557510142225\n",
      "[0.69045266 0.22677506] -16.085899999998617\n",
      "21250. Iteration Training loss: 4.922710205906707\n",
      "[ 0.27435849 -0.19033222] -16.12869999999859\n",
      "21300. Iteration Training loss: 2.806243262412322\n",
      "[0.48185823 0.01031622] -16.162299999998567\n",
      "21350. Iteration Training loss: 4.60457090055179\n",
      "[0.68478564 0.2335887 ] -16.19549999999854\n",
      "21400. Iteration Training loss: 4.942436707500569\n",
      "[ 0.27685834 -0.19266821] -16.238399999998514\n",
      "21450. Iteration Training loss: 2.806687938597028\n",
      "[0.48191807 0.01210583] -16.27189999999849\n",
      "21500. Iteration Training loss: 4.604554723793211\n",
      "[0.68484548 0.23537831] -16.30509999999846\n",
      "21550. Iteration Training loss: 4.936465327291625\n",
      "[ 0.27691818 -0.1908786 ] -16.347999999998436\n",
      "21600. Iteration Training loss: 2.8401644144323597\n",
      "[0.4795379  0.01802106] -16.38139999999841\n",
      "21650. Iteration Training loss: 4.6045746116675454\n",
      "[0.68819217 0.23626951] -16.414599999998384\n",
      "21700. Iteration Training loss: 4.95037983374014\n",
      "[ 0.27783942 -0.19109094] -16.45739999999836\n",
      "21750. Iteration Training loss: 2.821443615132645\n",
      "[0.48045913 0.01780872] -16.490799999998334\n",
      "21800. Iteration Training loss: 4.604604879589507\n",
      "[0.68830509 0.23981247] -16.523999999998306\n",
      "21850. Iteration Training loss: 4.907517373553811\n",
      "[ 0.27795234 -0.18754798] -16.56679999999828\n",
      "21900. Iteration Training loss: 2.8470343443448796\n",
      "[0.48057206 0.02135168] -16.600199999998257\n",
      "21950. Iteration Training loss: 4.60455256625208\n",
      "[0.68922633 0.23960013] -16.63339999999823\n",
      "22000. Iteration Training loss: 4.9208470647944065\n",
      "[ 0.27326989 -0.18117971] -16.675999999998204\n",
      "22050. Iteration Training loss: 2.8649107965055465\n",
      "[0.47588961 0.02771995] -16.70939999999818\n",
      "22100. Iteration Training loss: 4.604572037808168\n",
      "[0.68454388 0.2459684 ] -16.742599999998152\n",
      "22150. Iteration Training loss: 4.935364516211145\n",
      "[ 0.27419112 -0.18139205] -16.785399999998127\n",
      "22200. Iteration Training loss: 2.875096223821462\n",
      "[0.48179948 0.02604211] -16.818799999998102\n",
      "22250. Iteration Training loss: 4.604570402446765\n",
      "[0.69045375 0.24429056] -16.851999999998075\n",
      "22300. Iteration Training loss: 4.859340712960668\n",
      "[ 0.280101   -0.18306988] -16.89479999999805\n",
      "22350. Iteration Training loss: 2.8966713426320165\n",
      "[0.47395336 0.03486538] -16.927899999998026\n",
      "22400. Iteration Training loss: 4.604404706747462\n",
      "[0.67933535 0.24878311] -16.961199999997998\n",
      "22450. Iteration Training loss: 4.74933035690434\n",
      "[ 0.27796717 -0.1740415 ] -17.003899999997973\n",
      "22500. Iteration Training loss: 2.8432069516735665\n",
      "[0.47731461 0.03052744] -17.037399999997948\n",
      "22550. Iteration Training loss: 4.604594961828213\n",
      "[0.6885175 0.251436 ] -17.07049999999792\n",
      "22600. Iteration Training loss: 4.845500277755653\n",
      "[ 0.27500108 -0.17346945] -17.113199999997896\n",
      "22650. Iteration Training loss: 2.8940952100286084\n",
      "[0.47445712 0.03788521] -17.14649999999787\n",
      "22700. Iteration Training loss: 4.604712397331742\n",
      "[0.68748498 0.25978327] -17.179499999997844\n",
      "22750. Iteration Training loss: 4.704647779859422\n",
      "[ 0.27506987 -0.16580328] -17.22209999999782\n",
      "22800. Iteration Training loss: 2.6307663857576284\n",
      "[0.45987906 0.03177954] -17.255599999997795\n",
      "22850. Iteration Training loss: 4.60468730202271\n",
      "[0.6853828  0.26243762] -17.288399999997768\n",
      "22900. Iteration Training loss: 4.707615941817755\n",
      "[ 0.27613135 -0.16560393] -17.331099999997743\n",
      "22950. Iteration Training loss: 2.7768953847311826\n",
      "[0.46992513 0.03651471] -17.36449999999772\n",
      "23000. Iteration Training loss: 4.604728748103297\n",
      "[0.69170981 0.26257137] -17.397399999997692\n",
      "23050. Iteration Training loss: 4.4120417934489335\n",
      "[ 0.28090666 -0.15720431] -17.439699999997668\n",
      "23100. Iteration Training loss: 2.8266301519408668\n",
      "[0.46630632 0.0443383 ] -17.472899999997644\n",
      "23150. Iteration Training loss: 4.604824518140683\n",
      "[0.69211923 0.27029635] -17.505599999997617\n",
      "23200. Iteration Training loss: 4.461175909682182\n",
      "[ 0.277396   -0.15393388] -17.547999999997593\n",
      "23250. Iteration Training loss: 2.717145002842257\n",
      "[0.45987332 0.04322138] -17.58129999999757\n",
      "23300. Iteration Training loss: 4.604730508004212\n",
      "[0.68304519 0.27345191] -17.613899999997543\n",
      "23350. Iteration Training loss: 4.533130382326644\n",
      "[ 0.27702524 -0.15490232] -17.656399999997518\n",
      "23400. Iteration Training loss: 2.673658157195472\n",
      "[0.45950256 0.04225294] -17.689699999997494\n",
      "23450. Iteration Training loss: 4.604780403399541\n",
      "[0.68559676 0.27687081] -17.722199999997468\n",
      "23500. Iteration Training loss: 4.56988889826017\n",
      "[ 0.27401345 -0.15159829] -17.764699999997443\n",
      "23550. Iteration Training loss: 2.8424235258816175\n",
      "[0.46750158 0.04986393] -17.797799999997427\n",
      "23600. Iteration Training loss: 4.604732797961133\n",
      "[0.68416315 0.27763787] -17.830399999997404\n",
      "23650. Iteration Training loss: 4.57739264079616\n",
      "[ 0.27460607 -0.15106011] -17.872799999997387\n",
      "23700. Iteration Training loss: 2.7083740226060677\n",
      "[0.45955768 0.04817439] -17.905999999997366\n",
      "23750. Iteration Training loss: 4.604703793316677\n",
      "[0.68475578 0.27817605] -17.938499999997347\n",
      "23800. Iteration Training loss: 4.584887919316461\n",
      "[ 0.2751987  -0.15052193] -17.98089999999733\n",
      "23850. Iteration Training loss: 2.797425449197842\n",
      "[0.46357151 0.05313353] -18.01399999999731\n",
      "23900. Iteration Training loss: 4.604672860626969\n",
      "[0.68534841 0.27871423] -18.04659999999729\n",
      "23950. Iteration Training loss: 4.484844913617433\n",
      "[ 0.27359833 -0.14340316] -18.08889999999727\n",
      "24000. Iteration Training loss: 2.8807816808985804\n",
      "[0.46708647 0.05805906] -18.121999999997254\n",
      "24050. Iteration Training loss: 4.604769271917166\n",
      "[0.68886337 0.28363976] -18.154599999997235\n",
      "24100. Iteration Training loss: 4.406128660883619\n",
      "[ 0.27930628 -0.14505821] -18.196999999997217\n",
      "24150. Iteration Training loss: 2.904250341947091\n",
      "[0.46503806 0.06286972] -18.229999999997197\n",
      "24200. Iteration Training loss: 4.604786126359777\n",
      "[0.68681495 0.28845042] -18.262599999997178\n",
      "24250. Iteration Training loss: 4.375026469325183\n",
      "[ 0.27725787 -0.14024756] -18.30499999999716\n",
      "24300. Iteration Training loss: 2.826118986758442\n",
      "[0.45956844 0.06325942] -18.33809999999714\n",
      "24350. Iteration Training loss: 4.604693851079297\n",
      "[0.68461262 0.28745075] -18.370599999997133\n",
      "24400. Iteration Training loss: 4.503042946295441\n",
      "[ 0.27505554 -0.14124723] -18.412999999997115\n",
      "24450. Iteration Training loss: 2.6790409509009003\n",
      "[0.45022266 0.06190339] -18.445899999997106\n",
      "24500. Iteration Training loss: 4.6048893194468095\n",
      "[0.69316657 0.29606895] -18.47759999999712\n",
      "24550. Iteration Training loss: 4.078588326004836\n",
      "[ 0.28001646 -0.12952035] -18.51929999999715\n",
      "24600. Iteration Training loss: 2.778241647368753\n",
      "[0.46071837 0.06268054] -18.55239999999714\n",
      "24650. Iteration Training loss: 4.604949007936179\n",
      "[0.68873035 0.30703316] -18.58419999999714\n",
      "24700. Iteration Training loss: 4.047850441794019\n",
      "[ 0.28146165 -0.12849351] -18.625899999997166\n",
      "24750. Iteration Training loss: 2.283923354061622\n",
      "[0.43541283 0.04926411] -18.658599999997193\n",
      "24800. Iteration Training loss: 4.604934725675303\n",
      "[0.68893172 0.30781256] -18.689799999997216\n",
      "24850. Iteration Training loss: 4.133774580274274\n",
      "[ 0.27549523 -0.12401286] -18.731499999997244\n",
      "24900. Iteration Training loss: 2.2964139836684567\n",
      "[0.43534603 0.0520268 ] -18.764199999997267\n",
      "24950. Iteration Training loss: 4.604943794776593\n",
      "[0.69503272 0.30687401] -18.79539999999729\n",
      "25000. Iteration Training loss: 4.022367221690147\n",
      "[ 0.28159623 -0.12495141] -18.837099999997317\n",
      "25050. Iteration Training loss: 2.284724971709992\n",
      "[0.4355474 0.0528062] -18.869799999997344\n",
      "25100. Iteration Training loss: 4.60492916927245\n",
      "[0.69523409 0.30765341] -18.900999999997367\n",
      "25150. Iteration Training loss: 4.0341549187842105\n",
      "[ 0.2817976  -0.12417201] -18.942699999997394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25200. Iteration Training loss: 2.273043770573422\n",
      "[0.43574877 0.05358561] -18.97539999999742\n",
      "25250. Iteration Training loss: 4.604907283867586\n",
      "[0.68926767 0.31213406] -19.006599999997444\n",
      "25300. Iteration Training loss: 4.120998904566511\n",
      "[ 0.27583117 -0.11969136] -19.04829999999747\n",
      "25350. Iteration Training loss: 3.3596966684178944\n",
      "[0.48440483 0.09491383] -19.080699999997492\n",
      "25400. Iteration Training loss: 4.604716161658467\n",
      "[0.6747168  0.31200547] -19.11289999999752\n",
      "25450. Iteration Training loss: 4.040562191395028\n",
      "[ 0.27748672 -0.11721344] -19.154699999997547\n",
      "25500. Iteration Training loss: 2.457052835127369\n",
      "[0.44513892 0.06276811] -19.187199999997585\n",
      "25550. Iteration Training loss: 4.604886775609654\n",
      "[0.69005455 0.31460779] -19.21849999999762\n",
      "25600. Iteration Training loss: 3.936135537719799\n",
      "[ 0.28359449 -0.1184047 ] -19.259999999997653\n",
      "25650. Iteration Training loss: 2.2844028955699334\n",
      "[0.43552684 0.05997143] -19.29259999999769\n",
      "25700. Iteration Training loss: 4.6048812104382995\n",
      "[0.69266363 0.31471032] -19.323499999997722\n",
      "25750. Iteration Training loss: 3.873787075214315\n",
      "[ 0.28021419 -0.11153729] -19.364999999997757\n",
      "25800. Iteration Training loss: 2.3193319023079666\n",
      "[0.43778326 0.06241076] -19.39739999999779\n",
      "25850. Iteration Training loss: 4.604913395139308\n",
      "[0.69549556 0.31782826] -19.428299999997822\n",
      "25900. Iteration Training loss: 3.9383918946168643\n",
      "[ 0.28567697 -0.11780673] -19.469699999997857\n",
      "25950. Iteration Training loss: 2.2667432458609853\n",
      "[0.43765376 0.06052117] -19.502099999997892\n",
      "26000. Iteration Training loss: 4.604913441001623\n",
      "[0.69598234 0.31984417] -19.532999999997923\n",
      "26050. Iteration Training loss: 3.983444951820904\n",
      "[ 0.28046791 -0.11273183] -19.574399999997958\n",
      "26100. Iteration Training loss: 2.267882041419039\n",
      "[0.43515872 0.06440803] -19.60669999999799\n",
      "26150. Iteration Training loss: 4.6048671545512\n",
      "[0.69558502 0.31863749] -19.637499999998017\n",
      "26200. Iteration Training loss: 3.7493464117721884\n",
      "[ 0.28589404 -0.10884638] -19.678699999998045\n",
      "26250. Iteration Training loss: 2.160366120890896\n",
      "[0.4311405  0.06173659] -19.711099999998076\n",
      "26300. Iteration Training loss: 4.604893495628737\n",
      "[0.69498517 0.32348982] -19.741799999998104\n",
      "26350. Iteration Training loss: 3.8989860531851175\n",
      "[ 0.28228831 -0.10895335] -19.783099999998136\n",
      "26400. Iteration Training loss: 2.2922543685664447\n",
      "[0.43697911 0.06818652] -19.815399999998167\n",
      "26450. Iteration Training loss: 4.604871793448549\n",
      "[0.69740541 0.32241598] -19.846199999998195\n",
      "26500. Iteration Training loss: 3.899851223251582\n",
      "[ 0.28470855 -0.1100272 ] -19.887499999998226\n",
      "26550. Iteration Training loss: 2.1839795625166816\n",
      "[0.43296089 0.06551507] -19.919799999998254\n",
      "26600. Iteration Training loss: 4.604897707541897\n",
      "[0.69680557 0.32726831] -19.950499999998282\n",
      "26650. Iteration Training loss: 3.814124230478878\n",
      "[ 0.2841087  -0.10517487] -19.991799999998314\n",
      "26700. Iteration Training loss: 2.3169092674464493\n",
      "[0.4387995 0.071965 ] -20.024099999998345\n",
      "26750. Iteration Training loss: 4.604884309460726\n",
      "[0.69017974 0.33308752] -20.05479999999837\n",
      "26800. Iteration Training loss: 3.879552712101027\n",
      "[ 0.27748287 -0.09935566] -20.0960999999984\n",
      "26850. Iteration Training loss: 2.2989237616907157\n",
      "[0.43217368 0.07778421] -20.128399999998432\n",
      "26900. Iteration Training loss: 4.604861898634739\n",
      "[0.69259998 0.33201367] -20.15919999999846\n",
      "26950. Iteration Training loss: 3.8802421689731195\n",
      "[ 0.27990312 -0.1004295 ] -20.20049999999849\n",
      "27000. Iteration Training loss: 2.2861335084352583\n",
      "[0.43459392 0.07671036] -20.232799999998523\n",
      "27050. Iteration Training loss: 4.604737695492955\n",
      "[0.68858177 0.32934223] -20.263599999998547\n",
      "27100. Iteration Training loss: 3.94075522286733\n",
      "[ 0.27520553 -0.09628209] -20.30469999999856\n",
      "27150. Iteration Training loss: 2.1915955587683196\n",
      "[0.43266373 0.07342946] -20.33709999999859\n",
      "27200. Iteration Training loss: 4.604838904600751\n",
      "[0.69609592 0.33261821] -20.367799999998613\n",
      "27250. Iteration Training loss: 3.9358266703144467\n",
      "[ 0.27612143 -0.09566603] -20.408899999998628\n",
      "27300. Iteration Training loss: 2.2918396706202118\n",
      "[0.43658552 0.07900481] -20.441199999998652\n",
      "27350. Iteration Training loss: 4.6048824362603265\n",
      "[0.69604631 0.33823973] -20.471799999998662\n",
      "27400. Iteration Training loss: 3.761481612263582\n",
      "[ 0.28249518 -0.09499229] -20.513099999998687\n",
      "27450. Iteration Training loss: 2.1860287519336605\n",
      "[0.42690248 0.08134721] -20.545299999998704\n",
      "27500. Iteration Training loss: 4.604825215064723\n",
      "[0.69522024 0.33696684] -20.57599999999872\n",
      "27550. Iteration Training loss: 3.5388447740359847\n",
      "[ 0.28698226 -0.09072446] -20.617099999998736\n",
      "27600. Iteration Training loss: 2.32390072596923\n",
      "[0.4366876  0.08461039] -20.64929999999875\n",
      "27650. Iteration Training loss: 4.604862424344649\n",
      "[0.69513434 0.34199413] -20.679899999998756\n",
      "27700. Iteration Training loss: 3.646892897760274\n",
      "[ 0.28118582 -0.08725699] -20.72099999999877\n",
      "27750. Iteration Training loss: 2.172317663186108\n",
      "[0.42788528 0.08311856] -20.753299999998788\n",
      "27800. Iteration Training loss: 4.604806229360832\n",
      "[0.69277048 0.34209989] -20.7838999999988\n",
      "27850. Iteration Training loss: 3.7123523271290186\n",
      "[ 0.28121117 -0.088024  ] -20.825099999998816\n",
      "27900. Iteration Training loss: 2.3056660129937536\n",
      "[0.433744   0.08920652] -20.857199999998826\n",
      "27950. Iteration Training loss: 4.604782873188046\n",
      "[0.69201232 0.34352663] -20.887799999998833\n",
      "28000. Iteration Training loss: 3.6099455219598724\n",
      "[ 0.28060993 -0.08278626] -20.928799999998844\n",
      "28050. Iteration Training loss: 2.2057668970571225\n",
      "[0.42967711 0.08746389] -20.960999999998858\n",
      "28100. Iteration Training loss: 4.6046783186380535\n",
      "[0.68839201 0.34286434] -20.99159999999887\n",
      "28150. Iteration Training loss: 3.834300841220553\n",
      "[ 0.27654305 -0.08452889] -21.032599999998876\n",
      "28200. Iteration Training loss: 2.315269419893948\n",
      "[0.43505457 0.09227816] -21.06469999999889\n",
      "28250. Iteration Training loss: 4.604832182804957\n",
      "[0.69132945 0.35180421] -21.0951999999989\n",
      "28300. Iteration Training loss: 3.754415575225439\n",
      "[ 0.2764746  -0.08054831] -21.13629999999891\n",
      "28350. Iteration Training loss: 2.458390763050633\n",
      "[0.43799201 0.10121804] -21.168299999998922\n",
      "28400. Iteration Training loss: 4.604835249283052\n",
      "[0.69081444 0.35470445] -21.198899999998932\n",
      "28450. Iteration Training loss: 3.4715907889410946\n",
      "[ 0.28239805 -0.07605047] -21.239999999998947\n",
      "28500. Iteration Training loss: 2.1828192430987254\n",
      "[0.42548653 0.09462315] -21.272199999998957\n",
      "28550. Iteration Training loss: 4.60464996801291\n",
      "[0.68519399 0.3507875 ] -21.302799999998975\n",
      "28600. Iteration Training loss: 3.6883276939786556\n",
      "[ 0.27887715 -0.07810953] -21.343799999998982\n",
      "28650. Iteration Training loss: 2.344437375820901\n",
      "[0.42968293 0.10369835] -21.375799999999\n",
      "28700. Iteration Training loss: 4.604634468126361\n",
      "[0.68521178 0.35243058] -21.406499999999014\n",
      "28750. Iteration Training loss: 3.6870543694329263\n",
      "[ 0.27607483 -0.07391332] -21.447399999999025\n",
      "28800. Iteration Training loss: 2.348129575843806\n",
      "[0.43131967 0.10436651] -21.47939999999904\n",
      "28850. Iteration Training loss: 4.604513327829915\n",
      "[0.6862086  0.34936877] -21.510099999999053\n",
      "28900. Iteration Training loss: 3.682467259856068\n",
      "[ 0.2748118  -0.07114062] -21.550899999999064\n",
      "28950. Iteration Training loss: 2.232669454390967\n",
      "[0.42415099 0.10428447] -21.582899999999082\n",
      "29000. Iteration Training loss: 4.6048377786421675\n",
      "[0.69420215 0.36185223] -21.613299999999096\n",
      "29050. Iteration Training loss: 3.592324890149168\n",
      "[ 0.27706    -0.06920433] -21.654299999999118\n",
      "29100. Iteration Training loss: 2.3699404301212237\n",
      "[0.42967147 0.11055147] -21.686199999999136\n",
      "29150. Iteration Training loss: 4.6048280901460155\n",
      "[0.69099787 0.36574091] -21.716699999999154\n",
      "29200. Iteration Training loss: 3.3465776442687227\n",
      "[ 0.28002777 -0.06308947] -21.757699999999176\n",
      "29250. Iteration Training loss: 2.3750093080691075\n",
      "[0.42944662 0.11278419] -21.789699999999197\n",
      "29300. Iteration Training loss: 4.6045339248941595\n",
      "[0.68151543 0.36033957] -21.820299999999214\n",
      "29350. Iteration Training loss: 3.5470666722901787\n",
      "[ 0.27320414 -0.06127482] -21.86099999999923\n",
      "29400. Iteration Training loss: 1.9776695554210915\n",
      "[0.41249315 0.10237925] -21.893099999999247\n",
      "29450. Iteration Training loss: 4.604149343349559\n",
      "[0.66982841 0.35984783] -21.92349999999927\n",
      "29500. Iteration Training loss: 5.336567557443188\n",
      "[ 0.22990546 -0.0703588 ] -21.964599999999287\n",
      "29550. Iteration Training loss: 3.6308339817177915\n",
      "[0.49171587 0.15763005] -21.995599999999303\n",
      "29600. Iteration Training loss: 4.593213316073454\n",
      "[0.62768597 0.33060541] -22.02759999999932\n",
      "29650. Iteration Training loss: 5.300816843354401\n",
      "[ 0.23185077 -0.06981528] -22.067899999999344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29700. Iteration Training loss: 3.2313888168636025\n",
      "[0.46076079 0.1510232 ] -22.098999999999368\n",
      "29750. Iteration Training loss: 4.593218164104854\n",
      "[0.61950244 0.33862629] -22.130699999999383\n",
      "29800. Iteration Training loss: 4.955274722826526\n",
      "[ 0.24600977 -0.07232752] -22.171199999999395\n",
      "29850. Iteration Training loss: 2.1783934893089305\n",
      "[0.41502333 0.11873713] -22.202999999999417\n",
      "29900. Iteration Training loss: 4.604998560454841\n",
      "[0.70079126 0.38661102] -22.233099999999432\n",
      "29950. Iteration Training loss: 4.504983321556965\n",
      "[ 0.24708173 -0.05732333] -22.27419999999945\n",
      "30000. Iteration Training loss: 1.979980905542441\n",
      "[0.41055676 0.11134248] -22.30619999999947\n",
      "30050. Iteration Training loss: 4.60382044657851\n",
      "[0.65659017 0.37185658] -22.33659999999949\n",
      "30100. Iteration Training loss: 2.9984671107004757\n",
      "[ 0.28260106 -0.04659774] -22.37719999999951\n",
      "30150. Iteration Training loss: 1.995991361577285\n",
      "[0.40900252 0.11551827] -22.409299999999533\n",
      "30200. Iteration Training loss: 4.60219654132844\n",
      "[0.64217109 0.36550485] -22.439899999999554\n",
      "30250. Iteration Training loss: 3.423470840102885\n",
      "[ 0.26320015 -0.03948125] -22.480099999999574\n",
      "30300. Iteration Training loss: 3.7326547630392835\n",
      "[0.44231396 0.20927997] -22.510799999999595\n",
      "30350. Iteration Training loss: 4.604872383588263\n",
      "[0.68167158 0.39404742] -22.542399999999617\n",
      "30400. Iteration Training loss: 2.8821193149936755\n",
      "[ 0.27670749 -0.03484645] -22.58319999999964\n",
      "30450. Iteration Training loss: 3.6920840883137567\n",
      "[0.48316346 0.17862537] -22.61419999999966\n",
      "30500. Iteration Training loss: 4.593563064165381\n",
      "[0.61792484 0.35207917] -22.645999999999677\n",
      "30550. Iteration Training loss: 3.824916352936196\n",
      "[ 0.26514003 -0.05048486] -22.686099999999698\n",
      "30600. Iteration Training loss: 3.1358275157893725\n",
      "[0.45061514 0.16184782] -22.717299999999724\n",
      "30650. Iteration Training loss: 4.591653397360416\n",
      "[0.62452915 0.3454588 ] -22.748899999999747\n",
      "30700. Iteration Training loss: 4.425417664014372\n",
      "[ 0.24183475 -0.04203996] -22.78879999999976\n",
      "30750. Iteration Training loss: 3.186499128353473\n",
      "[0.46346598 0.15774869] -22.81999999999977\n",
      "30800. Iteration Training loss: 4.59534615863371\n",
      "[0.62892346 0.35359537] -22.851199999999782\n",
      "30850. Iteration Training loss: 3.8012881257915656\n",
      "[ 0.2615806  -0.04335746] -22.891099999999792\n",
      "30900. Iteration Training loss: 3.1638244049489384\n",
      "[0.46553178 0.15637677] -22.922299999999815\n",
      "30950. Iteration Training loss: 4.583796140383599\n",
      "[0.614772   0.34308282] -22.95359999999983\n",
      "31000. Iteration Training loss: 5.8415897581491345\n",
      "[ 0.22045711 -0.06542285] -22.993799999999847\n",
      "31050. Iteration Training loss: 3.150614192469959\n",
      "[0.46383599 0.15834828] -23.02459999999987\n",
      "31100. Iteration Training loss: 4.57494271370682\n",
      "[0.61335403 0.3349424 ] -23.056099999999887\n",
      "31150. Iteration Training loss: 5.570841619870193\n",
      "[ 0.22700237 -0.05985031] -23.09609999999991\n",
      "31200. Iteration Training loss: 2.971773369258949\n",
      "[0.43240839 0.17102503] -23.126799999999932\n",
      "31250. Iteration Training loss: 4.596721455330907\n",
      "[0.61862299 0.37173789] -23.15769999999995\n",
      "31300. Iteration Training loss: 3.5777567651083224\n",
      "[ 0.26634928 -0.03756353] -23.197699999999966\n",
      "31350. Iteration Training loss: 2.6814411354867334\n",
      "[0.44174101 0.14683504] -23.22919999999999\n",
      "31400. Iteration Training loss: 4.6043179446535785\n",
      "[0.67508963 0.39047095] -23.259600000000013\n",
      "31450. Iteration Training loss: 3.544527582367323\n",
      "[ 0.26767008 -0.03655043] -23.300000000000033\n",
      "31500. Iteration Training loss: 1.130606840367167\n",
      "[0.37771109 0.09464247] -23.332200000000046\n",
      "31550. Iteration Training loss: 4.596339494959375\n",
      "[0.61845129 0.37525943] -23.361900000000063\n",
      "31600. Iteration Training loss: 3.127321126082648\n",
      "[ 2.44413863e-01 -1.59925758e-04] -23.401500000000084\n",
      "31650. Iteration Training loss: 2.3199680572414056\n",
      "[0.40559528 0.15615964] -23.4331000000001\n",
      "31700. Iteration Training loss: 4.532738980765243\n",
      "[0.58611621 0.32812149] -23.46450000000012\n",
      "31750. Iteration Training loss: 2.7258433507966395\n",
      "[ 0.26215778 -0.00452294] -23.50340000000014\n",
      "31800. Iteration Training loss: 2.8655451955498767\n",
      "[0.4431492  0.16275327] -23.534800000000153\n",
      "31850. Iteration Training loss: 4.60455477245604\n",
      "[0.68512543 0.39791195] -23.565100000000175\n",
      "31900. Iteration Training loss: 5.3633129841215785\n",
      "[ 0.22114132 -0.03747966] -23.605700000000194\n",
      "31950. Iteration Training loss: 3.15227075711194\n",
      "[0.45983417 0.17131335] -23.6363000000002\n",
      "32000. Iteration Training loss: 4.6050770364134115\n",
      "[0.68316111 0.44582594] -23.666300000000223\n",
      "32050. Iteration Training loss: 5.9472206362315285\n",
      "[ 0.18079476 -0.01082619] -23.707200000000245\n",
      "32100. Iteration Training loss: 3.8970147779319837\n",
      "[0.46453434 0.23140935] -23.737400000000264\n",
      "32150. Iteration Training loss: 4.60499237732741\n",
      "[0.67662259 0.43729576] -23.76820000000028\n",
      "32200. Iteration Training loss: 2.464659732330666\n",
      "[ 0.27341309 -0.00299067] -23.8086000000003\n",
      "32250. Iteration Training loss: 1.843459698392454\n",
      "[0.40372888 0.13490491] -23.84040000000032\n",
      "32300. Iteration Training loss: 4.604402812629255\n",
      "[0.67096872 0.40955183] -23.870000000000335\n",
      "32350. Iteration Training loss: 5.3769392343666444\n",
      "[ 0.21785979 -0.02981954] -23.91030000000035\n",
      "32400. Iteration Training loss: 3.8028933087233168\n",
      "[0.48230828 0.21196285] -23.94040000000037\n",
      "32450. Iteration Training loss: 4.604981995033084\n",
      "[0.68992977 0.43124832] -23.970800000000388\n",
      "32500. Iteration Training loss: 2.4527647064252243\n",
      "[ 0.27909387 -0.00512154] -24.01120000000041\n",
      "32550. Iteration Training loss: 3.5611838640815843\n",
      "[0.44883435 0.21611438] -24.041800000000425\n",
      "32600. Iteration Training loss: 4.603409879293574\n",
      "[0.64833021 0.41052192] -24.072600000000442\n",
      "32650. Iteration Training loss: 2.370623118806031\n",
      "[ 0.2789261  -0.00104807] -24.11270000000046\n",
      "32700. Iteration Training loss: 3.8560905952044857\n",
      "[0.47831726 0.22398075] -24.143200000000476\n",
      "32750. Iteration Training loss: 4.6050507881611145\n",
      "[0.67844736 0.45463074] -24.17370000000049\n",
      "32800. Iteration Training loss: 2.266295003000744\n",
      "[0.26801484 0.01333686] -24.21420000000051\n",
      "32850. Iteration Training loss: 3.420280446014548\n",
      "[0.43620435 0.21780335] -24.24500000000053\n",
      "32900. Iteration Training loss: 4.604952191018567\n",
      "[0.6841336  0.43873533] -24.27520000000054\n",
      "32950. Iteration Training loss: 3.1677561961086025\n",
      "[0.2428957  0.01254998] -24.315400000000558\n",
      "33000. Iteration Training loss: 3.419139548694693\n",
      "[0.45280984 0.20713907] -24.346200000000575\n",
      "33050. Iteration Training loss: 4.604705595815475\n",
      "[0.65610562 0.44302281] -24.376300000000583\n",
      "33100. Iteration Training loss: 5.783962711211306\n",
      "[ 0.19752707 -0.01398152] -24.4167000000006\n",
      "33150. Iteration Training loss: 1.1387279681040463\n",
      "[0.36250547 0.13423851] -24.44800000000061\n",
      "33200. Iteration Training loss: 4.604883325590357\n",
      "[0.68430259 0.43677074] -24.476700000000616\n",
      "33250. Iteration Training loss: 2.799243787101557\n",
      "[0.25320819 0.01531876] -24.516600000000626\n",
      "33300. Iteration Training loss: 3.3573353920184457\n",
      "[0.45202541 0.20644811] -24.547100000000636\n",
      "33350. Iteration Training loss: 4.588920334622073\n",
      "[0.63372951 0.37454369] -24.577800000000646\n",
      "33400. Iteration Training loss: 3.55789601606654\n",
      "[0.24357122 0.00538332] -24.616600000000652\n",
      "33450. Iteration Training loss: 3.826322733056111\n",
      "[0.47816286 0.22975144] -24.646800000000663\n",
      "33500. Iteration Training loss: 4.604954579282509\n",
      "[0.68821424 0.4452255 ] -24.67680000000067\n",
      "33550. Iteration Training loss: 3.0098106392683888\n",
      "[0.24727556 0.01791844] -24.71670000000068\n",
      "33600. Iteration Training loss: 3.796782391548346\n",
      "[0.48031077 0.22682663] -24.74690000000069\n",
      "33650. Iteration Training loss: 4.60495334609712\n",
      "[0.69089479 0.44548341] -24.776900000000698\n",
      "33700. Iteration Training loss: 2.1981741061805233\n",
      "[0.27495539 0.01709639] -24.81680000000071\n",
      "33750. Iteration Training loss: 3.804576249097945\n",
      "[0.4819781  0.22818095] -24.84690000000072\n",
      "33800. Iteration Training loss: 4.604848689008476\n",
      "[0.68283117 0.44420136] -24.877000000000727\n",
      "33850. Iteration Training loss: 5.200432331096846\n",
      "[ 0.2330105  -0.02697791] -24.917100000000737\n",
      "33900. Iteration Training loss: 3.166175458279531\n",
      "[0.47519646 0.18255322] -24.94720000000075\n",
      "33950. Iteration Training loss: 4.5765519718641015\n",
      "[0.60201482 0.38756945] -24.977200000000764\n",
      "34000. Iteration Training loss: 5.155329537094073\n",
      "[ 0.2379065  -0.02966993] -25.01650000000077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34050. Iteration Training loss: 2.9622257786387722\n",
      "[0.45013107 0.188673  ] -25.046500000000783\n",
      "34100. Iteration Training loss: 4.602156169125464\n",
      "[0.65013329 0.4190234 ] -25.07590000000079\n",
      "34150. Iteration Training loss: 5.07961442314823\n",
      "[ 0.23289465 -0.02001024] -25.1154000000008\n",
      "34200. Iteration Training loss: 2.924086068498206\n",
      "[0.45024788 0.18768081] -25.1454000000008\n",
      "34250. Iteration Training loss: 4.603572244000792\n",
      "[0.67216995 0.42103478] -25.17490000000081\n",
      "34300. Iteration Training loss: 5.927362002890179\n",
      "[ 0.19983241 -0.0112839 ] -25.214400000000822\n",
      "34350. Iteration Training loss: 3.3938760880311083\n",
      "[0.45446133 0.21904773] -25.24370000000083\n",
      "34400. Iteration Training loss: 4.481188008527995\n",
      "[0.58825072 0.33702704] -25.274800000000834\n",
      "34450. Iteration Training loss: 5.8797650489147735\n",
      "[ 0.20050545 -0.00866662] -25.31260000000084\n",
      "34500. Iteration Training loss: 1.144857273774191\n",
      "[0.36476719 0.14894601] -25.343300000000845\n",
      "34550. Iteration Training loss: 4.603717704489082\n",
      "[0.64597797 0.44607466] -25.372100000000856\n",
      "34600. Iteration Training loss: 3.5936196414263826\n",
      "[0.24979126 0.00880315] -25.411600000000867\n",
      "34650. Iteration Training loss: 3.2532635489857227\n",
      "[0.46739922 0.20244346] -25.44160000000087\n",
      "34700. Iteration Training loss: 4.600359403291696\n",
      "[0.65371043 0.41386054] -25.471200000000877\n",
      "34750. Iteration Training loss: 2.862262181372879\n",
      "[0.27546847 0.00529884] -25.510300000000885\n",
      "34800. Iteration Training loss: 2.2168356404582066\n",
      "[0.4263254  0.16824445] -25.54060000000089\n",
      "34850. Iteration Training loss: 4.523757289155631\n",
      "[0.59504992 0.36257405] -25.570500000000894\n",
      "34900. Iteration Training loss: 5.489972418187442\n",
      "[ 0.22779474 -0.02147741] -25.609100000000904\n",
      "34950. Iteration Training loss: 3.3765054490293394\n",
      "[0.47805167 0.20704932] -25.638600000000913\n",
      "35000. Iteration Training loss: 4.603108170655721\n",
      "[0.67833033 0.42181776] -25.668100000000916\n",
      "35050. Iteration Training loss: 2.921568482348058\n",
      "[0.2775318  0.00416269] -25.707200000000924\n",
      "35100. Iteration Training loss: 5.022176782204337\n",
      "[ 0.22872575 -0.00475962] -25.73910000000094\n",
      "35150. Iteration Training loss: 3.3653799071621084\n",
      "[0.47825446 0.20820831] -25.768700000000948\n",
      "35200. Iteration Training loss: 4.603537253417342\n",
      "[0.68273086 0.42721386] -25.797800000000954\n",
      "35250. Iteration Training loss: 2.8666024833254062\n",
      "[0.26875887 0.01596774] -25.83670000000096\n",
      "35300. Iteration Training loss: 3.4007440543247665\n",
      "[0.48047027 0.21092517] -25.866600000000968\n",
      "35350. Iteration Training loss: 4.567570890284609\n",
      "[0.60281349 0.39770451] -25.896300000000974\n",
      "35400. Iteration Training loss: 2.503358050917974\n",
      "[0.26417211 0.0323016 ] -25.934100000000974\n",
      "35450. Iteration Training loss: 3.0423432535399577\n",
      "[0.43770966 0.21852425] -25.96390000000098\n",
      "35500. Iteration Training loss: 4.600158541882429\n",
      "[0.66362518 0.4176569 ] -25.993200000000982\n",
      "35550. Iteration Training loss: 2.280330905258259\n",
      "[0.27402827 0.03119207] -26.031500000000978\n",
      "35600. Iteration Training loss: 3.259132928084059\n",
      "[0.44071203 0.23304262] -26.061100000000987\n",
      "35650. Iteration Training loss: 4.599607312798713\n",
      "[0.65968057 0.41994945] -26.09060000000099\n",
      "35700. Iteration Training loss: 2.612275310476914\n",
      "[0.28315262 0.01379325] -26.129300000000992\n",
      "35750. Iteration Training loss: 3.3627384377867804\n",
      "[0.47910401 0.21384361] -26.159100000001004\n",
      "35800. Iteration Training loss: 4.603564416265862\n",
      "[0.68268091 0.43646915] -26.188300000001007\n",
      "35850. Iteration Training loss: 5.978503839136742\n",
      "[ 0.21299722 -0.01502356] -26.227800000001015\n",
      "35900. Iteration Training loss: 2.3081701938769585\n",
      "[0.41024945 0.19943159] -26.25710000000102\n",
      "35950. Iteration Training loss: 4.602782369839473\n",
      "[0.67769026 0.43270156] -26.285800000001018\n",
      "36000. Iteration Training loss: 5.455959088716123\n",
      "[ 0.23243484 -0.0155134 ] -26.325200000001026\n",
      "36050. Iteration Training loss: 3.306865469167818\n",
      "[0.45117106 0.23357784] -26.35400000000103\n",
      "36100. Iteration Training loss: 4.58459627545509\n",
      "[0.61308246 0.42219892] -26.383300000001032\n",
      "36150. Iteration Training loss: 3.6828550167879213\n",
      "[0.25890358 0.01064873] -26.42180000000103\n",
      "36200. Iteration Training loss: 1.9665427562019195\n",
      "[0.40853005 0.18355406] -26.451500000001033\n",
      "36250. Iteration Training loss: 4.603074229388906\n",
      "[0.68023376 0.43844054] -26.480100000001034\n",
      "36300. Iteration Training loss: 5.619268336952243\n",
      "[ 0.22585853 -0.0111903 ] -26.51950000000104\n",
      "36350. Iteration Training loss: 3.235104315483028\n",
      "[0.38238524 0.28721168] -26.54620000000105\n",
      "36400. Iteration Training loss: 4.55517149072441\n",
      "[0.61721403 0.39147276] -26.576800000001057\n",
      "36450. Iteration Training loss: 5.1544725722167115\n",
      "[0.22596494 0.00629567] -26.614800000001058\n",
      "36500. Iteration Training loss: 3.3156994096749566\n",
      "[0.45644832 0.23512934] -26.643400000001055\n",
      "36550. Iteration Training loss: 4.601728893159289\n",
      "[0.68007696 0.4308758 ] -26.672600000001058\n",
      "36600. Iteration Training loss: 3.4379641252414213\n",
      "[0.27249687 0.00752606] -26.71140000000107\n",
      "36650. Iteration Training loss: 3.667895353626414\n",
      "[0.47494663 0.25294348] -26.740300000001074\n",
      "36700. Iteration Training loss: 4.604855741328472\n",
      "[0.68209974 0.48807113] -26.76890000000107\n",
      "36750. Iteration Training loss: 2.274742388086457\n",
      "[0.26255097 0.05226069] -26.808100000001076\n",
      "36800. Iteration Training loss: 2.8884572674620155\n",
      "[0.46095066 0.20502593] -26.83820000000108\n",
      "36850. Iteration Training loss: 4.604507003492747\n",
      "[0.69841175 0.46151545] -26.866700000001078\n",
      "36900. Iteration Training loss: 4.4621427976500945\n",
      "[0.5850562  0.36540538] -26.899800000001086\n",
      "36950. Iteration Training loss: 3.219624652366067\n",
      "[0.27276417 0.01656907] -26.936800000001085\n",
      "37000. Iteration Training loss: 1.056863805993408\n",
      "[0.36615333 0.16980865] -26.967100000001082\n",
      "37050. Iteration Training loss: 4.603047018180493\n",
      "[0.69329389 0.44063718] -26.995000000001088\n",
      "37100. Iteration Training loss: 3.5978267412464153\n",
      "[0.2663742  0.01361213] -27.033300000001088\n",
      "37150. Iteration Training loss: 3.304100508109392\n",
      "[0.45731922 0.24064293] -27.062200000001095\n",
      "37200. Iteration Training loss: 4.604555758186907\n",
      "[0.68973308 0.47443777] -27.090800000001092\n",
      "37250. Iteration Training loss: 4.001646520394495\n",
      "[0.25248947 0.01819452] -27.129800000001097\n",
      "37300. Iteration Training loss: 2.4055147763200666\n",
      "[0.3925812  0.23557433] -27.158100000001106\n",
      "37350. Iteration Training loss: 3.5842847859348574\n",
      "[0.42436121 0.29280268] -27.188600000001106\n",
      "37400. Iteration Training loss: 3.3596138076436195\n",
      "[0.43875554 0.26212183] -27.220600000001113\n",
      "37450. Iteration Training loss: 4.375232758763921\n",
      "[0.5532511  0.35595774] -27.251300000001116\n",
      "37500. Iteration Training loss: 2.4215684974387925\n",
      "[0.39367035 0.23775624] -27.284200000001128\n",
      "37550. Iteration Training loss: 4.588529200112852\n",
      "[0.64180366 0.42950641] -27.313100000001135\n",
      "37600. Iteration Training loss: 3.551175375170233\n",
      "[0.27363891 0.01177772] -27.351300000001135\n",
      "37650. Iteration Training loss: 3.418540718827976\n",
      "[0.48636936 0.2332376 ] -27.380400000001142\n",
      "37700. Iteration Training loss: 4.593418187047946\n",
      "[0.67318102 0.41996694] -27.409400000001142\n",
      "37750. Iteration Training loss: 3.232609175176931\n",
      "[0.27052279 0.02534189] -27.44770000000115\n",
      "37800. Iteration Training loss: 4.592786206229003\n",
      "[0.60073469 0.47086903] -27.471800000001153\n",
      "37850. Iteration Training loss: 2.715249979650112\n",
      "[0.27148607 0.04014512] -27.51010000000116\n",
      "37900. Iteration Training loss: 1.453400610532402\n",
      "[0.38294711 0.19295494] -27.53980000000116\n",
      "37950. Iteration Training loss: 2.726300309329974\n",
      "[0.44868142 0.21601632] -27.570800000001167\n",
      "38000. Iteration Training loss: 3.23131914105093\n",
      "[0.46323555 0.23973117] -27.602200000001172\n",
      "38050. Iteration Training loss: 4.596980387374256\n",
      "[0.67132444 0.43633895] -27.631400000001175\n",
      "38100. Iteration Training loss: 2.013720918804752\n",
      "[0.31003674 0.29544523] -27.663000000001176\n",
      "38150. Iteration Training loss: 4.57048405509769\n",
      "[0.62197953 0.42726662] -27.692500000001186\n",
      "38200. Iteration Training loss: 5.310239154299074\n",
      "[0.21954789 0.02392017] -27.730600000001193\n",
      "38250. Iteration Training loss: 4.22676047230502\n",
      "[0.4433792  0.39181766] -27.7550000000012\n",
      "38300. Iteration Training loss: 3.406490534935242\n",
      "[0.46539127 0.25497249] -27.78810000000121\n",
      "38350. Iteration Training loss: 4.466831946543866\n",
      "[0.58347428 0.38901588] -27.818000000001216\n",
      "38400. Iteration Training loss: 3.7602855091353775\n",
      "[0.26838721 0.01812021] -27.855300000001225\n",
      "38450. Iteration Training loss: 4.857276865545149\n",
      "[0.22493094 0.03408122] -27.88650000000123\n",
      "38500. Iteration Training loss: 1.4022161065484868\n",
      "[0.3942671  0.18551495] -27.916200000001233\n",
      "38550. Iteration Training loss: 4.602347869320287\n",
      "[0.68350162 0.46206312] -27.94360000000124\n",
      "38600. Iteration Training loss: 2.89349923062443\n",
      "[0.2741617  0.03868545] -27.981700000001247\n",
      "38650. Iteration Training loss: 3.494551056122369\n",
      "[0.47929873 0.25566913] -28.01020000000126\n",
      "38700. Iteration Training loss: 4.286860554692649\n",
      "[0.55899032 0.33306593] -28.040800000001262\n",
      "38750. Iteration Training loss: 3.6578397312071953\n",
      "[0.45785987 0.28898465] -28.070500000001267\n",
      "38800. Iteration Training loss: 4.465392173385109\n",
      "[0.58480198 0.3935682 ] -28.100700000001275\n",
      "38850. Iteration Training loss: 4.526862602835582\n",
      "[0.54025607 0.45989905] -28.128600000001285\n",
      "38900. Iteration Training loss: 5.324907311133021\n",
      "[0.22791238 0.02003689] -28.166600000001292\n",
      "38950. Iteration Training loss: 5.397329694237095\n",
      "[0.22610732 0.02006935] -28.197700000001305\n",
      "39000. Iteration Training loss: 2.2660647813767776\n",
      "[0.39473635 0.24379553] -28.223900000001315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39050. Iteration Training loss: 4.597778289896677\n",
      "[0.67164634 0.45289137] -28.252600000001326\n",
      "39100. Iteration Training loss: 4.604305316468855\n",
      "[0.6654803  0.51046864] -28.281600000001323\n",
      "39150. Iteration Training loss: 2.2274370433767507\n",
      "[0.27630364 0.06102994] -28.31950000000133\n",
      "39200. Iteration Training loss: 3.0431344278895307\n",
      "[0.37593694 0.31151012] -28.345100000001334\n",
      "39250. Iteration Training loss: 4.604272690633135\n",
      "[0.66469427 0.51221014] -28.37220000000133\n",
      "39300. Iteration Training loss: 2.564809705888992\n",
      "[0.27714247 0.05121923] -28.409900000001336\n",
      "39350. Iteration Training loss: 2.54193585286247\n",
      "[0.27308182 0.05620836] -28.441200000001345\n",
      "39400. Iteration Training loss: 4.57456643319178\n",
      "[0.619516   0.45086528] -28.465200000001353\n",
      "39450. Iteration Training loss: 4.124583742875721\n",
      "[0.26406829 0.02120711] -28.503500000001363\n",
      "39500. Iteration Training loss: 4.60106025806052\n",
      "[0.67254899 0.47364484] -28.52670000000137\n",
      "39550. Iteration Training loss: 2.803279970575431\n",
      "[0.20949129 0.11312349] -28.560400000001373\n",
      "39600. Iteration Training loss: 3.2091837641240564\n",
      "[0.47003538 0.24944524] -28.589900000001382\n",
      "39650. Iteration Training loss: 4.6026474783231395\n",
      "[0.65779026 0.4980346 ] -28.61480000000138\n",
      "39700. Iteration Training loss: 0.23798723739292607\n",
      "[0.26868892 0.1993626 ] -28.649900000001384\n",
      "39750. Iteration Training loss: 1.618526359218614\n",
      "[0.39712755 0.21056637] -28.679600000001386\n",
      "39800. Iteration Training loss: 5.127626500559699\n",
      "[0.20414237 0.06004673] -28.712300000001395\n",
      "39850. Iteration Training loss: 1.2753498012302726\n",
      "[0.39772842 0.18762705] -28.742300000001407\n",
      "39900. Iteration Training loss: 4.602615852702246\n",
      "[0.69287283 0.47662014] -28.76980000000142\n",
      "39950. Iteration Training loss: 4.6020023247622115\n",
      "[0.64648131 0.50462819] -28.80000000000143\n",
      "40000. Iteration Training loss: 2.0283986880365172\n",
      "[0.3028662  0.05021011] -28.838600000001442\n",
      "40050. Iteration Training loss: 4.599432657298418\n",
      "[0.67584799 0.47040848] -28.86260000000145\n",
      "40100. Iteration Training loss: 2.788736310052773\n",
      "[0.26846665 0.05970856] -28.900600000001464\n",
      "40150. Iteration Training loss: 1.8793016479777895\n",
      "[0.25954714 0.09585459] -28.931500000001478\n",
      "40200. Iteration Training loss: 1.8166523737246822\n",
      "[0.38587679 0.23761012] -28.957700000001484\n",
      "40250. Iteration Training loss: 1.8082609896477093\n",
      "[0.25681898 0.10153097] -28.989900000001494\n",
      "40300. Iteration Training loss: 1.2493521814630488\n",
      "[0.39492063 0.19327074] -29.0188000000015\n",
      "40350. Iteration Training loss: 4.601858465482094\n",
      "[0.68278865 0.48358436] -29.045800000001513\n",
      "40400. Iteration Training loss: 2.2403175157635524\n",
      "[0.2794487  0.06791487] -29.083900000001528\n",
      "40450. Iteration Training loss: 1.366052974321255\n",
      "[0.39413902 0.20414048] -29.113700000001543\n",
      "40500. Iteration Training loss: 3.358229793426973\n",
      "[0.47316355 0.26819079] -29.14390000000155\n",
      "40550. Iteration Training loss: 4.554188234162937\n",
      "[0.63202254 0.43790792] -29.173000000001565\n",
      "40600. Iteration Training loss: 2.31075853842293\n",
      "[0.27288959 0.07363785] -29.21000000000158\n",
      "40650. Iteration Training loss: 3.277694355372887\n",
      "[0.47470737 0.26207711] -29.239000000001592\n",
      "40700. Iteration Training loss: 3.3433035621510516\n",
      "[0.45669341 0.2821122 ] -29.269700000001606\n",
      "40750. Iteration Training loss: 4.473590930053176\n",
      "[0.60853202 0.40721742] -29.299100000001616\n",
      "40800. Iteration Training loss: 4.961414183739457\n",
      "[0.23478656 0.04003988] -29.336400000001632\n",
      "40850. Iteration Training loss: 1.3557487295990394\n",
      "[0.39618966 0.2059054 ] -29.365400000001646\n",
      "40900. Iteration Training loss: 4.599554402937437\n",
      "[0.68741638 0.47483812] -29.39300000000166\n",
      "40950. Iteration Training loss: 2.724332195771723\n",
      "[0.26831751 0.06888556] -29.430900000001674\n",
      "41000. Iteration Training loss: 3.2043765987473747\n",
      "[0.46714008 0.26598582] -29.45950000000169\n",
      "41050. Iteration Training loss: 4.59912025068555\n",
      "[0.67981818 0.48036743] -29.487500000001702\n",
      "41100. Iteration Training loss: 4.599520151854674\n",
      "[0.6736507  0.48717726] -29.518000000001713\n",
      "41150. Iteration Training loss: 4.869236586631615\n",
      "[0.24208951 0.03780374] -29.555800000001728\n",
      "41200. Iteration Training loss: 1.1890580473182448\n",
      "[0.39356424 0.19998179] -29.584800000001742\n",
      "41250. Iteration Training loss: 0.7130254747773845\n",
      "[0.34616754 0.20705814] -29.615400000001756\n",
      "41300. Iteration Training loss: 4.472178070879179\n",
      "[0.60050514 0.41976244] -29.64340000000177\n",
      "41350. Iteration Training loss: 4.529590434609526\n",
      "[0.56866722 0.47567829] -29.66980000000177\n",
      "41400. Iteration Training loss: 4.860951065113042\n",
      "[0.24392683 0.03810864] -29.707100000001784\n",
      "41450. Iteration Training loss: 0.9898168375874825\n",
      "[0.38938449 0.1900457 ] -29.736200000001798\n",
      "41500. Iteration Training loss: 4.533579201912684\n",
      "[0.61512233 0.44749207] -29.76250000000181\n",
      "41550. Iteration Training loss: 2.125909532092569\n",
      "[0.27402823 0.08617352] -29.798500000001823\n",
      "41600. Iteration Training loss: 2.0556628712574603\n",
      "[0.41738717 0.2392232 ] -29.826400000001833\n",
      "41650. Iteration Training loss: 4.601222390251252\n",
      "[0.67631048 0.50204348] -29.85300000000184\n",
      "41700. Iteration Training loss: 2.1303824012933887\n",
      "[0.28928839 0.07302186] -29.890400000001847\n",
      "41750. Iteration Training loss: 3.249766080572658\n",
      "[0.48619845 0.2625033 ] -29.917800000001854\n",
      "41800. Iteration Training loss: 1.085548855961155\n",
      "[0.31034128 0.2789084 ] -29.94490000000186\n",
      "41850. Iteration Training loss: 3.6588768285803166\n",
      "[0.46074645 0.32138333] -29.97180000000186\n",
      "41900. Iteration Training loss: 1.7537353030721017\n",
      "[0.40608164 0.23385797] -30.00140000000187\n",
      "41950. Iteration Training loss: 4.433066576531945\n",
      "[0.6009561  0.40884145] -30.02990000000188\n",
      "42000. Iteration Training loss: 4.552114433486883\n",
      "[0.61799711 0.46584506] -30.059600000001886\n",
      "42050. Iteration Training loss: 4.28550302923761\n",
      "[0.53514266 0.39327059] -30.091200000001898\n",
      "42100. Iteration Training loss: 4.561310670949273\n",
      "[0.61875845 0.47459405] -30.117200000001908\n",
      "42150. Iteration Training loss: 2.0611815542994334\n",
      "[0.27826093 0.08902596] -30.152600000001915\n",
      "42200. Iteration Training loss: 0.9368117383316741\n",
      "[0.38016972 0.20274197] -30.180900000001923\n",
      "42250. Iteration Training loss: 0.25243826360850863\n",
      "[0.28840112 0.21011093] -30.209200000001925\n",
      "42300. Iteration Training loss: 0.6835093603243729\n",
      "[0.35346604 0.20702368] -30.23730000000193\n",
      "42350. Iteration Training loss: 2.079401418452411\n",
      "[0.26705317 0.10057752] -30.266500000001944\n",
      "42400. Iteration Training loss: 1.1288399726523386\n",
      "[0.3990432  0.20239054] -30.294900000001952\n",
      "42450. Iteration Training loss: 4.453366329971007\n",
      "[0.59339604 0.43033273] -30.321200000001966\n",
      "42500. Iteration Training loss: 4.917981671373425\n",
      "[0.22686362 0.06393775] -30.356100000001973\n",
      "42550. Iteration Training loss: 3.317013057417366\n",
      "[0.46648149 0.2914225 ] -30.38290000000198\n",
      "42600. Iteration Training loss: 4.44464735099686\n",
      "[0.59573255 0.42640543] -30.410200000001986\n",
      "42650. Iteration Training loss: 2.1192787149321854\n",
      "[0.29380671 0.0766352 ] -30.446200000002\n",
      "42700. Iteration Training loss: 1.0697336896731102\n",
      "[0.3847193  0.21485619] -30.473300000002013\n",
      "42750. Iteration Training loss: 4.599296594251057\n",
      "[0.67765179 0.50553899] -30.498100000002022\n",
      "42800. Iteration Training loss: 2.1587972048419015\n",
      "[0.29171173 0.07848128] -30.534400000002034\n",
      "42850. Iteration Training loss: 1.2395052917968659\n",
      "[0.40732245 0.20786862] -30.563700000002047\n",
      "42900. Iteration Training loss: 0.5757344658894821\n",
      "[0.35487077 0.19827527] -30.593100000002053\n",
      "42950. Iteration Training loss: 2.1586550651036447\n",
      "[0.27871364 0.09188705] -30.623300000002065\n",
      "43000. Iteration Training loss: 0.7625151449998304\n",
      "[0.38566444 0.18665555] -30.651700000002077\n",
      "43050. Iteration Training loss: 4.599503288877878\n",
      "[0.67413152 0.5129854 ] -30.676100000002087\n",
      "43100. Iteration Training loss: 2.4987047459521508\n",
      "[0.28206118 0.07950046] -30.712600000002098\n",
      "43150. Iteration Training loss: 3.2918584036570735\n",
      "[0.46813005 0.29418219] -30.73920000000211\n",
      "43200. Iteration Training loss: 4.554234468083154\n",
      "[0.62290103 0.4798652 ] -30.76540000000212\n",
      "43250. Iteration Training loss: 2.9961673912815887\n",
      "[0.25253132 0.0955858 ] -30.80080000000213\n",
      "43300. Iteration Training loss: 1.6679213960157246\n",
      "[0.41020163 0.23911773] -30.828600000002144\n",
      "43350. Iteration Training loss: 4.399052098426958\n",
      "[0.58254266 0.42436803] -30.855200000002153\n",
      "43400. Iteration Training loss: 4.570767344479316\n",
      "[0.22755658 0.08037733] -30.88970000000217\n",
      "43450. Iteration Training loss: 1.0328793340905829\n",
      "[0.38911067 0.21527252] -30.917100000002183\n",
      "43500. Iteration Training loss: 4.435714525110357\n",
      "[0.59818966 0.43210338] -30.943300000002193\n",
      "43550. Iteration Training loss: 2.3240698790690946\n",
      "[0.27363089 0.09647344] -30.978100000002204\n",
      "43600. Iteration Training loss: 3.2842490841403533\n",
      "[0.47000256 0.29665336] -31.00480000000222\n",
      "43650. Iteration Training loss: 4.561721968486012\n",
      "[0.62339767 0.49211238] -31.030900000002234\n",
      "43700. Iteration Training loss: 2.1217728046690407\n",
      "[0.28109927 0.09680179] -31.066400000002247\n",
      "43750. Iteration Training loss: 0.760863632965655\n",
      "[0.38852673 0.19154725] -31.09480000000226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43800. Iteration Training loss: 4.508891286858907\n",
      "[0.62482861 0.45508888] -31.12040000000227\n",
      "43850. Iteration Training loss: 4.441865582862768\n",
      "[0.59257177 0.44357784] -31.150500000002285\n",
      "43900. Iteration Training loss: 2.068384826063118\n",
      "[0.27343364 0.10738802] -31.185300000002297\n",
      "43950. Iteration Training loss: 1.0319603010370013\n",
      "[0.39122815 0.21832001] -31.21330000000231\n",
      "44000. Iteration Training loss: 4.432259973434169\n",
      "[0.6007838  0.43512783] -31.23950000000232\n",
      "44050. Iteration Training loss: 5.0837177079990195\n",
      "[0.23129803 0.066607  ] -31.27440000000233\n",
      "44100. Iteration Training loss: 3.157362267495027\n",
      "[0.45367269 0.30496215] -31.300000000002346\n",
      "44150. Iteration Training loss: 4.368477132799626\n",
      "[0.57478157 0.42660466] -31.327800000002355\n",
      "44200. Iteration Training loss: 5.368539812354503\n",
      "[0.18624356 0.10752515] -31.359000000002368\n",
      "44250. Iteration Training loss: 1.0096316868554382\n",
      "[0.39131548 0.21939182] -31.38720000000238\n",
      "44300. Iteration Training loss: 4.3421831653199225\n",
      "[0.57144584 0.41939966] -31.413400000002394\n",
      "44350. Iteration Training loss: 3.280059909007939\n",
      "[0.47751575 0.29787584] -31.44420000000241\n",
      "44400. Iteration Training loss: 4.534457857035373\n",
      "[0.62874254 0.47634429] -31.47060000000242\n",
      "44450. Iteration Training loss: 2.2892193511292596\n",
      "[0.28294411 0.09578053] -31.50600000000243\n",
      "44500. Iteration Training loss: 1.7830344283505613\n",
      "[0.42646358 0.24402732] -31.533600000002444\n",
      "44550. Iteration Training loss: 4.596406159322615\n",
      "[0.68531031 0.51339194] -31.559600000002455\n",
      "44600. Iteration Training loss: 2.6274344993326872\n",
      "[0.26428299 0.10506633] -31.594900000002465\n",
      "44650. Iteration Training loss: 3.1579522897705368\n",
      "[0.47100619 0.29632093] -31.621600000002477\n",
      "44700. Iteration Training loss: 4.53093128864049\n",
      "[0.63029807 0.4767604 ] -31.648000000002487\n",
      "44750. Iteration Training loss: 5.708563491620294\n",
      "[0.19799398 0.08811223] -31.6824000000025\n",
      "44800. Iteration Training loss: 4.597627918226533\n",
      "[0.65776953 0.54015764] -31.70120000000251\n",
      "44850. Iteration Training loss: 1.9806639601094036\n",
      "[0.2838298  0.10791368] -31.737100000002524\n",
      "44900. Iteration Training loss: 3.108857360881085\n",
      "[0.4514939  0.31117407] -31.763200000002538\n",
      "44950. Iteration Training loss: 0.5894186762391154\n",
      "[0.31162233 0.27054525] -31.787800000002544\n",
      "45000. Iteration Training loss: 4.314987654378489\n",
      "[0.2349187  0.09221174] -31.81890000000256\n",
      "45050. Iteration Training loss: 3.9392304864384857\n",
      "[0.21241734 0.12593272] -31.843900000002566\n",
      "45100. Iteration Training loss: 4.543000441744381\n",
      "[0.62465438 0.49417019] -31.864700000002582\n",
      "45150. Iteration Training loss: 1.1448295129815769\n",
      "[0.27956926 0.14687958] -31.899300000002594\n",
      "45200. Iteration Training loss: 0.2536976738727071\n",
      "[0.29896379 0.2326544 ] -31.925300000002608\n",
      "45250. Iteration Training loss: 2.100385822422298\n",
      "[0.42740484 0.26972132] -31.95360000000262\n",
      "45300. Iteration Training loss: 4.6261860564383\n",
      "[0.18891228 0.13415734] -31.98330000000263\n",
      "45350. Iteration Training loss: 4.5064837780853155\n",
      "[0.58134483 0.50400623] -32.00220000000264\n",
      "45400. Iteration Training loss: 4.601474304382681\n",
      "[0.63771692 0.5802511 ] -32.02730000000262\n",
      "45450. Iteration Training loss: 0.4705730031120548\n",
      "[0.35567865 0.20901027] -32.06020000000259\n",
      "45500. Iteration Training loss: 1.98088998145835\n",
      "[0.28480304 0.11181273] -32.090500000002564\n",
      "45550. Iteration Training loss: 4.6000740247164424\n",
      "[0.63026449 0.57923625] -32.10940000000253\n",
      "45600. Iteration Training loss: 4.467989698655345\n",
      "[0.59725186 0.47482603] -32.137500000002504\n",
      "45650. Iteration Training loss: 3.43642290014932\n",
      "[0.43692754 0.35896617] -32.163700000002486\n",
      "45700. Iteration Training loss: 3.056126808791342\n",
      "[0.2581367  0.10732089] -32.194200000002475\n",
      "45750. Iteration Training loss: 4.22320749872117\n",
      "[0.50236632 0.44058134] -32.21360000000245\n",
      "45800. Iteration Training loss: 0.2352212196484871\n",
      "[0.29210567 0.24031736] -32.23480000000242\n",
      "45850. Iteration Training loss: 0.24634580285220994\n",
      "[0.29741899 0.23863158] -32.26140000000242\n",
      "45900. Iteration Training loss: 3.411478865072508\n",
      "[0.42822758 0.36654137] -32.282200000002405\n",
      "45950. Iteration Training loss: 4.533642661028767\n",
      "[0.57209195 0.53446119] -32.30050000000239\n",
      "46000. Iteration Training loss: 4.596844099032125\n",
      "[0.65335662 0.55477394] -32.32740000000238\n",
      "46050. Iteration Training loss: 3.056176337667891\n",
      "[0.4090207  0.35538652] -32.35620000000235\n",
      "46100. Iteration Training loss: 2.1913025837619675\n",
      "[0.22213503 0.1702104 ] -32.37960000000234\n",
      "46150. Iteration Training loss: 0.7517305895343549\n",
      "[0.33557816 0.27143362] -32.40140000000232\n",
      "46200. Iteration Training loss: 4.513208740831403\n",
      "[0.59759038 0.50580438] -32.42370000000231\n",
      "46250. Iteration Training loss: 2.37846641902078\n",
      "[0.28159635 0.10733264] -32.45950000000229\n",
      "46300. Iteration Training loss: 1.845731024458571\n",
      "[0.42473226 0.26579606] -32.486800000002255\n",
      "46350. Iteration Training loss: 0.2613857907402737\n",
      "[0.27982094 0.22698474] -32.51420000000223\n",
      "46400. Iteration Training loss: 4.2869301831913\n",
      "[0.51191707 0.46391773] -32.526700000002215\n",
      "46450. Iteration Training loss: 0.27921176136147663\n",
      "[0.31923972 0.22001027] -32.5555000000022\n",
      "46500. Iteration Training loss: 3.2586721619539656\n",
      "[0.4811776  0.31291528] -32.58280000000219\n",
      "46550. Iteration Training loss: 4.601426535213208\n",
      "[0.6384157  0.59239297] -32.597300000002186\n",
      "46600. Iteration Training loss: 0.7573012343098516\n",
      "[0.33238165 0.27902395] -32.625200000002174\n",
      "46650. Iteration Training loss: 4.456151698701196\n",
      "[0.56201275 0.50505794] -32.64170000000216\n",
      "46700. Iteration Training loss: 0.3589022729127615\n",
      "[0.26516253 0.22580736] -32.66800000000215\n",
      "46750. Iteration Training loss: 4.171049091686532\n",
      "[0.50693485 0.42998104] -32.68530000000216\n",
      "46800. Iteration Training loss: 1.942914749688379\n",
      "[0.36829022 0.32601151] -32.71040000000216\n",
      "46850. Iteration Training loss: 1.444659495162376\n",
      "[0.28107828 0.14390131] -32.73880000000215\n",
      "46900. Iteration Training loss: 4.595583142699026\n",
      "[0.6470466  0.56505644] -32.75730000000213\n",
      "46950. Iteration Training loss: 5.012407654691332\n",
      "[0.24054314 0.07933139] -32.793900000002104\n",
      "47000. Iteration Training loss: 0.6985739480812125\n",
      "[0.3802788  0.22584467] -32.82110000000206\n",
      "47050. Iteration Training loss: 3.02306243790556\n",
      "[0.41902339 0.35254327] -32.83880000000205\n",
      "47100. Iteration Training loss: 2.2476556942237624\n",
      "[0.23655622 0.16093914] -32.865000000002055\n",
      "47150. Iteration Training loss: 1.9011146585545766\n",
      "[0.41917959 0.28097894] -32.89040000000203\n",
      "47200. Iteration Training loss: 4.417364438876868\n",
      "[0.60680592 0.46069017] -32.916900000002\n",
      "47250. Iteration Training loss: 4.595152341403517\n",
      "[0.65083594 0.56522262] -32.938000000002006\n",
      "47300. Iteration Training loss: 3.2658707509084746\n",
      "[0.4536249  0.34348516] -32.96670000000197\n",
      "47350. Iteration Training loss: 3.9621920496136886\n",
      "[0.50463596 0.3859518 ] -32.98800000000198\n",
      "47400. Iteration Training loss: 4.5998003007738495\n",
      "[0.63411219 0.59558551] -33.00820000000198\n",
      "47450. Iteration Training loss: 0.37173598433661026\n",
      "[0.29766029 0.2039635 ] -33.03600000000196\n",
      "47500. Iteration Training loss: 1.6450001682869801\n",
      "[0.41669892 0.2703015 ] -33.062600000001936\n",
      "47550. Iteration Training loss: 4.410296987179432\n",
      "[0.61125374 0.45802735] -33.08880000000191\n",
      "47600. Iteration Training loss: 0.2861547380460059\n",
      "[0.30055011 0.2170597 ] -33.11720000000191\n",
      "47650. Iteration Training loss: 3.1496595349180843\n",
      "[0.41715839 0.3695193 ] -33.1358000000019\n",
      "47700. Iteration Training loss: 0.33430787467750067\n",
      "[0.32241898 0.24851874] -33.15950000000189\n",
      "47750. Iteration Training loss: 4.601386612697832\n",
      "[0.65389606 0.59432312] -33.17530000000189\n",
      "47800. Iteration Training loss: 4.467007671488402\n",
      "[0.60451225 0.49259169] -33.201400000001875\n",
      "47850. Iteration Training loss: 1.7071522831082941\n",
      "[0.27808704 0.14347881] -33.232800000001866\n",
      "47900. Iteration Training loss: 4.5981082599699405\n",
      "[0.66773463 0.57000206] -33.24850000000184\n",
      "47950. Iteration Training loss: 2.534889736975535\n",
      "[0.21768475 0.17698497] -33.27630000000185\n",
      "48000. Iteration Training loss: 4.597611738757417\n",
      "[0.65753222 0.57642245] -33.294600000001836\n",
      "48050. Iteration Training loss: 5.19756946787338\n",
      "[0.20538974 0.11855137] -33.32360000000182\n",
      "48100. Iteration Training loss: 4.592468037712813\n",
      "[0.65193606 0.56668103] -33.3423000000018\n",
      "48150. Iteration Training loss: 1.0125063813858024\n",
      "[0.34168329 0.30240876] -33.36970000000179\n",
      "48200. Iteration Training loss: 1.6481831104166045\n",
      "[0.25157466 0.1737565 ] -33.39610000000177\n",
      "48250. Iteration Training loss: 4.600692918409291\n",
      "[0.64792933 0.59967742] -33.41260000000176\n",
      "48300. Iteration Training loss: 4.599812764490546\n",
      "[0.64435684 0.59812748] -33.43650000000176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48350. Iteration Training loss: 0.7932912203312239\n",
      "[0.3812137  0.24660519] -33.467700000001734\n",
      "48400. Iteration Training loss: 1.0269065389009713\n",
      "[0.35404062 0.29362985] -33.4913000000017\n",
      "48450. Iteration Training loss: 1.80475163328534\n",
      "[0.23307472 0.18815788] -33.51830000000169\n",
      "48500. Iteration Training loss: 1.8149774074339688\n",
      "[0.24747923 0.17376896] -33.54020000000168\n",
      "48550. Iteration Training loss: 1.8281360614124265\n",
      "[0.25531754 0.16589091] -33.56250000000167\n",
      "48600. Iteration Training loss: 3.162373288377049\n",
      "[0.43270694 0.36449031] -33.57960000000166\n",
      "48650. Iteration Training loss: 3.217671246965348\n",
      "[0.22681767 0.15377258] -33.60660000000166\n",
      "48700. Iteration Training loss: 1.8754074117854302\n",
      "[0.24968083 0.17074344] -33.62950000000167\n",
      "48750. Iteration Training loss: 0.319905671406302\n",
      "[0.33654199 0.20474024] -33.65330000000166\n",
      "48800. Iteration Training loss: 4.407630806736548\n",
      "[0.21425664 0.13647079] -33.679300000001625\n",
      "48850. Iteration Training loss: 3.242469399213736\n",
      "[0.47503542 0.33631389] -33.7016000000016\n",
      "48900. Iteration Training loss: 3.233641211789325\n",
      "[0.45701762 0.35144329] -33.72560000000159\n",
      "48950. Iteration Training loss: 4.133152355829106\n",
      "[0.51280008 0.43649916] -33.7438000000016\n",
      "49000. Iteration Training loss: 1.179355669198571\n",
      "[0.36952562 0.29456504] -33.768200000001585\n",
      "49050. Iteration Training loss: 4.403921369527618\n",
      "[0.5628516  0.50490333] -33.78530000000157\n",
      "49100. Iteration Training loss: 0.2781039438301008\n",
      "[0.31411497 0.25722096] -33.81180000000158\n",
      "49150. Iteration Training loss: 4.595267565351262\n",
      "[0.67549546 0.56835421] -33.83130000000157\n",
      "49200. Iteration Training loss: 4.242828376107784\n",
      "[0.53858236 0.45571723] -33.85750000000156\n",
      "49250. Iteration Training loss: 4.23697062057646\n",
      "[0.54982909 0.44553676] -33.87950000000155\n",
      "49300. Iteration Training loss: 4.084654269563555\n",
      "[0.51903978 0.420944  ] -33.90260000000154\n",
      "49350. Iteration Training loss: 2.0925828408996425\n",
      "[0.24115274 0.17611443] -33.92730000000152\n",
      "49400. Iteration Training loss: 3.227326253501459\n",
      "[0.22491115 0.1600967 ] -33.948500000001516\n",
      "49450. Iteration Training loss: 4.594789344638403\n",
      "[0.68219191 0.56539986] -33.96970000000152\n",
      "49500. Iteration Training loss: 1.7972875160694748\n",
      "[0.36807946 0.33969975] -33.993600000001514\n",
      "49550. Iteration Training loss: 0.33348987313218315\n",
      "[0.31284536 0.21262564] -34.0193000000015\n",
      "49600. Iteration Training loss: 0.5108628106956197\n",
      "[0.33505487 0.27650983] -34.04030000000152\n",
      "49650. Iteration Training loss: 1.6928046595263089\n",
      "[0.27331307 0.160347  ] -34.06720000000152\n",
      "49700. Iteration Training loss: 2.777472563259707\n",
      "[0.43435665 0.34348994] -34.08550000000151\n",
      "49750. Iteration Training loss: 2.94788702195814\n",
      "[0.23495121 0.15974099] -34.1133000000015\n",
      "49800. Iteration Training loss: 0.24920922910115162\n",
      "[0.30269752 0.23775964] -34.13220000000151\n",
      "49850. Iteration Training loss: 0.3296116175631407\n",
      "[0.33275911 0.25270432] -34.15320000000151\n",
      "49900. Iteration Training loss: 4.041532743702546\n",
      "[0.48291131 0.44729327] -34.16990000000152\n",
      "49950. Iteration Training loss: 0.4000566404766618\n",
      "[0.31908116 0.20143079] -34.1974000000015\n",
      "50000. Iteration Training loss: 2.1312463201298346\n",
      "[0.40506248 0.33054253] -34.2193000000015\n",
      "50050. Iteration Training loss: 1.7443445699383378\n",
      "[0.23805063 0.19580334] -34.2444000000015\n",
      "50100. Iteration Training loss: 3.7959200919536586\n",
      "[0.47307261 0.41101706] -34.2628000000015\n",
      "50150. Iteration Training loss: 2.9987858011497632\n",
      "[0.24058346 0.15511436] -34.28910000000148\n",
      "50200. Iteration Training loss: 0.32188016006572545\n",
      "[0.3083264  0.22082664] -34.30980000000149\n",
      "50250. Iteration Training loss: 4.832906615899346\n",
      "[0.20186195 0.14626804] -34.332400000001485\n",
      "50300. Iteration Training loss: 4.5974525375657755\n",
      "[0.64398528 0.60935941] -34.346900000001476\n",
      "50350. Iteration Training loss: 4.594876690779489\n",
      "[0.64996915 0.59754287] -34.36910000000147\n",
      "50400. Iteration Training loss: 2.9995324845190807\n",
      "[0.43810477 0.36166099] -34.393900000001445\n",
      "50450. Iteration Training loss: 4.1107092718841685\n",
      "[0.5119314  0.44515262] -34.41400000000146\n",
      "50500. Iteration Training loss: 4.074515677946634\n",
      "[0.5287081  0.42136904] -34.43710000000145\n",
      "50550. Iteration Training loss: 3.2134277891090357\n",
      "[0.45399671 0.36569069] -34.461200000001455\n",
      "50600. Iteration Training loss: 3.2700086272422073\n",
      "[0.43996552 0.3836228 ] -34.481300000001454\n",
      "50650. Iteration Training loss: 4.545235202757492\n",
      "[0.63471814 0.54745737] -34.49910000000146\n",
      "50700. Iteration Training loss: 0.37659370446715323\n",
      "[0.29330275 0.2268424 ] -34.52470000000146\n",
      "50750. Iteration Training loss: 2.214417529029426\n",
      "[0.24701799 0.17507726] -34.54640000000146\n",
      "50800. Iteration Training loss: 4.064489966784725\n",
      "[0.49943988 0.44672217] -34.565200000001454\n",
      "50850. Iteration Training loss: 1.6965322451246891\n",
      "[0.24117794 0.19945298] -34.58970000000144\n",
      "50900. Iteration Training loss: 4.351841323180758\n",
      "[0.56324479 0.49933731] -34.60580000000143\n",
      "50950. Iteration Training loss: 1.787532391508821\n",
      "[0.25988929 0.17800023] -34.63050000000142\n",
      "51000. Iteration Training loss: 0.327682516251848\n",
      "[0.31520664 0.22005838] -34.6496000000014\n",
      "51050. Iteration Training loss: 4.185970632444147\n",
      "[0.53887101 0.45237288] -34.6666000000014\n",
      "51100. Iteration Training loss: 3.3073191528685895\n",
      "[0.4423172  0.38867181] -34.687100000001394\n",
      "51150. Iteration Training loss: 1.7527201658045968\n",
      "[0.25906083 0.18122741] -34.7095000000014\n",
      "51200. Iteration Training loss: 1.674729911870938\n",
      "[0.25001206 0.19346109] -34.72970000000137\n",
      "51250. Iteration Training loss: 1.7791370106239615\n",
      "[0.24080867 0.19902511] -34.75010000000135\n",
      "51300. Iteration Training loss: 1.9475256721686955\n",
      "[0.26594985 0.1683583 ] -34.77150000000135\n",
      "51350. Iteration Training loss: 4.187255373395384\n",
      "[0.51879109 0.47139613] -34.787000000001356\n",
      "51400. Iteration Training loss: 0.3627894346464825\n",
      "[0.28696207 0.23783865] -34.80960000000136\n",
      "51450. Iteration Training loss: 4.00656406544556\n",
      "[0.19984026 0.17709905] -34.828700000001355\n",
      "51500. Iteration Training loss: 0.8450190396108184\n",
      "[0.26623192 0.21857522] -34.84750000000137\n",
      "51550. Iteration Training loss: 0.49456082703479987\n",
      "[0.33980844 0.2838174 ] -34.867100000001365\n",
      "51600. Iteration Training loss: 0.43478805062725256\n",
      "[0.34400804 0.2715371 ] -34.888600000001354\n",
      "51650. Iteration Training loss: 4.5648383495776095\n",
      "[0.21701157 0.14578991] -34.912400000001355\n",
      "51700. Iteration Training loss: 0.4167305934106567\n",
      "[0.33805479 0.27636997] -34.93190000000135\n",
      "51750. Iteration Training loss: 4.5950762263523925\n",
      "[0.65412751 0.60827112] -34.94660000000136\n",
      "51800. Iteration Training loss: 0.27984445516056317\n",
      "[0.33705267 0.24499166] -34.97360000000135\n",
      "51850. Iteration Training loss: 1.9077680997900979\n",
      "[0.25300196 0.18568657] -34.994100000001346\n",
      "51900. Iteration Training loss: 3.11708784431918\n",
      "[0.43710124 0.38302109] -35.01120000000134\n",
      "51950. Iteration Training loss: 0.25824230279512345\n",
      "[0.3098561  0.24242717] -35.033000000001316\n",
      "52000. Iteration Training loss: 4.05809912395066\n",
      "[0.49614676 0.45840386] -35.0492000000013\n",
      "52050. Iteration Training loss: 4.428351552134226\n",
      "[0.60244277 0.51621736] -35.069200000001295\n",
      "52100. Iteration Training loss: 0.29243536161040035\n",
      "[0.32844494 0.22645878] -35.09740000000127\n",
      "52150. Iteration Training loss: 1.776831112347877\n",
      "[0.37311843 0.3533842 ] -35.11570000000126\n",
      "52200. Iteration Training loss: 3.5420027369486546\n",
      "[0.21958577 0.17336437] -35.13800000000127\n",
      "52250. Iteration Training loss: 3.5337045745619626\n",
      "[0.2385964  0.15459909] -35.160100000001265\n",
      "52300. Iteration Training loss: 0.6634707909792911\n",
      "[0.37196173 0.27362953] -35.18170000000127\n",
      "52350. Iteration Training loss: 4.168549046863263\n",
      "[0.54719977 0.4514096 ] -35.20070000000129\n",
      "52400. Iteration Training loss: 3.1941057383716998\n",
      "[0.44785092 0.38346962] -35.22220000000129\n",
      "52450. Iteration Training loss: 0.285138120803342\n",
      "[0.32084501 0.2767295 ] -35.246300000001284\n",
      "52500. Iteration Training loss: 1.4852494042725253\n",
      "[0.3883526  0.32333529] -35.26850000000128\n",
      "52550. Iteration Training loss: 3.419948761482408\n",
      "[0.45679649 0.39763768] -35.288800000001274\n",
      "52600. Iteration Training loss: 2.8833348982812477\n",
      "[0.44221445 0.36591529] -35.31090000000128\n",
      "52650. Iteration Training loss: 2.328974272287167\n",
      "[0.41725225 0.35159219] -35.331100000001314\n",
      "52700. Iteration Training loss: 3.785567288703662\n",
      "[0.47121331 0.43305993] -35.35020000000132\n",
      "52750. Iteration Training loss: 0.3499183704157502\n",
      "[0.30672032 0.23184583] -35.37680000000133\n",
      "52800. Iteration Training loss: 3.180698302970955\n",
      "[0.4676232  0.36783865] -35.39890000000132\n",
      "52850. Iteration Training loss: 1.7664534488325074\n",
      "[0.28415325 0.16627565] -35.42480000000132\n",
      "52900. Iteration Training loss: 0.6398309062556976\n",
      "[0.34468203 0.3043683 ] -35.44330000000131\n",
      "52950. Iteration Training loss: 3.185912144444761\n",
      "[0.45366635 0.38195072] -35.46520000000132\n",
      "53000. Iteration Training loss: 1.9240206130027153\n",
      "[0.26086942 0.18435167] -35.48960000000132\n",
      "53050. Iteration Training loss: 4.231656880374976\n",
      "[0.55336481 0.4761679 ] -35.50430000000134\n",
      "53100. Iteration Training loss: 3.192108304635707\n",
      "[0.45368249 0.38356878] -35.52560000000136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53150. Iteration Training loss: 3.792223163180261\n",
      "[0.48033172 0.42950154] -35.54390000000139\n",
      "53200. Iteration Training loss: 4.570119141491613\n",
      "[0.66204502 0.57505355] -35.565900000001406\n",
      "53250. Iteration Training loss: 4.114177879638394\n",
      "[0.54807646 0.44145738] -35.5933000000014\n",
      "53300. Iteration Training loss: 3.2621752884549164\n",
      "[0.45480441 0.39036258] -35.616700000001394\n",
      "53350. Iteration Training loss: 1.7272061988627023\n",
      "[0.27863489 0.17620085] -35.6405000000014\n",
      "53400. Iteration Training loss: 0.30149118350537063\n",
      "[0.31472591 0.23781787] -35.660700000001434\n",
      "53450. Iteration Training loss: 1.0705061589120308\n",
      "[0.28303618 0.20178276] -35.682200000001444\n",
      "53500. Iteration Training loss: 4.554420942257217\n",
      "[0.20336005 0.17055121] -35.703300000001455\n",
      "53550. Iteration Training loss: 0.49635793953055013\n",
      "[0.30360289 0.22283977] -35.724600000001445\n",
      "53600. Iteration Training loss: 3.8179238954457166\n",
      "[0.25111528 0.14208361] -35.74910000000144\n",
      "53650. Iteration Training loss: 0.477154722779267\n",
      "[0.29701323 0.23087988] -35.770800000001444\n",
      "53700. Iteration Training loss: 0.2575567990351297\n",
      "[0.33462154 0.25189611] -35.79330000000144\n",
      "53750. Iteration Training loss: 3.728099967699622\n",
      "[0.24258184 0.15421584] -35.81670000000143\n",
      "53800. Iteration Training loss: 2.2763227169754563\n",
      "[0.43164846 0.34410009] -35.83790000000142\n",
      "53850. Iteration Training loss: 4.591766278282488\n",
      "[0.66260916 0.61446726] -35.85420000000143\n",
      "53900. Iteration Training loss: 3.9410521555686757\n",
      "[0.21764953 0.17487468] -35.883100000001406\n",
      "53950. Iteration Training loss: 3.148787942729517\n",
      "[0.46698061 0.37486245] -35.90340000000138\n",
      "54000. Iteration Training loss: 0.6022817086119144\n",
      "[0.29280043 0.226048  ] -35.92660000000141\n",
      "54050. Iteration Training loss: 3.2796454368745973\n",
      "[0.44896523 0.40343131] -35.94640000000143\n",
      "54100. Iteration Training loss: 4.5803176281999205\n",
      "[0.6807599 0.5836918] -35.96570000000144\n",
      "54150. Iteration Training loss: 0.7374490716319856\n",
      "[0.37138477 0.29573641] -35.99030000000144\n",
      "54200. Iteration Training loss: 4.585584008489697\n",
      "[0.65979841 0.60780441] -36.00670000000144\n",
      "54250. Iteration Training loss: 4.495161447035713\n",
      "[0.60355502 0.57069352] -36.02570000000145\n",
      "54300. Iteration Training loss: 0.3360331235866914\n",
      "[0.3502865  0.21925913] -36.05320000000145\n",
      "54350. Iteration Training loss: 1.8973371523760179\n",
      "[0.38853866 0.36347508] -36.069400000001465\n",
      "54400. Iteration Training loss: 2.9331882039339217\n",
      "[0.46488107 0.36315035] -36.08890000000151\n",
      "54450. Iteration Training loss: 0.8031130384621135\n",
      "[0.30942904 0.19947781] -36.119400000001484\n",
      "54500. Iteration Training loss: 0.5831346534173715\n",
      "[0.36397025 0.2903833 ] -36.140400000001485\n",
      "54550. Iteration Training loss: 0.3264065163589496\n",
      "[0.35552873 0.22216971] -36.166000000001475\n",
      "54600. Iteration Training loss: 0.48647943702204904\n",
      "[0.29787868 0.23526968] -36.185700000001475\n",
      "54650. Iteration Training loss: 0.620608859835889\n",
      "[0.35026011 0.30982555] -36.20560000000147\n",
      "54700. Iteration Training loss: 2.8487344657237763\n",
      "[0.43959068 0.38233654] -36.223500000001444\n",
      "54750. Iteration Training loss: 3.946181946693504\n",
      "[0.22141907 0.17579933] -36.24830000000142\n",
      "54800. Iteration Training loss: 4.039808605370502\n",
      "[0.23141091 0.16338478] -36.26990000000144\n",
      "54850. Iteration Training loss: 2.9919901247726823\n",
      "[0.23488384 0.18843857] -36.290500000001444\n",
      "54900. Iteration Training loss: 3.329240970658753\n",
      "[0.46605235 0.39915739] -36.307900000001446\n",
      "54950. Iteration Training loss: 0.29018609463876877\n",
      "[0.30883306 0.25292973] -36.329300000001446\n",
      "55000. Iteration Training loss: 4.384640334155546\n",
      "[0.61313897 0.51645906] -36.347300000001454\n",
      "55050. Iteration Training loss: 3.3748293448763307\n",
      "[0.24806151 0.16576244] -36.37370000000147\n",
      "55100. Iteration Training loss: 2.4987230719262916\n",
      "[0.40659837 0.3917172 ] -36.38920000000145\n",
      "55150. Iteration Training loss: 2.6179651935558867\n",
      "[0.44815832 0.36170854] -36.41140000000146\n",
      "55200. Iteration Training loss: 1.9538867410250247\n",
      "[0.26349674 0.19413308] -36.434900000001484\n",
      "55250. Iteration Training loss: 4.228440972993852\n",
      "[0.59999031 0.46102681] -36.45460000000148\n",
      "55300. Iteration Training loss: 4.54414625921133\n",
      "[0.66238275 0.57105963] -36.47600000000147\n",
      "55350. Iteration Training loss: 1.4242244855812312\n",
      "[0.2636994 0.2160492] -36.50560000000145\n",
      "55400. Iteration Training loss: 2.215453187734835\n",
      "[0.42383601 0.35937557] -36.52230000000147\n",
      "55450. Iteration Training loss: 1.548479431293166\n",
      "[0.26274534 0.21218513] -36.54450000000148\n",
      "55500. Iteration Training loss: 4.538304747499535\n",
      "[0.25867305 0.12493631] -36.568700000001485\n",
      "55550. Iteration Training loss: 1.9341516868347606\n",
      "[0.27745448 0.18320053] -36.5884000000015\n",
      "55600. Iteration Training loss: 2.0970052214743045\n",
      "[0.27214263 0.18307788] -36.6116000000015\n",
      "55650. Iteration Training loss: 1.3477640299942024\n",
      "[0.25840573 0.22679005] -36.6330000000015\n",
      "55700. Iteration Training loss: 2.788365320116548\n",
      "[0.27729343 0.15689335] -36.656900000001514\n",
      "55750. Iteration Training loss: 2.4398486586781742\n",
      "[0.46439686 0.34007834] -36.67890000000149\n",
      "55800. Iteration Training loss: 0.5255932366008387\n",
      "[0.35500119 0.30303042] -36.698200000001506\n",
      "55850. Iteration Training loss: 4.580164866743252\n",
      "[0.66628575 0.61065988] -36.71170000000152\n",
      "55900. Iteration Training loss: 2.014766455218868\n",
      "[0.27241058 0.18749558] -36.74170000000153\n",
      "55950. Iteration Training loss: 3.387186578588875\n",
      "[0.23494541 0.18393354] -36.759500000001545\n",
      "56000. Iteration Training loss: 0.3353291395867224\n",
      "[0.32188733 0.24224664] -36.78410000000155\n",
      "56050. Iteration Training loss: 2.8197279989588435\n",
      "[0.44432637 0.38633074] -36.79970000000154\n",
      "56100. Iteration Training loss: 1.7399500514756883\n",
      "[0.40537485 0.35098663] -36.82430000000154\n",
      "56150. Iteration Training loss: 0.9851503417636696\n",
      "[0.36172145 0.34031277] -36.844000000001564\n",
      "56200. Iteration Training loss: 4.5692706503725455\n",
      "[0.70507311 0.57273205] -36.86580000000158\n",
      "56250. Iteration Training loss: 0.879888505179852\n",
      "[0.40774824 0.2877585 ] -36.89380000000157\n",
      "56300. Iteration Training loss: 3.577882208571562\n",
      "[0.26282401 0.15252137] -36.91800000000156\n",
      "56350. Iteration Training loss: 0.35759665925943745\n",
      "[0.38385894 0.22765242] -36.94260000000155\n",
      "56400. Iteration Training loss: 2.0132313116458307\n",
      "[0.27980488 0.1833601 ] -36.961700000001564\n",
      "56450. Iteration Training loss: 0.4129134823677645\n",
      "[0.30219745 0.25062865] -36.981300000001575\n",
      "56500. Iteration Training loss: 1.389001256517143\n",
      "[0.26143212 0.22726364] -36.999700000001575\n",
      "56550. Iteration Training loss: 2.6277168889165052\n",
      "[0.43487115 0.38549233] -37.01830000000157\n",
      "56600. Iteration Training loss: 1.9518244707655383\n",
      "[0.41562296 0.35902816] -37.04070000000157\n",
      "56650. Iteration Training loss: 3.0268090426477583\n",
      "[0.26010459 0.17276616] -37.06560000000158\n",
      "56700. Iteration Training loss: 1.917588440996977\n",
      "[0.39880054 0.37313014] -37.08510000000158\n",
      "56750. Iteration Training loss: 4.478142352355723\n",
      "[0.64531436 0.5554454 ] -37.104000000001555\n",
      "56800. Iteration Training loss: 0.763418146329102\n",
      "[0.28782002 0.23695014] -37.129700000001584\n",
      "56850. Iteration Training loss: 4.435018111885671\n",
      "[0.61713963 0.55460224] -37.146400000001584\n",
      "56900. Iteration Training loss: 4.5410894535485005\n",
      "[0.65477451 0.58951898] -37.16650000000158\n",
      "56950. Iteration Training loss: 0.35356642710705133\n",
      "[0.33999097 0.30321445] -37.196000000001554\n",
      "57000. Iteration Training loss: 0.7652895709677372\n",
      "[0.28969577 0.23632422] -37.21760000000155\n",
      "57050. Iteration Training loss: 0.293699365168023\n",
      "[0.30622872 0.26743025] -37.23950000000155\n",
      "57100. Iteration Training loss: 2.8022336258521507\n",
      "[0.43570429 0.4013212 ] -37.25390000000158\n",
      "57150. Iteration Training loss: 4.488363527957054\n",
      "[0.62530141 0.57890258] -37.26940000000161\n",
      "57200. Iteration Training loss: 1.996005966842024\n",
      "[0.2728128  0.19560494] -37.294000000001645\n",
      "57250. Iteration Training loss: 1.1759926526297546\n",
      "[0.3866914  0.33855367] -37.308400000001676\n",
      "57300. Iteration Training loss: 4.522868404953214\n",
      "[0.67521355 0.56560344] -37.32840000000168\n",
      "57350. Iteration Training loss: 4.373620500860747\n",
      "[0.61731553 0.53051815] -37.35070000000167\n",
      "57400. Iteration Training loss: 4.572767207639307\n",
      "[0.67428356 0.61011931] -37.37080000000167\n",
      "57450. Iteration Training loss: 2.5685461881460805\n",
      "[0.44140119 0.38221575] -37.39640000000167\n",
      "57500. Iteration Training loss: 2.9209851501348236\n",
      "[0.24137232 0.19944685] -37.419400000001666\n",
      "57550. Iteration Training loss: 2.9668945676854794\n",
      "[0.48601394 0.37112372] -37.440100000001635\n",
      "57600. Iteration Training loss: 0.6013098249804922\n",
      "[0.36027598 0.31900933] -37.45880000000165\n",
      "57650. Iteration Training loss: 4.3662192365494095\n",
      "[0.61975235 0.52825394] -37.47840000000162\n",
      "57700. Iteration Training loss: 4.569948640139812\n",
      "[0.67873335 0.60652777] -37.4991000000016\n",
      "57750. Iteration Training loss: 4.352004203889652\n",
      "[0.62047633 0.52234202] -37.521900000001594\n",
      "57800. Iteration Training loss: 1.0996445685436393\n",
      "[0.40805154 0.31704696] -37.54830000000156\n",
      "57850. Iteration Training loss: 1.6708457081992611\n",
      "[0.41459802 0.35095206] -37.56760000000156\n",
      "57900. Iteration Training loss: 3.5993822060270744\n",
      "[0.25735838 0.16667768] -37.592800000001546\n",
      "57950. Iteration Training loss: 2.7624863291933623\n",
      "[0.48567011 0.35947722] -37.61440000000152\n",
      "58000. Iteration Training loss: 4.125142218194215\n",
      "[0.57174799 0.46964511] -37.636200000001494\n",
      "58050. Iteration Training loss: 2.2781212354126557\n",
      "[0.43289295 0.37533572] -37.65910000000148\n",
      "58100. Iteration Training loss: 4.401594728738188\n",
      "[0.61112726 0.55484879] -37.67780000000146\n",
      "58150. Iteration Training loss: 0.2815463211396596\n",
      "[0.33369181 0.25699657] -37.706700000001454\n",
      "58200. Iteration Training loss: 0.7229518985813442\n",
      "[0.3026546 0.2345222] -37.72820000000144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58250. Iteration Training loss: 2.6994324294164045\n",
      "[0.43789857 0.40086888] -37.74570000000144\n",
      "58300. Iteration Training loss: 2.744108752513223\n",
      "[0.27456736 0.17630401] -37.773000000001396\n",
      "58350. Iteration Training loss: 2.616398840939659\n",
      "[0.43463428 0.39892716] -37.79160000000135\n",
      "58400. Iteration Training loss: 2.960858420652379\n",
      "[0.47332503 0.38877576] -37.815600000001346\n",
      "58450. Iteration Training loss: 4.363721372458533\n",
      "[0.61761196 0.53644566] -37.83310000000133\n",
      "58500. Iteration Training loss: 0.2883544429538952\n",
      "[0.31465667 0.27062641] -37.86160000000128\n",
      "58550. Iteration Training loss: 3.090741377808288\n",
      "[0.25190104 0.19040373] -37.884300000001275\n",
      "58600. Iteration Training loss: 4.402910421809037\n",
      "[0.61271323 0.55923336] -37.901300000001264\n",
      "58650. Iteration Training loss: 1.0726293686431547\n",
      "[0.31330081 0.20568484] -37.9328000000012\n",
      "58700. Iteration Training loss: 2.4283510812642892\n",
      "[0.27528177 0.18791665] -37.95650000000115\n",
      "58750. Iteration Training loss: 3.3750058339229354\n",
      "[0.24905709 0.18655578] -37.977400000001104\n",
      "58800. Iteration Training loss: 1.0718366373806303\n",
      "[0.41127183 0.31949367] -37.997300000001104\n",
      "58850. Iteration Training loss: 2.6799303702692128\n",
      "[0.27199797 0.18421743] -38.0198000000011\n",
      "58900. Iteration Training loss: 3.5549870610438123\n",
      "[0.25437998 0.1770338 ] -38.0418000000011\n",
      "58950. Iteration Training loss: 3.116661998994689\n",
      "[0.26670878 0.17722012] -38.063500000001085\n",
      "59000. Iteration Training loss: 0.378440372722551\n",
      "[0.33513127 0.24430814] -38.085100000001056\n",
      "59050. Iteration Training loss: 0.4737187139149246\n",
      "[0.30350443 0.2598623 ] -38.10710000000101\n",
      "59100. Iteration Training loss: 1.3794601901765942\n",
      "[0.41487923 0.34087102] -38.13010000000096\n",
      "59150. Iteration Training loss: 2.5857804418889674\n",
      "[0.45479324 0.38463668] -38.15070000000096\n",
      "59200. Iteration Training loss: 1.111496494417247\n",
      "[0.40326017 0.33304995] -38.1743000000009\n",
      "59250. Iteration Training loss: 2.742945013461948\n",
      "[0.47309935 0.37976799] -38.19780000000085\n",
      "59300. Iteration Training loss: 4.522498983443759\n",
      "[0.69161437 0.57352908] -38.22020000000082\n",
      "59350. Iteration Training loss: 0.8854132445958061\n",
      "[0.40100658 0.31825527] -38.24850000000078\n",
      "59400. Iteration Training loss: 0.8896968910658875\n",
      "[0.39595825 0.32396839] -38.27230000000073\n",
      "59450. Iteration Training loss: 1.8663721105968591\n",
      "[0.28392469 0.20371702] -38.29860000000068\n",
      "59500. Iteration Training loss: 4.426518486653658\n",
      "[0.63808584 0.56129876] -38.31450000000064\n",
      "59550. Iteration Training loss: 4.42956406711784\n",
      "[0.64611756 0.55757537] -38.33890000000059\n",
      "59600. Iteration Training loss: 4.3965462735419125\n",
      "[0.6403475  0.54672493] -38.36260000000054\n",
      "59650. Iteration Training loss: 0.2478544412591082\n",
      "[0.3441594  0.28841915] -38.38790000000049\n",
      "59700. Iteration Training loss: 2.359125855471528\n",
      "[0.46019411 0.36906466] -38.409400000000446\n",
      "59750. Iteration Training loss: 1.3748803638045184\n",
      "[0.42381949 0.33756495] -38.43190000000041\n",
      "59800. Iteration Training loss: 0.30490196471150954\n",
      "[0.3439477  0.25556797] -38.45630000000036\n",
      "59850. Iteration Training loss: 2.008557172587168\n",
      "[0.43360475 0.3714206 ] -38.474900000000325\n",
      "59900. Iteration Training loss: 1.5452226041343737\n",
      "[0.30842452 0.19550863] -38.5006000000003\n",
      "59950. Iteration Training loss: 1.3677396982989338\n",
      "[0.41274368 0.34856452] -38.516900000000255\n",
      "60000. Iteration Training loss: 2.046282377168972\n",
      "[0.43103234 0.37739087] -38.539800000000255\n",
      "60050. Iteration Training loss: 0.637350676275691\n",
      "[0.30468677 0.25145626] -38.566300000000226\n",
      "60100. Iteration Training loss: 2.1394345965627637\n",
      "[0.2953381  0.18664209] -38.59200000000018\n",
      "60150. Iteration Training loss: 2.6478935822536824\n",
      "[0.25773583 0.20766969] -38.612400000000164\n",
      "60200. Iteration Training loss: 1.3217101081042324\n",
      "[0.40587161 0.35360768] -38.632900000000134\n",
      "60250. Iteration Training loss: 4.350873989393748\n",
      "[0.62011985 0.54689029] -38.65240000000009\n",
      "60300. Iteration Training loss: 4.332091253487827\n",
      "[0.60816455 0.54767303] -38.67660000000004\n",
      "60350. Iteration Training loss: 4.501908699707737\n",
      "[0.67868824 0.5804166 ] -38.69989999999999\n",
      "60400. Iteration Training loss: 2.4951057657972777\n",
      "[0.27123762 0.20059219] -38.72869999999995\n",
      "60450. Iteration Training loss: 0.4355430616965531\n",
      "[0.32083223 0.25794046] -38.75069999999991\n",
      "60500. Iteration Training loss: 0.8487983791641696\n",
      "[0.4354477  0.28959739] -38.77559999999986\n",
      "60550. Iteration Training loss: 4.293381473818222\n",
      "[0.61530167 0.52813036] -38.79229999999981\n",
      "60600. Iteration Training loss: 1.560673544363351\n",
      "[0.4433487  0.33927647] -38.818099999999774\n",
      "60650. Iteration Training loss: 1.3100119053404413\n",
      "[0.29163536 0.2279207 ] -38.842299999999724\n",
      "60700. Iteration Training loss: 3.222325323555696\n",
      "[0.30213049 0.14929225] -38.868299999999685\n",
      "60750. Iteration Training loss: 3.5734923020434364\n",
      "[0.51687863 0.4288041 ] -38.88659999999965\n",
      "60800. Iteration Training loss: 0.7719876996282268\n",
      "[0.41425302 0.30581654] -38.911799999999616\n",
      "60850. Iteration Training loss: 1.8650595200881106\n",
      "[0.4291921  0.37401238] -38.9320999999996\n",
      "60900. Iteration Training loss: 1.6154418422649925\n",
      "[0.41854066 0.36757063] -38.955399999999564\n",
      "60950. Iteration Training loss: 1.005360760539878\n",
      "[0.41765938 0.3247705 ] -38.97739999999952\n",
      "61000. Iteration Training loss: 1.4547072289463503\n",
      "[0.29847153 0.21664927] -39.00339999999948\n",
      "61050. Iteration Training loss: 2.4620360306664986\n",
      "[0.28141504 0.19566875] -39.025399999999436\n",
      "61100. Iteration Training loss: 0.9850835976079693\n",
      "[0.41749705 0.32441319] -39.046199999999416\n",
      "61150. Iteration Training loss: 3.566335246817573\n",
      "[0.51439432 0.43350619] -39.065499999999396\n",
      "61200. Iteration Training loss: 0.31079603834109004\n",
      "[0.34193968 0.26373421] -39.09079999999938\n",
      "61250. Iteration Training loss: 3.4646761331968996\n",
      "[0.30097844 0.14657706] -39.11749999999935\n",
      "61300. Iteration Training loss: 3.556381499526456\n",
      "[0.51673942 0.43139554] -39.134499999999306\n",
      "61350. Iteration Training loss: 2.54121062006068\n",
      "[0.28765355 0.18882092] -39.16179999999928\n",
      "61400. Iteration Training loss: 1.1913032531626655\n",
      "[0.42054921 0.34002849] -39.18059999999926\n",
      "61450. Iteration Training loss: 4.1171921811389165\n",
      "[0.58656676 0.48860051] -39.19979999999924\n",
      "61500. Iteration Training loss: 2.742110903713784\n",
      "[0.27610602 0.19490042] -39.22489999999921\n",
      "61550. Iteration Training loss: 1.642214213141668\n",
      "[0.42725134 0.36639067] -39.2435999999992\n",
      "61600. Iteration Training loss: 4.414139812323539\n",
      "[0.63340203 0.579593  ] -39.261499999999195\n",
      "61650. Iteration Training loss: 4.309138995320202\n",
      "[0.62786523 0.53679918] -39.28639999999916\n",
      "61700. Iteration Training loss: 3.624756435655485\n",
      "[0.2690161  0.17739022] -39.31479999999914\n",
      "61750. Iteration Training loss: 0.5884485732570998\n",
      "[0.39780453 0.31040025] -39.33389999999912\n",
      "61800. Iteration Training loss: 1.9197882574979663\n",
      "[0.44643294 0.36932973] -39.35459999999908\n",
      "61850. Iteration Training loss: 3.40828677579258\n",
      "[0.28270357 0.17052344] -39.37789999999909\n",
      "61900. Iteration Training loss: 3.4016969684448735\n",
      "[0.49010484 0.442326  ] -39.39589999999906\n",
      "61950. Iteration Training loss: 1.4292732924020188\n",
      "[0.29265413 0.22979331] -39.42089999999905\n",
      "62000. Iteration Training loss: 4.360787392803775\n",
      "[0.62980994 0.56159064] -39.43629999999905\n",
      "62050. Iteration Training loss: 3.2884346854268323\n",
      "[0.4813824  0.43976475] -39.45609999999903\n",
      "62100. Iteration Training loss: 2.4704547112101922\n",
      "[0.44745631 0.40734858] -39.475899999999044\n",
      "62150. Iteration Training loss: 1.7356760330689176\n",
      "[0.42247988 0.38131903] -39.493299999999024\n",
      "62200. Iteration Training loss: 4.4552519103557655\n",
      "[0.65155196 0.59213874] -39.51019999999899\n",
      "62250. Iteration Training loss: 0.977920344918199\n",
      "[0.29720462 0.25049449] -39.532799999999014\n",
      "62300. Iteration Training loss: 4.327308375193893\n",
      "[0.6261171  0.55174394] -39.54589999999899\n",
      "62350. Iteration Training loss: 0.787673350280564\n",
      "[0.30721906 0.25315948] -39.56739999999899\n",
      "62400. Iteration Training loss: 2.710057132889196\n",
      "[0.27066608 0.20635379] -39.587199999998965\n",
      "62450. Iteration Training loss: 0.27583818556587425\n",
      "[0.34790584 0.27463584] -39.60579999999897\n",
      "62500. Iteration Training loss: 1.127066096239784\n",
      "[0.41080159 0.35166855] -39.621099999999004\n",
      "62550. Iteration Training loss: 1.1685535962187823\n",
      "[0.42190107 0.34488477] -39.64249999999895\n",
      "62600. Iteration Training loss: 0.26487249387681755\n",
      "[0.36701516 0.2803934 ] -39.6644999999989\n",
      "62650. Iteration Training loss: 2.0052712464692517\n",
      "[0.4406619  0.38594526] -39.682199999998865\n",
      "62700. Iteration Training loss: 0.332938009672165\n",
      "[0.36579809 0.2539099 ] -39.705899999998856\n",
      "62750. Iteration Training loss: 0.3297887064993344\n",
      "[0.36384925 0.31508139] -39.723599999998825\n",
      "62800. Iteration Training loss: 0.8425022774101443\n",
      "[0.40255875 0.3384715 ] -39.74219999999882\n",
      "62850. Iteration Training loss: 1.2833759387340045\n",
      "[0.41799538 0.35887988] -39.76019999999879\n",
      "62900. Iteration Training loss: 2.199095251561991\n",
      "[0.28903731 0.20762538] -39.78289999999876\n",
      "62950. Iteration Training loss: 3.588063280171818\n",
      "[0.51330767 0.45145402] -39.79459999999877\n",
      "63000. Iteration Training loss: 3.6800223338206206\n",
      "[0.5284443  0.45133586] -39.81179999999877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63050. Iteration Training loss: 0.2455251312824894\n",
      "[0.35495024 0.29617803] -39.832999999998755\n",
      "63100. Iteration Training loss: 1.0329730265546708\n",
      "[0.29391188 0.2553932 ] -39.84849999999877\n",
      "63150. Iteration Training loss: 0.49468453692317277\n",
      "[0.38206341 0.32482461] -39.8646999999988\n",
      "63200. Iteration Training loss: 3.3683796823159238\n",
      "[0.49374532 0.44445313] -39.88059999999877\n",
      "63250. Iteration Training loss: 2.4549466728751654\n",
      "[0.28209468 0.20750049] -39.90229999999874\n",
      "63300. Iteration Training loss: 0.2501679705038254\n",
      "[0.35374839 0.305085  ] -39.91809999999874\n",
      "63350. Iteration Training loss: 2.5050629869000933\n",
      "[0.26847838 0.21994154] -39.935899999998725\n",
      "63400. Iteration Training loss: 1.5197435954380978\n",
      "[0.41428909 0.38208546] -39.950899999998704\n",
      "63450. Iteration Training loss: 3.512158509215683\n",
      "[0.51707023 0.44128546] -39.97019999999868\n",
      "63500. Iteration Training loss: 4.324427927952119\n",
      "[0.62565161 0.56049659] -39.987599999998665\n",
      "63550. Iteration Training loss: 2.4245673982010785\n",
      "[0.45335042 0.40829398] -40.00789999999865\n",
      "63600. Iteration Training loss: 1.0979039361183218\n",
      "[0.41146024 0.35570985] -40.028099999998624\n",
      "63650. Iteration Training loss: 1.8230484595196208\n",
      "[0.43447466 0.38576518] -40.044499999998614\n",
      "63700. Iteration Training loss: 2.637179964339248\n",
      "[0.27791429 0.20804657] -40.065299999998594\n",
      "63750. Iteration Training loss: 0.8188856822011332\n",
      "[0.39722992 0.3473554 ] -40.08049999999858\n",
      "63800. Iteration Training loss: 4.414944881471702\n",
      "[0.65438351 0.5833026 ] -40.09319999999861\n",
      "63850. Iteration Training loss: 0.5553979211835111\n",
      "[0.38232378 0.33645779] -40.11419999999862\n",
      "63900. Iteration Training loss: 4.09315016350483\n",
      "[0.58955729 0.4978689 ] -40.132099999998594\n",
      "63950. Iteration Training loss: 3.374981769106106\n",
      "[0.25269004 0.21242949] -40.152499999998604\n",
      "64000. Iteration Training loss: 3.750266640422437\n",
      "[0.54338935 0.45603729] -40.16539999999863\n",
      "64050. Iteration Training loss: 2.4680564195472474\n",
      "[0.45318948 0.41456176] -40.18139999999863\n",
      "64100. Iteration Training loss: 3.262755148867401\n",
      "[0.49116084 0.44199772] -40.19729999999864\n",
      "64150. Iteration Training loss: 0.28944573790623307\n",
      "[0.35945681 0.27281116] -40.218599999998624\n",
      "64200. Iteration Training loss: 0.3190301448718892\n",
      "[0.34378147 0.27643667] -40.233699999998635\n",
      "64250. Iteration Training loss: 2.10519167375223\n",
      "[0.29234427 0.2143795 ] -40.25209999999864\n",
      "64300. Iteration Training loss: 1.4949547069444094\n",
      "[0.4306777 0.3707544] -40.264899999998676\n",
      "64350. Iteration Training loss: 4.286526167517003\n",
      "[0.62163746 0.55315179] -40.27839999999867\n",
      "64400. Iteration Training loss: 0.368097654495543\n",
      "[0.39172938 0.2999737 ] -40.30139999999866\n",
      "64450. Iteration Training loss: 1.260014985799802\n",
      "[0.29650626 0.24755487] -40.31709999999867\n",
      "64500. Iteration Training loss: 2.7291237975581084\n",
      "[0.2786269  0.20818074] -40.33529999999864\n",
      "64550. Iteration Training loss: 0.5380604666436583\n",
      "[0.39650358 0.32329685] -40.350199999998644\n",
      "64600. Iteration Training loss: 1.4088456010180352\n",
      "[0.28133418 0.25633845] -40.36529999999869\n",
      "64650. Iteration Training loss: 1.0471386148448625\n",
      "[0.31900142 0.23770584] -40.38269999999868\n",
      "64700. Iteration Training loss: 3.458119416372098\n",
      "[0.26666397 0.19926916] -40.39909999999868\n",
      "64750. Iteration Training loss: 0.5500386555441189\n",
      "[0.37769852 0.34567932] -40.41229999999868\n",
      "64800. Iteration Training loss: 0.596124618755151\n",
      "[0.31631827 0.27179221] -40.42839999999871\n",
      "64850. Iteration Training loss: 1.6910564208888272\n",
      "[0.31282002 0.21311239] -40.44669999999871\n",
      "64900. Iteration Training loss: 4.375594320189229\n",
      "[0.65347231 0.57376541] -40.45659999999873\n",
      "64950. Iteration Training loss: 2.881863853292879\n",
      "[0.48204481 0.42301817] -40.47449999999872\n",
      "65000. Iteration Training loss: 1.009194238116526\n",
      "[0.29539875 0.26502562] -40.489599999998724\n",
      "65050. Iteration Training loss: 1.975838080691637\n",
      "[0.44723409 0.39237822] -40.503599999998706\n",
      "65100. Iteration Training loss: 4.248752969027703\n",
      "[0.62575876 0.53910304] -40.51929999999872\n",
      "65150. Iteration Training loss: 0.4684106341100688\n",
      "[0.31989004 0.28186143] -40.53819999999874\n",
      "65200. Iteration Training loss: 2.4966815901220256\n",
      "[0.27477254 0.22263662] -40.55719999999872\n",
      "65250. Iteration Training loss: 0.6918381258858162\n",
      "[0.3193209  0.26352514] -40.57329999999871\n",
      "65300. Iteration Training loss: 2.7541834657697297\n",
      "[0.2788444  0.21072209] -40.590799999998715\n",
      "65350. Iteration Training loss: 4.192011722217583\n",
      "[0.59383371 0.54130579] -40.601799999998725\n",
      "65400. Iteration Training loss: 1.2018177854844077\n",
      "[0.42742735 0.35881809] -40.62179999999872\n",
      "65450. Iteration Training loss: 3.4146468221632835\n",
      "[0.49794408 0.46003562] -40.63499999999868\n",
      "65500. Iteration Training loss: 2.5855062324552813\n",
      "[0.2788808  0.21695731] -40.65379999999868\n",
      "65550. Iteration Training loss: 2.4361829146966967\n",
      "[0.46236029 0.41259507] -40.667799999998685\n",
      "65600. Iteration Training loss: 3.8424063154535566\n",
      "[0.54106348 0.48623857] -40.68199999999869\n",
      "65650. Iteration Training loss: 0.9363824688299576\n",
      "[0.29490325 0.27313668] -40.699299999998686\n",
      "65700. Iteration Training loss: 0.28917690712395905\n",
      "[0.36283774 0.27726901] -40.715599999998716\n",
      "65750. Iteration Training loss: 1.2157310487959683\n",
      "[0.29824457 0.25429892] -40.72939999999871\n",
      "65800. Iteration Training loss: 4.368969373587416\n",
      "[0.65417877 0.5764562 ] -40.73889999999875\n",
      "65850. Iteration Training loss: 1.202774532906001\n",
      "[0.31243229 0.24115141] -40.757699999998785\n",
      "65900. Iteration Training loss: 3.8457991044509536\n",
      "[0.54884429 0.48131236] -40.76899999999882\n",
      "65950. Iteration Training loss: 2.1408404507767465\n",
      "[0.45551074 0.40089419] -40.786299999998825\n",
      "66000. Iteration Training loss: 0.2772088960547798\n",
      "[0.34041325 0.29546903] -40.8048999999988\n",
      "66050. Iteration Training loss: 4.028785592927985\n",
      "[0.56857279 0.50986301] -40.8174999999988\n",
      "66100. Iteration Training loss: 0.40900226965229\n",
      "[0.3812657  0.32971157] -40.836499999998814\n",
      "66150. Iteration Training loss: 1.2005579898519851\n",
      "[0.43137021 0.35898708] -40.85199999999883\n",
      "66200. Iteration Training loss: 3.0964585412754424\n",
      "[0.26163873 0.22136064] -40.869899999998836\n",
      "66250. Iteration Training loss: 4.212713758843396\n",
      "[0.61168083 0.54223906] -40.88219999999883\n",
      "66300. Iteration Training loss: 1.2143698474981044\n",
      "[0.42134938 0.37004196] -40.90109999999884\n",
      "66350. Iteration Training loss: 1.1040162754651432\n",
      "[0.30291636 0.25850985] -40.920299999998804\n",
      "66400. Iteration Training loss: 2.4220270646998\n",
      "[0.46399256 0.41487696] -40.93349999999881\n",
      "66450. Iteration Training loss: 1.055321103051269\n",
      "[0.31848418 0.24619726] -40.95019999999883\n",
      "66500. Iteration Training loss: 1.2062367400226308\n",
      "[0.42845398 0.36396613] -40.963499999998845\n",
      "66550. Iteration Training loss: 1.0689652489936323\n",
      "[0.41549348 0.36574432] -40.97889999999885\n",
      "66600. Iteration Training loss: 2.0590611932934424\n",
      "[0.29404373 0.22506629] -40.99739999999884\n",
      "66650. Iteration Training loss: 1.1766388188187802\n",
      "[0.40743335 0.38193921] -41.009499999998845\n",
      "66700. Iteration Training loss: 1.292438856955297\n",
      "[0.30759385 0.24530163] -41.02319999999891\n",
      "66750. Iteration Training loss: 0.5709964884264688\n",
      "[0.38520146 0.35098667] -41.0369999999989\n",
      "66800. Iteration Training loss: 1.9289597697636693\n",
      "[0.45125109 0.39512029] -41.05049999999892\n",
      "66850. Iteration Training loss: 3.6188412437024327\n",
      "[0.53194615 0.46317057] -41.065999999998915\n",
      "66900. Iteration Training loss: 3.624238315330851\n",
      "[0.53765954 0.45891237] -41.07919999999896\n",
      "66950. Iteration Training loss: 0.24942600554626096\n",
      "[0.33922201 0.30886895] -41.097499999998924\n",
      "67000. Iteration Training loss: 0.5289758237804499\n",
      "[0.31474533 0.28969303] -41.11079999999892\n",
      "67050. Iteration Training loss: 1.5015039404775974\n",
      "[0.41774183 0.39777662] -41.12209999999894\n",
      "67100. Iteration Training loss: 3.2688252190497464\n",
      "[0.51881827 0.43469284] -41.13549999999894\n",
      "67150. Iteration Training loss: 0.5087957525502065\n",
      "[0.39670981 0.33310821] -41.150999999998966\n",
      "67200. Iteration Training loss: 1.6454578706563117\n",
      "[0.29457067 0.24387441] -41.16619999999896\n",
      "67250. Iteration Training loss: 0.49591764550413026\n",
      "[0.39729805 0.33123088] -41.18179999999893\n",
      "67300. Iteration Training loss: 2.892513913438107\n",
      "[0.28016697 0.21346156] -41.196599999998966\n",
      "67350. Iteration Training loss: 2.2808211112576386\n",
      "[0.2892273  0.22485476] -41.214299999998936\n",
      "67400. Iteration Training loss: 4.3214760187482\n",
      "[0.63245281 0.58144396] -41.22409999999889\n",
      "67450. Iteration Training loss: 0.25977587622060716\n",
      "[0.37490363 0.29930161] -41.24289999999892\n",
      "67500. Iteration Training loss: 0.275306758570643\n",
      "[0.34880402 0.29618807] -41.25779999999892\n",
      "67550. Iteration Training loss: 2.6118707730803017\n",
      "[0.27090325 0.23276857] -41.27289999999894\n",
      "67600. Iteration Training loss: 0.7688465418763384\n",
      "[0.40370095 0.35686602] -41.285399999998965\n",
      "67650. Iteration Training loss: 0.829799157006697\n",
      "[0.30431499 0.27963063] -41.299499999999\n",
      "67700. Iteration Training loss: 0.9061174874724071\n",
      "[0.4058467  0.36735131] -41.312999999999015\n",
      "67750. Iteration Training loss: 0.5125502838569862\n",
      "[0.3141843  0.29519595] -41.32749999999902\n",
      "67800. Iteration Training loss: 4.0168238776454634\n",
      "[0.57147111 0.51487145] -41.33969999999903\n",
      "67850. Iteration Training loss: 4.014815890575533\n",
      "[0.57021565 0.51569038] -41.352999999999014\n",
      "67900. Iteration Training loss: 2.9589441775279925\n",
      "[0.49637038 0.43212627] -41.364999999999064\n",
      "67950. Iteration Training loss: 0.3590617558473682\n",
      "[0.34056859 0.28908024] -41.38119999999907\n",
      "68000. Iteration Training loss: 4.096280999144474\n",
      "[0.57361013 0.53956084] -41.389099999999125\n",
      "68050. Iteration Training loss: 1.673432993480092\n",
      "[0.30856948 0.23211595] -41.408099999999166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68100. Iteration Training loss: 3.540412713788523\n",
      "[0.54096609 0.45063209] -41.41949999999918\n",
      "68150. Iteration Training loss: 0.36054763219765495\n",
      "[0.33094305 0.29814966] -41.433699999999206\n",
      "68200. Iteration Training loss: 1.2032483595084367\n",
      "[0.29667966 0.26740429] -41.44619999999923\n",
      "68250. Iteration Training loss: 1.2839358641684655\n",
      "[0.31272742 0.24711693] -41.46009999999928\n",
      "68300. Iteration Training loss: 1.156350162763707\n",
      "[0.29927014 0.26768276] -41.4714999999993\n",
      "68350. Iteration Training loss: 1.0301072991489573\n",
      "[0.31137968 0.26275415] -41.483999999999334\n",
      "68400. Iteration Training loss: 1.6827973399999976\n",
      "[0.30940354 0.23219216] -41.49809999999938\n",
      "68450. Iteration Training loss: 1.2515292923905936\n",
      "[0.30617775 0.25613538] -41.50939999999941\n",
      "68500. Iteration Training loss: 2.900924862222831\n",
      "[0.27557999 0.22229208] -41.521299999999435\n",
      "68550. Iteration Training loss: 2.268266095553077\n",
      "[0.29045362 0.22862974] -41.534599999999465\n",
      "68600. Iteration Training loss: 4.288068589825673\n",
      "[0.61643579 0.58535367] -41.5413999999995\n",
      "68650. Iteration Training loss: 0.5084150376147906\n",
      "[0.39344741 0.34357235] -41.55839999999953\n",
      "68700. Iteration Training loss: 3.6933866799554385\n",
      "[0.55001587 0.46788   ] -41.5692999999996\n",
      "68750. Iteration Training loss: 2.383294146963582\n",
      "[0.4604055  0.42717656] -41.58189999999964\n",
      "68800. Iteration Training loss: 3.1758883996984792\n",
      "[0.50230249 0.44945042] -41.59529999999966\n",
      "68850. Iteration Training loss: 0.5299620944463177\n",
      "[0.38426494 0.35675519] -41.60879999999969\n",
      "68900. Iteration Training loss: 0.36447304101349837\n",
      "[0.33677002 0.29518869] -41.62269999999973\n",
      "68950. Iteration Training loss: 1.1245335191065475\n",
      "[0.30650068 0.26455908] -41.63469999999981\n",
      "69000. Iteration Training loss: 2.454553460260125\n",
      "[0.46621487 0.42787394] -41.645199999999825\n",
      "69050. Iteration Training loss: 1.1130695758840334\n",
      "[0.29888965 0.27335663] -41.66039999999983\n",
      "69100. Iteration Training loss: 0.928944385501173\n",
      "[0.42810067 0.35407359] -41.671899999999894\n",
      "69150. Iteration Training loss: 3.111475742259768\n",
      "[0.5148631 0.43382  ] -41.6833999999999\n",
      "69200. Iteration Training loss: 0.6734851154577163\n",
      "[0.34033889 0.26219988] -41.69869999999995\n",
      "69250. Iteration Training loss: 0.29820335117795854\n",
      "[0.3846657  0.31604317] -41.711099999999966\n",
      "69300. Iteration Training loss: 1.1551395691822823\n",
      "[0.30880162 0.26188848] -41.72420000000001\n",
      "69350. Iteration Training loss: 3.13938089704993\n",
      "[0.51015148 0.44149242] -41.73430000000004\n",
      "69400. Iteration Training loss: 1.639195745902226\n",
      "[0.32023387 0.22701818] -41.75240000000009\n",
      "69450. Iteration Training loss: 4.004226239284247\n",
      "[0.57458059 0.51732398] -41.76090000000012\n",
      "69500. Iteration Training loss: 0.9521327091418155\n",
      "[0.31416907 0.26912087] -41.77810000000014\n",
      "69550. Iteration Training loss: 3.2081457173453973\n",
      "[0.50243698 0.455915  ] -41.7870000000002\n",
      "69600. Iteration Training loss: 2.410416896886213\n",
      "[0.2805849  0.23732893] -41.80380000000021\n",
      "69650. Iteration Training loss: 3.908942222790911\n",
      "[0.55273732 0.51345211] -41.81290000000023\n",
      "69700. Iteration Training loss: 0.5959894965215047\n",
      "[0.33865792 0.272021  ] -41.82870000000025\n",
      "69750. Iteration Training loss: 3.364721181259176\n",
      "[0.51433208 0.46177707] -41.83660000000025\n",
      "69800. Iteration Training loss: 1.2837127140315285\n",
      "[0.30910991 0.25664277] -41.85160000000028\n",
      "69850. Iteration Training loss: 2.515041252443421\n",
      "[0.46813772 0.4342097 ] -41.85990000000029\n",
      "69900. Iteration Training loss: 0.4002317720710347\n",
      "[0.37815384 0.34930086] -41.87100000000027\n",
      "69950. Iteration Training loss: 4.281080974386505\n",
      "[0.62343108 0.58463454] -41.87900000000026\n",
      "70000. Iteration Training loss: 0.46812515633655266\n",
      "[0.3886526 0.3488666] -41.89330000000024\n",
      "70050. Iteration Training loss: 0.296061190610967\n",
      "[0.38466978 0.31890329] -41.905300000000246\n",
      "70100. Iteration Training loss: 1.9667247866315467\n",
      "[0.29177601 0.24424985] -41.91950000000024\n",
      "70150. Iteration Training loss: 2.7949639682858516\n",
      "[0.4752648  0.44919267] -41.927300000000265\n",
      "70200. Iteration Training loss: 3.13233861130429\n",
      "[0.50118523 0.45289577] -41.93920000000031\n",
      "70250. Iteration Training loss: 0.46920046879439936\n",
      "[0.40017089 0.3376037 ] -41.95380000000034\n",
      "70300. Iteration Training loss: 1.9946694208016973\n",
      "[0.44948408 0.41732295] -41.964300000000364\n",
      "70350. Iteration Training loss: 2.4258214203231416\n",
      "[0.28447346 0.23538607] -41.980500000000355\n",
      "70400. Iteration Training loss: 2.3822820133321745\n",
      "[0.28146828 0.24011265] -41.99310000000038\n",
      "70450. Iteration Training loss: 0.8456986133400132\n",
      "[0.41373342 0.36618315] -42.00270000000038\n",
      "70500. Iteration Training loss: 0.4623156268707418\n",
      "[0.38204686 0.35701636] -42.012500000000394\n",
      "70550. Iteration Training loss: 0.2410162943356215\n",
      "[0.35315346 0.31653781] -42.02510000000041\n",
      "70600. Iteration Training loss: 0.389263624691731\n",
      "[0.33862496 0.2964088 ] -42.03640000000044\n",
      "70650. Iteration Training loss: 2.8600080011623583\n",
      "[0.27288197 0.23361423] -42.050300000000476\n",
      "70700. Iteration Training loss: 1.1197498467177502\n",
      "[0.30958016 0.26820308] -42.06310000000049\n",
      "70750. Iteration Training loss: 0.27141691007815993\n",
      "[0.38183649 0.31551666] -42.07470000000054\n",
      "70800. Iteration Training loss: 0.9754417155340254\n",
      "[0.30403442 0.28268864] -42.086000000000546\n",
      "70850. Iteration Training loss: 1.0543498496765062\n",
      "[0.30773058 0.27436121] -42.097800000000554\n",
      "70900. Iteration Training loss: 2.5145452099765273\n",
      "[0.29417911 0.22445435] -42.11200000000057\n",
      "70950. Iteration Training loss: 0.30759738082429616\n",
      "[0.3541746  0.29734942] -42.12220000000058\n",
      "71000. Iteration Training loss: 2.050149370549983\n",
      "[0.44851126 0.42502632] -42.13110000000063\n",
      "71050. Iteration Training loss: 2.271406902619657\n",
      "[0.47196209 0.41876507] -42.14340000000064\n",
      "71100. Iteration Training loss: 0.26558498117610285\n",
      "[0.372997   0.32731584] -42.157000000000686\n",
      "71150. Iteration Training loss: 0.8845906323069965\n",
      "[0.30853721 0.28510428] -42.16910000000069\n",
      "71200. Iteration Training loss: 3.073359838179041\n",
      "[0.49986725 0.45333115] -42.17800000000072\n",
      "71250. Iteration Training loss: 0.8753611236157246\n",
      "[0.42199115 0.36404597] -42.192600000000745\n",
      "71300. Iteration Training loss: 1.7553398594859442\n",
      "[0.32731674 0.22156673] -42.2068000000008\n",
      "71350. Iteration Training loss: 3.4720246748000925\n",
      "[0.53412887 0.46314872] -42.21480000000083\n",
      "71400. Iteration Training loss: 3.1014332227078527\n",
      "[0.51364938 0.4440231 ] -42.22870000000084\n",
      "71450. Iteration Training loss: 1.655605877801571\n",
      "[0.30159423 0.25227025] -42.245800000000834\n",
      "71500. Iteration Training loss: 0.33150671973140555\n",
      "[0.38062837 0.33987715] -42.25680000000084\n",
      "71550. Iteration Training loss: 3.0726694702303887\n",
      "[0.49947732 0.4552703 ] -42.26660000000087\n",
      "71600. Iteration Training loss: 0.741709547912623\n",
      "[0.329455   0.27586603] -42.283300000000885\n",
      "71650. Iteration Training loss: 1.3503172378730879\n",
      "[0.32597519 0.24298854] -42.29760000000093\n",
      "71700. Iteration Training loss: 3.3605227846324732\n",
      "[0.51901382 0.46582061] -42.30660000000092\n",
      "71750. Iteration Training loss: 0.2894916974368506\n",
      "[0.35617126 0.30248589] -42.32220000000095\n",
      "71800. Iteration Training loss: 2.0163041095841407\n",
      "[0.28359384 0.25655619] -42.332400000000966\n",
      "71850. Iteration Training loss: 0.6411643971141535\n",
      "[0.33570597 0.27860688] -42.343600000001\n",
      "71900. Iteration Training loss: 0.9017801945775901\n",
      "[0.32710415 0.26822986] -42.35570000000103\n",
      "71950. Iteration Training loss: 2.766053674477143\n",
      "[0.2733209 0.2406004] -42.36740000000107\n",
      "72000. Iteration Training loss: 3.082138973940045\n",
      "[0.52313867 0.43644891] -42.37670000000112\n",
      "72050. Iteration Training loss: 0.33435183027737003\n",
      "[0.35003383 0.29978447] -42.39130000000112\n",
      "72100. Iteration Training loss: 0.33311483210346565\n",
      "[0.36113312 0.29147718] -42.40250000000115\n",
      "72150. Iteration Training loss: 4.030564268911457\n",
      "[0.58377871 0.53111052] -42.41220000000113\n",
      "72200. Iteration Training loss: 0.6950837095386937\n",
      "[0.32541911 0.28549793] -42.428700000001115\n",
      "72250. Iteration Training loss: 2.0256786305717047\n",
      "[0.46620931 0.4124228 ] -42.440700000001115\n",
      "72300. Iteration Training loss: 4.169146486805397\n",
      "[0.60918521 0.56082152] -42.45090000000113\n",
      "72350. Iteration Training loss: 0.49957667654866456\n",
      "[0.39765798 0.35344849] -42.46760000000114\n",
      "72400. Iteration Training loss: 1.2542035618159804\n",
      "[0.31880897 0.25789153] -42.48300000000113\n",
      "72450. Iteration Training loss: 1.00164221854624\n",
      "[0.31556701 0.27560462] -42.495300000001144\n",
      "72500. Iteration Training loss: 2.04356959340121\n",
      "[0.45224388 0.42769025] -42.503000000001144\n",
      "72550. Iteration Training loss: 0.907502276968461\n",
      "[0.32518588 0.27223027] -42.51980000000116\n",
      "72600. Iteration Training loss: 2.735987450768533\n",
      "[0.48253328 0.44892799] -42.52790000000118\n",
      "72650. Iteration Training loss: 0.6190435239575578\n",
      "[0.33595845 0.283186  ] -42.54260000000118\n",
      "72700. Iteration Training loss: 0.44029631545543735\n",
      "[0.39095937 0.35350953] -42.552600000001206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72750. Iteration Training loss: 0.29446740857446657\n",
      "[0.39136463 0.32136226] -42.56530000000123\n",
      "72800. Iteration Training loss: 0.4625220303999155\n",
      "[0.39009526 0.35813594] -42.577100000001266\n",
      "72850. Iteration Training loss: 0.23621564422527386\n",
      "[0.35928244 0.32591691] -42.588100000001276\n",
      "72900. Iteration Training loss: 0.5569460687245094\n",
      "[0.40301312 0.35744053] -42.59880000000129\n",
      "72950. Iteration Training loss: 4.019555806344992\n",
      "[0.58963961 0.52655468] -42.607600000001305\n",
      "73000. Iteration Training loss: 0.28045665130406233\n",
      "[0.38341614 0.32801049] -42.62390000000132\n",
      "73050. Iteration Training loss: 0.6445550940706894\n",
      "[0.33978034 0.27890469] -42.63580000000135\n",
      "73100. Iteration Training loss: 0.375184911483068\n",
      "[0.35434205 0.29382832] -42.64750000000138\n",
      "73150. Iteration Training loss: 2.485086509935572\n",
      "[0.2789465  0.24848565] -42.65870000000141\n",
      "73200. Iteration Training loss: 0.3780055774183351\n",
      "[0.3548604  0.29333533] -42.669700000001434\n",
      "73250. Iteration Training loss: 1.66318309599573\n",
      "[0.30400426 0.25593628] -42.68140000000145\n",
      "73300. Iteration Training loss: 1.0100298241529204\n",
      "[0.31560412 0.27805339] -42.691500000001454\n",
      "73350. Iteration Training loss: 0.2924285195393763\n",
      "[0.35318521 0.30965979] -42.70210000000147\n",
      "73400. Iteration Training loss: 2.447165553409326\n",
      "[0.2936782  0.23577625] -42.7141000000015\n",
      "73450. Iteration Training loss: 0.5044256314457884\n",
      "[0.34260919 0.29019365] -42.72510000000149\n",
      "73500. Iteration Training loss: 2.078898465984793\n",
      "[0.29690185 0.24649665] -42.7372000000015\n",
      "73550. Iteration Training loss: 0.554236045962599\n",
      "[0.40029522 0.36241922] -42.74550000000154\n",
      "73600. Iteration Training loss: 0.511216103282408\n",
      "[0.40237806 0.35495172] -42.75710000000155\n",
      "73650. Iteration Training loss: 2.2915311855441307\n",
      "[0.29423404 0.24160165] -42.77020000000156\n",
      "73700. Iteration Training loss: 2.300990637718007\n",
      "[0.28472199 0.25100956] -42.78220000000156\n",
      "73750. Iteration Training loss: 4.165773389601484\n",
      "[0.61133026 0.56520088] -42.788600000001566\n",
      "73800. Iteration Training loss: 0.2665850571094158\n",
      "[0.38295098 0.32545357] -42.8034000000016\n",
      "73850. Iteration Training loss: 2.250287725975659\n",
      "[0.48219766 0.41978479] -42.81240000000163\n",
      "73900. Iteration Training loss: 0.4590729970386165\n",
      "[0.38822274 0.36377345] -42.82440000000163\n",
      "73950. Iteration Training loss: 4.133189787141891\n",
      "[0.59436018 0.56707743] -42.82990000000164\n",
      "74000. Iteration Training loss: 3.9630817767641817\n",
      "[0.58273132 0.52094428] -42.84320000000166\n",
      "74050. Iteration Training loss: 0.4462228063009658\n",
      "[0.38403441 0.36679312] -42.85560000000166\n",
      "74100. Iteration Training loss: 0.2569942226634239\n",
      "[0.37578276 0.33191971] -42.86750000000169\n",
      "74150. Iteration Training loss: 3.6817860873279513\n",
      "[0.53551694 0.50592593] -42.87540000000171\n",
      "74200. Iteration Training loss: 2.141413517879583\n",
      "[0.46059283 0.43355681] -42.88720000000173\n",
      "74250. Iteration Training loss: 0.44109492862021404\n",
      "[0.39752231 0.35249598] -42.90110000000173\n",
      "74300. Iteration Training loss: 4.006006933513809\n",
      "[0.57705756 0.54023966] -42.91000000000175\n",
      "74350. Iteration Training loss: 1.8515368390584022\n",
      "[0.29047001 0.26502164] -42.92360000000178\n",
      "74400. Iteration Training loss: 0.27799919502310716\n",
      "[0.34314463 0.32556127] -42.9340000000018\n",
      "74450. Iteration Training loss: 3.7465273435720237\n",
      "[0.54081105 0.51383973] -42.9406000000018\n",
      "74500. Iteration Training loss: 0.2828975900065368\n",
      "[0.35176311 0.31682022] -42.954500000001794\n",
      "74550. Iteration Training loss: 2.0245533907595585\n",
      "[0.45706319 0.43004898] -42.9640000000018\n",
      "74600. Iteration Training loss: 0.827039472630287\n",
      "[0.32041529 0.28931598] -42.97590000000181\n",
      "74650. Iteration Training loss: 0.44449113133573276\n",
      "[0.32921573 0.31332293] -42.98460000000184\n",
      "74700. Iteration Training loss: 3.918914861195558\n",
      "[0.55969894 0.53409313] -42.99030000000186\n",
      "74750. Iteration Training loss: 0.4347839658407844\n",
      "[0.38363108 0.36796127] -43.001800000001886\n",
      "74800. Iteration Training loss: 0.4891840107102769\n",
      "[0.39385997 0.36524747] -43.011700000001895\n",
      "74850. Iteration Training loss: 0.40208418400869034\n",
      "[0.38669477 0.35989759] -43.022900000001904\n",
      "74900. Iteration Training loss: 0.814908078207241\n",
      "[0.32113248 0.29033906] -43.0349000000019\n",
      "74950. Iteration Training loss: 0.2943522559641549\n",
      "[0.37881272 0.34586722] -43.043700000001934\n",
      "75000. Iteration Training loss: 1.0488896307869318\n",
      "[0.31470129 0.28218468] -43.054000000001956\n",
      "75050. Iteration Training loss: 0.520086151615624\n",
      "[0.39840935 0.36546031] -43.062500000001975\n",
      "75100. Iteration Training loss: 2.6989012651724607\n",
      "[0.48894787 0.4500448 ] -43.069300000002\n",
      "75150. Iteration Training loss: 0.575606131749738\n",
      "[0.39473181 0.3762208 ] -43.079100000002036\n",
      "75200. Iteration Training loss: 4.004490254874159\n",
      "[0.57602195 0.54451532] -43.086100000002034\n",
      "75250. Iteration Training loss: 0.6941884618032715\n",
      "[0.40115309 0.38272666] -43.09770000000204\n",
      "75300. Iteration Training loss: 3.742015081395829\n",
      "[0.54229914 0.51490191] -43.10610000000207\n",
      "75350. Iteration Training loss: 3.7517462423427417\n",
      "[0.55035487 0.5090468 ] -43.11610000000208\n",
      "75400. Iteration Training loss: 0.7773593272249885\n",
      "[0.31310983 0.30267725] -43.128700000002105\n",
      "75450. Iteration Training loss: 0.6249687825770305\n",
      "[0.32367517 0.30381176] -43.13780000000213\n",
      "75500. Iteration Training loss: 2.696039648792756\n",
      "[0.49152398 0.4488115 ] -43.14480000000215\n",
      "75550. Iteration Training loss: 1.9428499652078048\n",
      "[0.44984179 0.43441835] -43.15380000000216\n",
      "75600. Iteration Training loss: 0.3118196288758606\n",
      "[0.33866934 0.32612834] -43.16530000000216\n",
      "75650. Iteration Training loss: 1.9142758542418088\n",
      "[0.45878459 0.42441206] -43.17290000000218\n",
      "75700. Iteration Training loss: 0.26031712092253395\n",
      "[0.37502631 0.34043373] -43.184000000002214\n",
      "75750. Iteration Training loss: 0.29191310223821654\n",
      "[0.37890341 0.34756926] -43.19240000000224\n",
      "75800. Iteration Training loss: 0.7718686792192611\n",
      "[0.32181267 0.29528889] -43.20250000000226\n",
      "75850. Iteration Training loss: 0.2582315418992896\n",
      "[0.36163101 0.32003797] -43.21260000000228\n",
      "75900. Iteration Training loss: 0.2409436237749089\n",
      "[0.3706003  0.32743799] -43.22220000000233\n",
      "75950. Iteration Training loss: 0.827680590762188\n",
      "[0.32211729 0.29148455] -43.23130000000237\n",
      "76000. Iteration Training loss: 2.374640855476765\n",
      "[0.28274285 0.25686323] -43.240500000002406\n",
      "76050. Iteration Training loss: 2.352547682889295\n",
      "[0.28507791 0.25543727] -43.249400000002446\n",
      "76100. Iteration Training loss: 0.8136796927494762\n",
      "[0.32099592 0.29399738] -43.25750000000247\n",
      "76150. Iteration Training loss: 0.8825619593920437\n",
      "[0.3327995  0.27767553] -43.267100000002486\n",
      "76200. Iteration Training loss: 0.829086249745258\n",
      "[0.32426679 0.28989923] -43.27550000000253\n",
      "76250. Iteration Training loss: 0.6719484692709962\n",
      "[0.3290935 0.2968258] -43.28550000000257\n",
      "76300. Iteration Training loss: 0.31674449555775036\n",
      "[0.34412764 0.32186799] -43.29420000000261\n",
      "76350. Iteration Training loss: 3.9010187497539617\n",
      "[0.56798434 0.52820854] -43.302000000002664\n",
      "76400. Iteration Training loss: 0.4413778070023072\n",
      "[0.39841732 0.35860591] -43.3138000000027\n",
      "76450. Iteration Training loss: 0.25487167048819986\n",
      "[0.3605969  0.32367979] -43.323300000002746\n",
      "76500. Iteration Training loss: 2.17799389373438\n",
      "[0.28626513 0.26200015] -43.332400000002785\n",
      "76550. Iteration Training loss: 0.25637750888061456\n",
      "[0.37303423 0.34375414] -43.340300000002806\n",
      "76600. Iteration Training loss: 3.961155709073241\n",
      "[0.57155984 0.54158932] -43.34560000000284\n",
      "76650. Iteration Training loss: 0.7588746582206063\n",
      "[0.32586058 0.29455505] -43.35830000000286\n",
      "76700. Iteration Training loss: 0.4802770854314566\n",
      "[0.40299514 0.36039738] -43.36620000000288\n",
      "76750. Iteration Training loss: 3.992163078340984\n",
      "[0.58203244 0.54139341] -43.37240000000289\n",
      "76800. Iteration Training loss: 1.9055981058146942\n",
      "[0.46161701 0.4248688 ] -43.38270000000293\n",
      "76850. Iteration Training loss: 0.4220345497923414\n",
      "[0.39193206 0.36383066] -43.393200000002935\n",
      "76900. Iteration Training loss: 2.732212683758614\n",
      "[0.4851051  0.46214422] -43.400300000002915\n",
      "76950. Iteration Training loss: 2.412852501029029\n",
      "[0.28868149 0.25194089] -43.41320000000295\n",
      "77000. Iteration Training loss: 2.214678311201888\n",
      "[0.48011896 0.4300298 ] -43.41950000000299\n",
      "77050. Iteration Training loss: 3.6478548673321147\n",
      "[0.54338558 0.50350638] -43.42760000000303\n",
      "77100. Iteration Training loss: 3.9894214204508955\n",
      "[0.58382033 0.54028424] -43.436400000003076\n",
      "77150. Iteration Training loss: 0.5156849228906555\n",
      "[0.40327635 0.36634264] -43.44800000000311\n",
      "77200. Iteration Training loss: 0.7434781798696929\n",
      "[0.32606578 0.29703516] -43.45880000000316\n",
      "77250. Iteration Training loss: 3.289460181571242\n",
      "[0.52290292 0.47636873] -43.46480000000318\n",
      "77300. Iteration Training loss: 2.543171118219095\n",
      "[0.48279972 0.45172896] -43.474300000003225\n",
      "77350. Iteration Training loss: 0.7719828371067\n",
      "[0.32443414 0.29701539] -43.48630000000327\n",
      "77400. Iteration Training loss: 3.3333252100492476\n",
      "[0.51694513 0.48733161] -43.49240000000331\n",
      "77450. Iteration Training loss: 2.2417246774595627\n",
      "[0.46845703 0.4442216 ] -43.50190000000336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77500. Iteration Training loss: 2.3952389166658885\n",
      "[0.29041038 0.25227304] -43.514200000003406\n",
      "77550. Iteration Training loss: 2.468893740085434\n",
      "[0.2853959  0.25481232] -43.5236000000034\n",
      "77600. Iteration Training loss: 0.3421535913494688\n",
      "[0.38499674 0.3589792 ] -43.53130000000338\n",
      "77650. Iteration Training loss: 0.9826629930417938\n",
      "[0.31658021 0.29176484] -43.54140000000342\n",
      "77700. Iteration Training loss: 0.5023437433596919\n",
      "[0.40092975 0.36873634] -43.54970000000347\n",
      "77750. Iteration Training loss: 2.3675484755073573\n",
      "[0.28227531 0.26217159] -43.56070000000349\n",
      "77800. Iteration Training loss: 3.894753937417767\n",
      "[0.56788674 0.53231284] -43.56640000000354\n",
      "77850. Iteration Training loss: 2.4516919296458655\n",
      "[0.2916254  0.24994643] -43.58000000000358\n",
      "77900. Iteration Training loss: 2.2078093215035803\n",
      "[0.48250059 0.4303186 ] -43.58630000000362\n",
      "77950. Iteration Training loss: 3.945986869539786\n",
      "[0.5751465  0.53927345] -43.59380000000365\n",
      "78000. Iteration Training loss: 0.24713620765798766\n",
      "[0.37919619 0.33171277] -43.6071000000037\n",
      "78050. Iteration Training loss: 2.02036383025494\n",
      "[0.30137596 0.25707649] -43.61720000000372\n",
      "78100. Iteration Training loss: 0.4986832190318048\n",
      "[0.33433131 0.3123977 ] -43.62570000000371\n",
      "78150. Iteration Training loss: 0.2986318842698866\n",
      "[0.35138092 0.32407631] -43.63510000000369\n",
      "78200. Iteration Training loss: 1.0849535545922664\n",
      "[0.42444203 0.40343696] -43.64200000000371\n",
      "78250. Iteration Training loss: 3.894610349127422\n",
      "[0.56387634 0.53789642] -43.64870000000372\n",
      "78300. Iteration Training loss: 1.9440946566692228\n",
      "[0.29221502 0.27017902] -43.66170000000376\n",
      "78350. Iteration Training loss: 3.9917003233687227\n",
      "[0.58063912 0.54886436] -43.66700000000374\n",
      "78400. Iteration Training loss: 1.9360264239717635\n",
      "[0.46047297 0.43326745] -43.67750000000373\n",
      "78450. Iteration Training loss: 0.5060065352388379\n",
      "[0.33076675 0.31622387] -43.68810000000375\n",
      "78500. Iteration Training loss: 1.9535629561218442\n",
      "[0.4626528  0.43279102] -43.695100000003784\n",
      "78550. Iteration Training loss: 3.9432587928658096\n",
      "[0.58603547 0.53051413] -43.703600000003824\n",
      "78600. Iteration Training loss: 3.9861702386168965\n",
      "[0.58399706 0.54503775] -43.711700000003844\n",
      "78650. Iteration Training loss: 2.0199771346165156\n",
      "[0.46291521 0.4377173 ] -43.722400000003866\n",
      "78700. Iteration Training loss: 0.3055171739052949\n",
      "[0.3570259  0.31912231] -43.733400000003876\n",
      "78750. Iteration Training loss: 3.762010223736141\n",
      "[0.55313812 0.52107009] -43.7393000000039\n",
      "78800. Iteration Training loss: 0.49066693526342897\n",
      "[0.39915212 0.3723852 ] -43.750300000003904\n",
      "78850. Iteration Training loss: 1.9286875767570757\n",
      "[0.45873045 0.43578599] -43.75770000000391\n",
      "78900. Iteration Training loss: 0.46895731058973056\n",
      "[0.39867665 0.37013754] -43.76790000000395\n",
      "78950. Iteration Training loss: 0.41745474668187943\n",
      "[0.39438898 0.36694491] -43.77740000000394\n",
      "79000. Iteration Training loss: 0.5658606612726911\n",
      "[0.40463365 0.37693323] -43.78620000000395\n",
      "79050. Iteration Training loss: 0.9844241388955537\n",
      "[0.3197981  0.29229496] -43.797200000004\n",
      "79100. Iteration Training loss: 2.3654125321696844\n",
      "[0.28708963 0.2608919 ] -43.80650000000402\n",
      "79150. Iteration Training loss: 0.24549105583626496\n",
      "[0.37849547 0.33538506] -43.81500000000404\n",
      "79200. Iteration Training loss: 0.33148734313874156\n",
      "[0.35540821 0.31706949] -43.82490000000409\n",
      "79250. Iteration Training loss: 2.1288671648152793\n",
      "[0.29093518 0.2664615 ] -43.834700000004105\n",
      "79300. Iteration Training loss: 2.009887566411826\n",
      "[0.46241488 0.43958126] -43.84150000000411\n",
      "79350. Iteration Training loss: 1.385521638714485\n",
      "[0.31541526 0.27515757] -43.85360000000413\n",
      "79400. Iteration Training loss: 1.5872540696007382\n",
      "[0.30212296 0.27897662] -43.86260000000416\n",
      "79450. Iteration Training loss: 0.2764525614504954\n",
      "[0.35774418 0.32739598] -43.87190000000422\n",
      "79500. Iteration Training loss: 2.3882413846943686\n",
      "[0.28519133 0.26303349] -43.88090000000422\n",
      "79550. Iteration Training loss: 2.822688160485358\n",
      "[0.50107087 0.46307399] -43.88680000000423\n",
      "79600. Iteration Training loss: 0.5217808668056461\n",
      "[0.3328425  0.31584034] -43.898100000004234\n",
      "79650. Iteration Training loss: 0.6172434672537751\n",
      "[0.4125615  0.37685629] -43.90630000000424\n",
      "79700. Iteration Training loss: 0.43794463519800675\n",
      "[0.39829269 0.36839939] -43.91590000000426\n",
      "79750. Iteration Training loss: 1.9377829441285752\n",
      "[0.46455205 0.43392876] -43.9229000000043\n",
      "79800. Iteration Training loss: 3.5699123746714445\n",
      "[0.53547417 0.5088556 ] -43.93080000000431\n",
      "79850. Iteration Training loss: 0.29061192748147474\n",
      "[0.38377445 0.35422324] -43.942600000004354\n",
      "79900. Iteration Training loss: 3.8812711277992276\n",
      "[0.58464144 0.52083178] -43.94990000000439\n",
      "79950. Iteration Training loss: 0.29652487020798624\n",
      "[0.35773938 0.32389006] -43.961900000004405\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "967dc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = logistic_regressor.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b4a3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e11ec029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.58\n",
      "Test Precision:  1.0\n",
      "Test Recall:  0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: \",accuracy_score(y,train_preds))\n",
    "print(\"Test Precision: \",precision_score(y,train_preds))\n",
    "print(\"Test Recall: \",recall_score(y,train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "877ead67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22d9fe70790>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=df['x1'],y=train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a8edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a3dcc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b13227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18463803",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac971376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.89\n",
      "Test Precision:  0.9016393442622951\n",
      "Test Recall:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: \",accuracy_score(y,preds))\n",
    "print(\"Test Precision: \",precision_score(y,preds))\n",
    "print(\"Test Recall: \",recall_score(y,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2146e237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.20623222, 0.2014719 ]]), array([-25.16138457]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_,lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7a90d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.58443093, 0.48469676]), -47.947167362204105)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regressor.w,logistic_regressor.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdef24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor.w[0] = 0.29141818\n",
    "logistic_regressor.w[1] = 0.2374198\n",
    "logistic_regressor.b = -42.63006736219058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ef40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
